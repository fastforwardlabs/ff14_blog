{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Build a QA System with BERT on Wikipedia\n",
    "> A high-level walk-through of building an IR-based QA system.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- hide: true\n",
    "- permalink: /hidden/\n",
    "- search_exclude: false\n",
    "- categories: [jupyter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So you've decided to build a QA system. \n",
    "You want to start with something general and straightforward so you plan to make it open domain using Wikipedia as a corpus for answering questions. You're going to use an IR-based design (see [previous post](https://qa.fastforwardlabs.com/methods/background/2020/04/28/Intro-to-QA.html)) since you're working with a large collection of unstructured text. You want to use the best NLP that your compute resources allow (you're lucky enough to have access to a GPU) so you're going to focus on the big, flashy Transformer models that are all the rage these days. \n",
    "\n",
    "Sounds like a plan! So where do you start? \n",
    "\n",
    "This was our initial thought process and in this post we'll discuss what you need to know to get going including\n",
    "\n",
    "- installing libraries and setting up an environment\n",
    "- training a Transformer style model with Hugging Face on the SQuAD dataset\n",
    "- understanding Huggingface's `run_squad.py` training script and output\n",
    "- getting BERT to accept a full Wikipedia article as context for a question\n",
    "\n",
    "\n",
    "# Setting up your virtual environment\n",
    "A virtual environment is always best practice and we're using `venv` (though I'm also partial to `conda`). For this project we'll be using Pytorch, which handles the heavy lifting of deep differentiable learning. If you have a GPU you'll want a PyTorch build that includes CUDA support, however, most cells in this notebook will work without a GPU. Check out [PyTorch's quick install guide](https://pytorch.org/) to determine the best build for your GPU and OS. We'll also be using the [Transformers](https://huggingface.co/transformers/index.html) libarary, which provides easy-to-use implementations of all the popular Transformer architectures, like BERT. Finally, we'll need the [wikipedia](https://pypi.org/project/wikipedia/) library for easy access and parsing of Wikipedia pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can recreate our env (with CUDA support) with the following commands in your command line:\n",
    "\n",
    "\n",
    "``` bash\n",
    "$ python3 -m venv myenv\n",
    "$ source myenv/bin/activate\n",
    "$ pip install torch==1.5.0+cu92 torchvision==0.6.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "$ pip install transformers\n",
    "$ pip install wikipedia\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note 1: our GPU machine sports an older version of CUDA (9.2 -- we're getting around to updating that), so we need to use an older version of PyTorch for the necessary CUDA support.  \n",
    "\n",
    "Note 2: The training script we'll be using requires some specific packages. More recent versions of PyTorch include these packages; however, older versions do not and thus you might need to install `TensorboardX` (see the hidden code cell below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-hide \n",
    "\n",
    "# line 69 of `run_squad.py` script shows why you might need to install \n",
    "# tensorboardX if you have an older version of torch\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers\n",
    "I'm new to PyTorch and even newer to HuggingFace but I'm quickly becoming a convert! The [HuggingFace Transformers](https://huggingface.co/transformers/#) package provides state-of-the-art general-purpose architecures for natural language understanding and natural language generation. They host dozens of pre-trained models operating in over 100 languages that you can use right out of the box. All of these models come with deep interoperability between PyTorch and Tensorflow 2.0, which means you can move a model from TF2.0 to PyTorch and back again with a line or two of code! \n",
    "\n",
    "\n",
    "If you're new to Hugging Face, we strongly recommend working through the HF [Quickstart guide](https://huggingface.co/transformers/quickstart.html) as well as their excellent [Transformer Notebooks](https://huggingface.co/transformers/notebooks.html) (we did!), as we won't cover that material in this notebook. We'll be using [`AutoClasses`](https://huggingface.co/transformers/model_doc/auto.html), which serve as a wrapper around pretty much any of the base Transformer classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a Transformer model for Question Answering\n",
    "\n",
    "To train a Transformer for QA with Hugging Face we'll need\n",
    "1. to pick a specific model architecture\n",
    "2. a QA dataset\n",
    "3. the training script\n",
    "\n",
    "\n",
    "### Pick a Model\n",
    "Not every Transformer architecture lends itself naturally to the task of Question Answering. For example, GPT does not do QA; similarly BERT does not do machine translation!  HF identify the following model types for the QA task: \n",
    "\n",
    "- BERT\n",
    "- distilBERT \n",
    "- ALBERT\n",
    "- RoBERTa\n",
    "- XLNet\n",
    "- XLM\n",
    "- FlauBERT\n",
    "\n",
    "\n",
    "We'll stick with the now-classic BERT model in this notebook, but in a future post we'll compare and contrast some of the most popular QA architectures. Next up -- a training set. \n",
    "\n",
    "\n",
    "### SQuAD dataset\n",
    "One of the most canonical datasets for QA is the Stanford Question Answering Dataset, or SQuAD, which comes in two flavors: SQuAD 1.1 and SQuAD 2.0. These reading comprehension datasets consist of questions posed on a set of Wikipedia articles, where the answer to every question is a segment (or span) of the corresponding passage. In SQuAD 1.1, all questions have an answer in the corresponding passage. SQuAD 2.0 steps up the difficulty by including questions that cannot be answered by the provided passage. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DATA_DIR=./data/squad\n",
      "--2020-05-11 21:36:52--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.109.153, 185.199.108.153, 185.199.111.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.109.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42123633 (40M) [application/json]\n",
      "Saving to: ‘./data/squad/train-v2.0.json’\n",
      "\n",
      "train-v2.0.json     100%[===================>]  40.17M  14.6MB/s    in 2.8s    \n",
      "\n",
      "2020-05-11 21:36:55 (14.6 MB/s) - ‘./data/squad/train-v2.0.json’ saved [42123633/42123633]\n",
      "\n",
      "--2020-05-11 21:36:56--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.111.153, 185.199.108.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4370528 (4.2M) [application/json]\n",
      "Saving to: ‘./data/squad/dev-v2.0.json’\n",
      "\n",
      "dev-v2.0.json       100%[===================>]   4.17M  6.68MB/s    in 0.6s    \n",
      "\n",
      "2020-05-11 21:36:56 (6.68 MB/s) - ‘./data/squad/dev-v2.0.json’ saved [4370528/4370528]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set path with magic\n",
    "%env DATA_DIR=./data/squad \n",
    "\n",
    "# Download the data\n",
    "def download_squad(version=1):\n",
    "    if version == 1:\n",
    "        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
    "        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
    "    else:\n",
    "        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
    "            \n",
    "download_squad(version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning script\n",
    "\n",
    "We've chosen a model and we've got some data. Time to train!\n",
    "\n",
    "\n",
    "HF helpfully provide a script that fine-tunes a Transformer model on one of the SQuAD datasets, called `run_squad.py`. You can grab the script [here](https://github.com/huggingface/transformers/blob/master/examples/question-answering/run_squad.py).  This script takes care of all the hard work that goes into fine-tuning a model and as such, it's pretty complicated. This script is meant to be run on the command line and hosts no fewer than 45 arguments, providing an impressive amount of flexibility and utility for those who do a lot of training. In a later post we'll go through more of the details for those who really want to get into the nuts and bolts, but for now we'll walk through the basic command to fine-tune BERT on SQuAD 1.1 or 2.0. Below are the most important arguments for the `run_squad.py` fine-tuning script. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning your own model for QA using HF's `run_squad.py`\n",
    "# Turn flags on and off according to the model you're training\n",
    "\n",
    "cmd = [\n",
    "    'python', \n",
    "#    '-m torch.distributed.launch --nproc_per_node 2', # use this to perform distributed training over multiple GPUs\n",
    "    'run_squad.py', \n",
    "    \n",
    "    '--model_type', 'bert',                            # model type \n",
    "    '--model_name_or_path', 'bert-base-uncased',       # specific model name of the given model type\n",
    "    '--output_dir', './models/bert/bbu_squad2',        # directory for model checkpoints and predictions\n",
    "#    '--overwrite_output_dir',                         # use when adding output to a directory that is non-empty\n",
    "    \n",
    "    '--do_train',     \n",
    "    '--train_file', '$DATA_DIR/train-v2.0.json',       # provide the training data\n",
    "    \n",
    "    '--version_2_with_negative',                       # **MUST use this flag if training on SQuAD 2.0 dataset!\n",
    "    '--do_lower_case',                                 # **Set this flag if using an uncased model\n",
    "    \n",
    "    '--do_eval',                                       # evaluate the model on the dev set after fine-tuning complete\n",
    "    '--predict_file', '$DATA_DIR/dev-v2.0.json',       # provide evaluation data (dev set)\n",
    "    '--eval_all_checkpoints',                          # evaluate the model on the dev set at each checkpoint\n",
    "    \n",
    "    '--num_train_epochs', '3',                         # model hyperparameters\n",
    "    '--learning_rate', '3e-5',\n",
    "    '--max_seq_length', '384',\n",
    "    '--doc_stride', '128',\n",
    "    '--per_gpu_eval_batch_size', '12',\n",
    "    '--per_gpu_train_batch_size', '12',\n",
    "    \n",
    "    '--save_steps', '10000',                           # How often checkpoints (complete model snapshot) are saved \n",
    "    '--threads', '8'                                   # num of CPU threads to use for converting examples to features\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I save the following as a shell script (`run_squad.sh`) and run on the command line (`$ source run_squad.sh`). Using shell scripts helps keep me from making numerous mistakes and mis-keys when typing args to a command line. It also helps me keep track of which arguments I used last (though, as we'll see below, the `run_squad.py` script has a solution for that). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/sh\n",
    "export DATA_DIR=./data/squad \n",
    "export MODEL_DIR=./models\n",
    "python run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path bert-base-uncased  \\\n",
    "    --output_dir $MODEL_DIR/bert/bbu_squad2/ \\\n",
    "    --overwrite_output_dir \\\n",
    "    --do_train  \\\n",
    "    --train_file $DATA_DIR/train-v2.0.json   \\\n",
    "    --version_2_with_negative \\\n",
    "    --do_lower_case  \\\n",
    "    --do_eval   \\\n",
    "    --predict_file $DATA_DIR/dev-v2.0.json   \\\n",
    "    --per_gpu_train_batch_size 12   \\\n",
    "    --learning_rate 3e-5   \\\n",
    "    --num_train_epochs 2.0   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128   \\\n",
    "    --save_steps 5000 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can run the command directly from this jupyter notebook using the cell below. However, we don't recommend running it unless you're rocking at least one GPU or don't mind waiting a couple days! For context, our GPU is several years old (GeForce GTX TITAN X) and while it's not nearly as fast as the Tesla V100 (the current cadillac of GPUs) it gets the job done. Fine-tuning BERT_base takes about 1.75 hours _per epoch_. It's typical to train for 2 - 3 epochs on either SQuAD dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from subprocess import PIPE, STDOUT, Popen\n",
    "\n",
    "# Live output from run_squad.py is through stderr (rather than stdout). \n",
    "# The following command runs the process and ports stderr to stdout\n",
    "p = Popen(cmd,\n",
    "          stdout=PIPE,\n",
    "          stderr=STDOUT)\n",
    "\n",
    "# Default behavior when using bash cells in jupyter is that you won't see the live output in the cell \n",
    "# -- you can only see output once the entire process has finished and then you get it all at once. \n",
    "# This is less than ideal when training models that can take hours or days of compute time! \n",
    "\n",
    "# This command combined with the above allows you to see the live output feed in the notebook, \n",
    "# though it is a bit asynchronous.\n",
    "for line in iter(p.stdout.readline, b''):\n",
    "    print(\">>> \" + line.decode().rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Output\n",
    "\n",
    "Successful completion of the `run_squad.py` yields a slew of output, which can be found in the `--output_dir` directory specified above. There you'll find...   \n",
    "\n",
    "Files for the model's tokenizer:\n",
    "* `tokenizer_config.json`\n",
    "* `vocab.txt`\n",
    "* `special_tokens_map.json`\n",
    "\n",
    "Files for the model itself:\n",
    "* `pytorch_model.bin`: these are the actual model weights (this file can be quite large)\n",
    "* `config.json`: details of the model architecture\n",
    "\n",
    "Binary representation of the command line arguments used to train this model (so you'll never forget which arguments you used!)\n",
    "* `training_args.bin`\n",
    "\n",
    "And if you included `--do_eval`, you'll also see these files:\n",
    "* `predictions_.json`: the official best answer for each example\n",
    "* `nbest_predictions_.json`: the top n best answers for each example\n",
    "\n",
    "\n",
    "Providing the path to this directory to `AutoModel` or `AutoModelForQuestionAnswering` will load your fine-tuned model for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# Load the fine-tuned model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/bert/bbu_squad2\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"./models/bert/bbu_squad2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a pre-fine-tuned model from the Hugging Face repository\n",
    "If you don't have access to GPUs or don't have the time to fiddle and train models, you're in luck! Hugging Face is more than a collection of slick Transformer classes -- it also hosts [a repository](https://huggingface.co/models) for pre-trained and fine-tuned models contributed from the wide community of NLP practitioners. Searching for \"squad\" brings up at least 55 models. \n",
    "\n",
    "![](images/post2/HF_repo.png)\n",
    "\n",
    "\n",
    "Clicking one of these links gives explicit code for using the model, and, in some cases, information on how it was trained and what results were achieved. Let's load one of these pre-fine-tuned models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# Executing these commands for the first time initiates a download of the \n",
    "# model weights to ~/.cache/torch/transformers/\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\") \n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try our model!\n",
    "\n",
    "Whether you fine-tuned your own or used a pre-fine-tuned model, it's time to play with it! There are three steps to QA: \n",
    "1. tokenize the input\n",
    "2. obtain model scores\n",
    "3. get the answer span\n",
    "\n",
    "These steps are discussed in detail in the HF [Transformer Notebooks](https://huggingface.co/transformers/notebooks.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the Argead dynasty'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who ruled Macedonia\"\n",
    "\n",
    "context = \"\"\"Macedonia was an ancient kingdom on the periphery of Archaic and Classical Greece, \n",
    "and later the dominant state of Hellenistic Greece. The kingdom was founded and initially ruled \n",
    "by the Argead dynasty, followed by the Antipatrid and Antigonid dynasties. Home to the ancient \n",
    "Macedonians, it originated on the northeastern part of the Greek peninsula. Before the 4th \n",
    "century BC, it was a small kingdom outside of the area dominated by the city-states of Athens, \n",
    "Sparta and Thebes, and briefly subordinate to Achaemenid Persia.\"\"\"\n",
    "\n",
    "\n",
    "# 1. TOKENIZE THE INPUT\n",
    "# Note: if you don't include return_tensors='pt' you'll get a list of lists which is easier for \n",
    "# exploration but you cannot feed that into a model. \n",
    "inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\") \n",
    "\n",
    "# 2. OBTAIN MODEL SCORES\n",
    "# The AutoModelForQuestionAnswering class includes a span predictor on top of the model. \n",
    "# The model returns answer start and end scores for each word in the text\n",
    "answer_start_scores, answer_end_scores = model(**inputs)\n",
    "answer_start = torch.argmax(answer_start_scores)  # Get the most likely beginning of answer with the argmax of the score\n",
    "answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "# 3. GET THE ANSWER SPAN\n",
    "# Once we have the most likely start and end tokens, we grab all the tokens between them\n",
    "# and convert tokens back to words!\n",
    "tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA on Wikipedia pages\n",
    "We tried our model on a question paired with a short passage. But what if we want to search for answers in longer documents? A typical Wikipedia page is much longer than the example above and we need to do a bit of massaging before we can use our model on longer contexts. \n",
    "\n",
    "Let's start by pulling up a Wikipedia page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia search results for our question:\n",
      "\n",
      "['Albatross',\n",
      " 'List of largest birds',\n",
      " 'Black-browed albatross',\n",
      " 'Argentavis',\n",
      " 'Pterosaur',\n",
      " 'Mollymawk',\n",
      " 'List of birds by flight speed',\n",
      " 'Largest body part',\n",
      " 'Pelican',\n",
      " 'Aspect ratio (aeronautics)']\n",
      "\n",
      "The Albatross Wikipedia article contains 38200 characters.\n"
     ]
    }
   ],
   "source": [
    "import wikipedia as wiki\n",
    "import pprint as pp\n",
    "\n",
    "question = 'What is the wingspan of an albatross?'\n",
    "\n",
    "results = wiki.search(question)\n",
    "print(\"Wikipedia search results for our question:\\n\")\n",
    "pp.pprint(results)\n",
    "\n",
    "page = wiki.page(results[0])\n",
    "text = page.content\n",
    "print(f\"\\nThe {results[0]} Wikipedia article contains {len(text)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This translates into 8824 tokens.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(question, text, return_tensors='pt')\n",
    "print(f\"This translates into {len(inputs['input_ids'][0])} tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer takes the input as text and returns tokens. In general, tokenizers convert words or pieces of words into a model-ingestable format. The specific tokens and format are dependent on the type of model. For example, BERT tokenizes words differently from RoBERTa, so be sure to always use the associated tokenizer appropriate for your model. \n",
    "\n",
    "In this case, the tokenizer converts our input text into 8824 tokens, but this far exceeds the maximum number of tokens that can be fed to the model at one time. Most BERT-esque models can only accept 512 tokens at once (thus the warning above). This means we'll have to split our input into chunks and each chunk must not exceed 512 tokens in total. \n",
    "\n",
    "When working with Question Answering, it's crucial that each chunk follows this format:\n",
    "\n",
    "[CLS] question tokens [SEP] context tokens [SEP]\n",
    "\n",
    "This means that, for each segment of a Wikipedia article, we must prepend the original question, followed by the next \"chunk\" of article tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question consists of 12 tokens.\n",
      "Each chunk will contain 497 tokens of the Wikipedia article.\n"
     ]
    }
   ],
   "source": [
    "# Time to chunk!\n",
    "from collections import OrderedDict\n",
    "\n",
    "# identify question tokens (token_type_ids = 0)\n",
    "qmask = inputs['token_type_ids'].lt(1)\n",
    "qt = torch.masked_select(inputs['input_ids'], qmask)\n",
    "print(f\"The question consists of {qt.size()[0]} tokens.\")\n",
    "\n",
    "chunk_size = model.config.max_position_embeddings - qt.size()[0] - 1 # the \"-1\" accounts for\n",
    "# having to add a [SEP] token to the end of each chunk\n",
    "print(f\"Each chunk will contain {chunk_size - 2} tokens of the Wikipedia article.\")\n",
    "\n",
    "# create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input\n",
    "chunked_input = OrderedDict()\n",
    "for k,v in inputs.items():\n",
    "    q = torch.masked_select(v, qmask)\n",
    "    c = torch.masked_select(v, ~qmask)\n",
    "    chunks = torch.split(c, chunk_size)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i not in chunked_input:\n",
    "            chunked_input[i] = {}\n",
    "\n",
    "        thing = torch.cat((q, chunk))\n",
    "        if i != len(chunks)-1:\n",
    "            if k == 'input_ids':\n",
    "                thing = torch.cat((thing, torch.tensor([102])))\n",
    "            else:\n",
    "                thing = torch.cat((thing, torch.tensor([1])))\n",
    "\n",
    "        chunked_input[i][k] = torch.unsqueeze(thing, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in chunk 0: 512\n",
      "Number of tokens in chunk 1: 512\n",
      "Number of tokens in chunk 2: 512\n",
      "Number of tokens in chunk 3: 512\n",
      "Number of tokens in chunk 4: 512\n",
      "Number of tokens in chunk 5: 512\n",
      "Number of tokens in chunk 6: 512\n",
      "Number of tokens in chunk 7: 512\n",
      "Number of tokens in chunk 8: 512\n",
      "Number of tokens in chunk 9: 512\n",
      "Number of tokens in chunk 10: 512\n",
      "Number of tokens in chunk 11: 512\n",
      "Number of tokens in chunk 12: 512\n",
      "Number of tokens in chunk 13: 512\n",
      "Number of tokens in chunk 14: 512\n",
      "Number of tokens in chunk 15: 512\n",
      "Number of tokens in chunk 16: 512\n",
      "Number of tokens in chunk 17: 341\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(chunked_input.keys())):\n",
    "    print(f\"Number of tokens in chunk {i}: {len(chunked_input[i]['input_ids'].tolist()[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these chunks (except for the last one) has the following structure: \n",
    "\n",
    "[CLS], 12 question tokens, [SEP], 497 tokens of the Wikipedia article, [SEP] token = 512 tokens\n",
    "\n",
    "Each of these chunks can now be fed to the model without causing indexing errors. We'll get an \"answer\" for each chunk, however not all answers are useful since not every segment of a Wikipedia article is informative for our question. The model will return the [CLS] token when it determines that the context does not contain an answer to the question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 . 7 m / \n"
     ]
    }
   ],
   "source": [
    "def convert_ids_to_string(tokenizer, input_ids):\n",
    "    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "\n",
    "answer = ''\n",
    "\n",
    "# Now we iterate over our chunks, looking for the best answer from each chunk\n",
    "for _, chunk in chunked_input.items():\n",
    "    answer_start_scores, answer_end_scores = model(**chunk)\n",
    "\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    ans = convert_ids_to_string(tokenizer, chunk['input_ids'][0][answer_start:answer_end])\n",
    "    \n",
    "    # If the ans == [CLS] then the model did not find a real answer in this chunk\n",
    "    if ans != '[CLS]':\n",
    "        answer += ans + \" / \"\n",
    "        \n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together\n",
    "\n",
    "Let's recap. We've essentially built a simple IR-based QA system! We're using `wikipedia`'s search engine to return a list of candidate documents that we then feed into our Document Reader (in this case, BERT fine-tuned on SQuAD 2.0).  Let's make our code easier to read and more self-contained by packaging the Document Reader into a class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "\n",
    "class DocumentReader:\n",
    "    def __init__(self, pretrained_model_name_or_path='bert-large-uncased'):\n",
    "        self.READER_PATH = pretrained_model_name_or_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.READER_PATH)\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(self.READER_PATH)\n",
    "        self.max_len = self.model.config.max_position_embeddings\n",
    "        self.chunked = False\n",
    "\n",
    "    def tokenize(self, question, text):\n",
    "        self.inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        self.input_ids = self.inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "        if len(self.input_ids) > self.max_len:\n",
    "            self.inputs = self.chunkify()\n",
    "            self.chunked = True\n",
    "\n",
    "    def chunkify(self):\n",
    "        \"\"\" \n",
    "        Break up a long article into chunks that fit within the max token\n",
    "        requirement for that Transformer model. \n",
    "\n",
    "        Calls to BERT / RoBERTa / ALBERT require the following format:\n",
    "        [CLS] question tokens [SEP] context tokens [SEP]\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: generalize this because not all models include token_type_ids (distilBERT)\n",
    "        # create question mask based on token_type_ids\n",
    "        # value is 0 for question tokens, 1 for context tokens\n",
    "        qmask = self.inputs['token_type_ids'].lt(1)\n",
    "        qt = torch.masked_select(self.inputs['input_ids'], qmask)\n",
    "        chunk_size = self.max_len - qt.size()[0] - 1 # the \"-1\" accounts for\n",
    "        # having to add an ending [SEP] token to the end\n",
    "\n",
    "        # create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input\n",
    "        chunked_input = OrderedDict()\n",
    "        for k,v in self.inputs.items():\n",
    "            q = torch.masked_select(v, qmask)\n",
    "            c = torch.masked_select(v, ~qmask)\n",
    "            chunks = torch.split(c, chunk_size)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if i not in chunked_input:\n",
    "                    chunked_input[i] = {}\n",
    "\n",
    "                thing = torch.cat((q, chunk))\n",
    "                if i != len(chunks)-1:\n",
    "                    if k == 'input_ids':\n",
    "                        thing = torch.cat((thing, torch.tensor([102])))\n",
    "                    else:\n",
    "                        thing = torch.cat((thing, torch.tensor([1])))\n",
    "\n",
    "                chunked_input[i][k] = torch.unsqueeze(thing, dim=0)\n",
    "        return chunked_input\n",
    "\n",
    "    def get_answer(self):\n",
    "        if self.chunked:\n",
    "            answer = ''\n",
    "            for k, chunk in self.inputs.items():\n",
    "                answer_start_scores, answer_end_scores = self.model(**chunk)\n",
    "\n",
    "                answer_start = torch.argmax(answer_start_scores)\n",
    "                answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "                ans = self.convert_ids_to_string(chunk['input_ids'][0][answer_start:answer_end])\n",
    "                if ans != '[CLS]':\n",
    "                    answer += ans + \" / \"\n",
    "            return answer\n",
    "        else:\n",
    "            answer_start_scores, answer_end_scores = self.model(**self.inputs)\n",
    "\n",
    "            answer_start = torch.argmax(answer_start_scores)  # Get the most likely beginning of answer with the argmax of the score\n",
    "            answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "        \n",
    "            return self.convert_ids_to_string(self.inputs['input_ids'][0][\n",
    "                                              answer_start:answer_end])\n",
    "\n",
    "    def convert_ids_to_string(self, input_ids):\n",
    "        return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's QA time!\n",
    "We have a fully working system now! Feel free to add your own questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-hide \n",
    "\n",
    "# To make the following output more readable I'll turn off the token sequence length warning\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When was Barack Obama born?\n",
      "Top wiki result: <WikipediaPage 'Barack Obama Sr.'>\n",
      "Answer: 18 June 1936 / February 2 , 1961 / \n",
      "\n",
      "Question: Why is the sky blue?\n",
      "Top wiki result: <WikipediaPage 'Diffuse sky radiation'>\n",
      "Answer: Rayleigh scattering / \n",
      "\n",
      "Question: How many sides does a pentagon have?\n",
      "Top wiki result: <WikipediaPage 'The Pentagon'>\n",
      "Answer: five / \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    'When was Barack Obama born?',\n",
    "    'Why is the sky blue?',\n",
    "    'How many sides does a pentagon have?'\n",
    "]\n",
    "\n",
    "reader = DocumentReader(\"deepset/bert-base-cased-squad2\") \n",
    "\n",
    "# if you trained your own model using the training cell earlier you can access it with this:\n",
    "#reader = DocumentReader(\"./models/bert/bbu_squad2\")\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    results = wiki.search(question)\n",
    "\n",
    "    page = wiki.page(results[0])\n",
    "    print(f\"Top wiki result: {page}\")\n",
    "\n",
    "    text = page.content\n",
    "\n",
    "    reader.tokenize(question, text)\n",
    "    print(f\"Answer: {reader.get_answer()}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got 2 out of 3 questions right! \n",
    "\n",
    "Notice that, at least for the current questions I've chosen, the QA systems fails not because of BERT but because of  wikipedia's default search engine! It pulls up the wrong page for two of our questions: giving us a page about Barack Obama Sr. instead of the former President, and an article about the US's Dept of Defense building The Pentagon instead of a page about geometry. In the latter we ended up with the correct answer anyway by coincidence! \n",
    "\n",
    "The best NLP model in the world can't fix these problems, which illustrates that any successful IR-based QA system requires a search engine (document retriever) as good as the document reader. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping Up\n",
    "\n",
    "There we have it! A working QA system on Wikipedia articles. This is great but it's admittedly not very sophisticated. Furthermore, I've left a lot of qustions unanswered:\n",
    "\n",
    "1. Why the SQuAD dataset and not something else? What other options are there? \n",
    "2. We used default hyperparameters to fine-tune BERT but could we have done better? How good can BERT get? And how do we define \"good\"?\n",
    "3. Why BERT and not another Transformer model? \n",
    "4. Currently, our QA system can return an answer for each chunk of a Wiki article but not all of those answers are correct -- How can we improve our `get_answer` method?\n",
    "5. Additionally, we're chunking a wiki article in such a way that we could be ending a chunk in the midde of a sentence -- Can we improve our `chunkify` method? \n",
    "\n",
    "\n",
    "Stay tuned for future posts as we'll tackle these questions and more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
