{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "> First steps towards building an IR-based QA system with BERT.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- hide: true\n",
    "- permalink: /hidden/\n",
    "- search_exclude: false\n",
    "- categories: [jupyter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So you've decided to build a QA system. \n",
    "You want to start with something general and straightforward so you plan to make it open domain using Wikipedia as a corpus for answering questions. You're going to use an IR-based design (see previous post) since you're working with a large collection of unstructured text. You want to use the best NLP that your compute resources allow (you're lucky enough to have access to a GPU) so you're going to focus on the big, flashy Transformer models that are all the rage these days. \n",
    "\n",
    "Sounds like a plan! So where do you start? \n",
    "\n",
    "This was our thought process when we first set out on this research path and in this post we'll discuss what you need to know to get going!\n",
    "\n",
    "- installing libraries and setting up an environment\n",
    "- understanding Huggingface's `run_squad.py` training script\n",
    "- Understanding the basic ins and outs of a BERT-esque model\n",
    "- getting BERT to accept a full Wikipedia article as context for a question\n",
    "\n",
    "\n",
    "### Setting up your virtual environment\n",
    "A virtual environment is always best practice and we're using `venv` (though Melanie is also partial to `conda`). Here's the bare minimum that you'll need to do what I did. For this project we'll be using Pytorch (though everything we do can also be accomplished in Tensorflow). Pytorch handles the heavy lifting of deep differentiable learning. Transformers is a library by Huggingface that provides super easy to use implementations (in torch) of all the popular Transformer architectures (more on this later). \n",
    "\n",
    "- PyTorch \n",
    "- Transformers\n",
    "- Wikipedia\n",
    "- TensorboardX (optional)\n",
    "\n",
    "A note on PyTorch: our GPU machine sports an older version of CUDA (9.2) that we're getting around to updating... In the meantime, this forces us to use an older version of PyTorch that supports this CUDA version in order to access our GPU for training. Some older verisons of PyTorch might require that you also install `TensorboardX` which is used in Huggingface's `run_squad.py` script. If you have want to use your GPU (pretty much required if you plan to _fine tune_ BERT on the SQuAD dataset) and you have CUDA 10+ you can use the most recent version of Pytorch and you won't need to install the additional TensorboardX package. \n",
    "\n",
    "Why are we using PyTorch instead of Tensorflow? Honestly? Because Tensorflow isn't playing nice with our GPU machine these days... You'll likely see some warning messages about it not being install properly in order to access the GPU. It's on our To Do list. \n",
    "\n",
    "### Huggingface's Transformer library and training script\n",
    "I'm new to PyTorch and Huggingface but I'm quickly becoming a convert!  Huggingface provides state-of-the-art general-purpose architecures for natural language understanding and natural language generation. They have tons of pre-trained models that work in dozens of languages. They even have interoperability between PyTorch and Tensorflow (all camps welcome!) which means if someone trained BERT using Tensorflow we can load those pre-trained weights through Huggingface methods and it will convert the weights to PyTorch for us! Yay. \n",
    "\n",
    "Huggingface provide more than just pre-trained models. They also have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qasystem import DocumentReader, MODEL_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default_bert_base_uncased': 'bert-base-uncased',\n",
       " 'bert_base_uncased_squad1': '/home/ryan/work/ff14/src/models/bert/bert-base-uncased-tuned-squad-1.0',\n",
       " 'bert_base_cased_squad2': '/home/ryan/work/ff14/src/models/bert/bert-base-cased-tuned-squad-2.0/'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = DocumentReader(MODEL_PATHS['bert_base_uncased_squad1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<qasystem.DocumentReader at 0x7f3141261e48>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
