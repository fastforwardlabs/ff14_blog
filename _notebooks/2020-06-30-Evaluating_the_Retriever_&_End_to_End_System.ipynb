{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl0KyQomBQ7l"
   },
   "source": [
    "# Evaluating the Retriever & End-to-End System\n",
    "> A review of Information Retrieval and the role it plays in a QA system\n",
    "\n",
    "- title: \"Evaluating the Retriever & End-to-End System\"\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- hide: true\n",
    "- permalink: /hidden/\n",
    "- search_exclude: false\n",
    "- use_math: true\n",
    "- categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Np3wkqy7lTrb"
   },
   "source": [
    "In our last post, [Evaluating QA: Metrics, Predictions, and the Null Response](https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html), we took a deep dive look at how to asses the quality of a BERT-like Reader for Question Answering (QA) using the Hugging Face framework. In this post, we'll focus on the former component of an end-to-end QA system - the Retriever. Specifically, we'll introduce Elasticsearch as a powerful and efficient Information Retrieval (IR) tool that can be used to scour through large corpora and retrieve relevant documents. Through the post, we'll explain how to implement and evaluate a Retriever in the context of Question Answering and demonstrate the impact it has on an end-to-end QA system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "np9SZdgqBQ7m"
   },
   "source": [
    "### Prerequisites\n",
    "* a basic understanding of Information Retrieval & Search\n",
    "* a basic understanding of IR based QA systems (see previous posts)\n",
    "* a basic understanding of Transformers and PyTorch\n",
    "* a basic understanding of the SQuAD2.0 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pWWsT7nlTrc"
   },
   "source": [
    "# Retrieving the right document is important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-q2l9AkZlTrd"
   },
   "source": [
    "![](https://github.com/fastforwardlabs/ff14_blog/blob/master/_notebooks/my_icons/michael_scott_quote.jpg?raw=1)\n",
    "\n",
    "\n",
    "We believe what Michael Scott really meant to say is:\n",
    "\n",
    "> \"***You miss 100% of the questions if the answer doesn't appear in the input context***\"\n",
    "\n",
    "As we have discussed throughout this blog series, many modern QA systems take a two-staged approach to answering questions. In the first stage, a document retriever selects *N* potentially relevant documents from a given corpus. Subsequently, a machine comprehension model processes each of the *N* documents to determine an answer to the input question. Because of recent advances in NLP and deep learning (i.e. flashy Transformer models), the machine comprehension component of question answering has typically been the main focus of evaluation. Stage one of these systems has recieved limited attention despite its obvious importance...stage two is bounded by performance at stage one. Let's get more specific.\n",
    "\n",
    "We [recently explained methods](https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html) that enable BERT-like models to produce robust answers given a question and context passage by selectively processing predictions and by refraining from answering certain questions at all. While the ability to properly comprehend a passage and produce a correct answer is a critical feature of any QA tool, the success of the overall system is highly dependent on first providing a correct passage to read through. Without being fed a context passage that actually contains the ground-truth answer, the overall system's performance is limited to how well it can predict no-answer questions. To demonstrate, we'll revisit an example from our [second blog post](https://qa.fastforwardlabs.com/pytorch/hugging%20face/wikipedia/bert/transformers/2020/05/19/Getting_Started_with_QA.html) where three questions were asked of the Wikipedia search engine based QA system:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73yug9x4lTrd"
   },
   "source": [
    "```\n",
    "**Example 1: Incorrect**\n",
    "Question: When was Barack Obama born?\n",
    "Top wiki result: <WikipediaPage 'Barack Obama Sr.'>\n",
    "Answer: 18 June 1936 / February 2 , 1961 / \n",
    "\n",
    "**Example 2: Correct**\n",
    "Question: Why is the sky blue?\n",
    "Top wiki result: <WikipediaPage 'Diffuse sky radiation'>\n",
    "Answer: Rayleigh scattering / \n",
    "\n",
    "**Example 3: Correct**\n",
    "Question: How many sides does a pentagon have?\n",
    "Top wiki result: <WikipediaPage 'The Pentagon'>\n",
    "Answer: five / \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YL4_stz3lTre"
   },
   "source": [
    "In Example 1, the Reader had no chance of producing the correct answer because of its outright absence from the context served up by the Retriever. Namely, the Retriever erroneously provided a page about Barack Obama Sr. instead of his son, the former US President. In this case, the only way the Reader could have possibly produced the correct answer was if the correct answer was actually not to answer at all. On the flip side, in Example 3, the Retriever did not identify the globally \"correct\" document - it returned an article about \"The Pentagon\" instead of a page about geometry - but nonetheless, it provided enough context for the Reader to succeed.\n",
    "\n",
    "These quick examples illustrate why an effective Retriever is crucial for an end-to-end QA system. Now let's take a deeper look at a classic tool used for information retrieval - Elasticsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sa6UyWz0lTre"
   },
   "source": [
    "# Elasticsearch as an IR Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1MjzT9zlTrf"
   },
   "source": [
    "![](https://github.com/fastforwardlabs/ff14_blog/blob/master/_notebooks/my_icons/elasticsearch-logo.png?raw=1)\n",
    "\n",
    "Modern QA systems employ a variety of techniques for the task of information retrieval ranging from traditional sparse vector word matching (ex. Elasticsearch) to [novel approaches](https://arxiv.org/pdf/2004.04906.pdf) using dense representations of encoded passages combined with [efficient search capabilities](https://github.com/facebookresearch/faiss). Despite the flurry of contemporary research efforts in this area, the traditional sparse vector approach performs very well overall and has only recently been overtaken by embedding-based systems for end-to-end QA retrieval tasks. For that reason, we'll explore Elasticsearch as an easy to use framework for document retrieval. So, what exactly is Elasticsearch?\n",
    "\n",
    "Elasticsearch is a powerful open-source search and analytics engine built on the [Apache Lucene](https://lucene.apache.org/) library that is capable of handling all types of data including textual, numerical, geospatial, structrured, and unstructured. It is built to scale with a robust set of features, rich ecosystem, and diverse list of client libraries making it easy to integrate and use. In the context of information retrieval for automated question answering, we are keenly interested in the features surrounding full-text search. Elasticsearch provides a convenient way to index documents so they can quickly be queried for nearest neighbor search using a TF-IDF based similarity metric. Specifically, it uses [BM25](https://opensourceconnections.com/blog/2015/10/16/bm25-the-next-generation-of-lucene-relevation/) term weighting to represent question and context passages as high-dimensional, sparse vectors that are efficiently searched in an inverted index. For more information on how an inverted index works under the hood, we recommend this quick and concise [blog post](https://codingexplained.com/coding/elasticsearch/understanding-the-inverted-index-in-elasticsearch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aGSmeXNtlTrh"
   },
   "source": [
    "## Using Elasticsearch with SQuAD2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KxL9yAI1lTri"
   },
   "source": [
    "With this basic understanding of how Elasticsearch works, let's dive in and build our own Document Retrieval system by indexing a set of Wikipedia article paragraphs that support questions and answers from the SQuAD2.0 dataset. Before we get started, we'll need to download and prepare data from SQuAD2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IiwrK5s6lTri"
   },
   "source": [
    "### Download and Prepare SQUAD2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "LR4iRtkKlTrj",
    "outputId": "45e3845e-dd88-40d1-fa3a-feb54038347a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "\n",
    "# Download the SQuAD2.0 train & dev sets\n",
    "!wget -P data/squad/ https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "!wget -P data/squad/ https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
    "    \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WSxO51NglTrn"
   },
   "source": [
    "A common practice in IR for QA is to segment large articles into smaller passages before indexing for two main reasons:\n",
    "\n",
    "1. **Transformer based Readers are very slow** - providing an entire Wikipedia article to BERT for processing takes a considerable amount of time and also defeats the purpose of using an IR tool to narrow the search space for the Reader.\n",
    "2. **Smaller passages reduce noise** - by identifying a more concise context passage for BERT to read through, we reduce the chance of BERT getting lost.\n",
    "\n",
    "Of course the chunking method proposed here doesn't come without a cost. Larger document size means each document contains more information. By reducing passage size, we are potentially trading off system recall for speed - though there are techniques to alleviate this as we will disucss later in the post.\n",
    "\n",
    "With our chunking approach, each article paragraph will be prepended with the article title and collectively serve as a corpus of documents for which our Elasticsearch Retriever will search over. In practice, open-domain QA systems sit atop massive collections of documents (think all of Wikipedia) to provide a breadth of information to answer general-knowledge questions from. For the purposes of demonstrating Elasticsearch functionality, we will limit our corpus to only the Wikipedia articles supporting SQuAD2.0 questions.\n",
    "\n",
    "The following `parse_qa_records` function will extract question/answer examples, as well as paragraph content from the SQuAD2.0 data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FNYE9t_5lTrn"
   },
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "\n",
    "def parse_qa_records(data):\n",
    "    '''\n",
    "    Loop through SQuAD2.0 dataset and parse out question/answer examples and unique article paragraphs\n",
    "    \n",
    "    Returns:\n",
    "        qa_records (list) - Question/answer examples as list of dictionaries\n",
    "        wiki_articles (list) - Unique Wikipedia titles and article paragraphs recreated from SQuAD data\n",
    "    \n",
    "    '''\n",
    "    num_with_ans = 0\n",
    "    num_without_ans = 0\n",
    "    qa_records = []\n",
    "    wiki_articles = {}\n",
    "    \n",
    "    for article in data:\n",
    "        \n",
    "        for i, paragraph in enumerate(article['paragraphs']):\n",
    "            \n",
    "            wiki_articles[article['title']+f'_{i}'] = article['title'] + ' ' + paragraph['context']\n",
    "            \n",
    "            for questions in paragraph['qas']:\n",
    "                \n",
    "                qa_record = {}\n",
    "                qa_record['example_id'] = questions['id']\n",
    "                qa_record['document_title'] = article['title']\n",
    "                qa_record['question_text'] = questions['question']\n",
    "                \n",
    "                try: \n",
    "                    qa_record['short_answer'] = questions['answers'][0]['text']\n",
    "                    num_with_ans += 1\n",
    "                except:\n",
    "                    qa_record['short_answer'] = \"\"\n",
    "                    num_without_ans += 1\n",
    "                    \n",
    "                qa_records.append(qa_record)\n",
    "        \n",
    "        \n",
    "    wiki_articles = [{'document_title':title, 'document_text': text}\\\n",
    "                         for title, text in wiki_articles.items()]\n",
    "                \n",
    "    print(f'Data contains {num_with_ans} question/answer pairs with a short answer, and {num_without_ans} without.'+\n",
    "          f'\\nThere are {len(wiki_articles)} unique wikipedia article paragraphs.')\n",
    "                \n",
    "    return qa_records, wiki_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "dZHrwJBblTrq",
    "outputId": "5be3286a-0b51-4075-a027-b45deba70a4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data contains 86821 question/answer pairs with a short answer, and 43498 without.\n",
      "There are 19035 unique wikipedia article paragraphs.\n",
      "Data contains 5928 question/answer pairs with a short answer, and 5945 without.\n",
      "There are 1204 unique wikipedia article paragraphs.\n"
     ]
    }
   ],
   "source": [
    "# load and parse data\n",
    "train_file = \"data/squad/train-v2.0.json\"\n",
    "dev_file = \"data/squad/dev-v2.0.json\"\n",
    "\n",
    "train = json.load(open(train_file, 'rb'))\n",
    "dev = json.load(open(dev_file, 'rb'))\n",
    "\n",
    "qa_records, wiki_articles = parse_qa_records(train['data'])\n",
    "qa_records_dev, wiki_articles_dev = parse_qa_records(dev['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fyk_WZn6lTrt",
    "outputId": "3d23397f-830d-47e3-8649-9591e59ab1b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_id': '56d43c5f2ccc5a1400d830ab',\n",
       " 'document_title': 'BeyoncÃ©',\n",
       " 'question_text': 'What was the first album BeyoncÃ© released as a solo artist?',\n",
       " 'short_answer': 'Dangerously in Love'}"
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsed record example\n",
    "qa_records[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJkzpTrElTry",
    "outputId": "f495bb9a-d411-47aa-9e0d-48a0e8e9cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document_title': 'BeyoncÃ©_10', 'document_text': 'BeyoncÃ© BeyoncÃ©\\'s first solo recording was a feature on Jay Z\\'s \"\\'03 Bonnie & Clyde\" that was released in October 2002, peaking at number four on the U.S. Billboard Hot 100 chart. Her first solo album Dangerously in Love was released on June 24, 2003, after Michelle Williams and Kelly Rowland had released their solo efforts. The album sold 317,000 copies in its first week, debuted atop the Billboard 200, and has since sold 11 million copies worldwide. The album\\'s lead single, \"Crazy in Love\", featuring Jay Z, became BeyoncÃ©\\'s first number-one single as a solo artist in the US. The single \"Baby Boy\" also reached number one, and singles, \"Me, Myself and I\" and \"Naughty Girl\", both reached the top-five. The album earned BeyoncÃ© a then record-tying five awards at the 46th Annual Grammy Awards; Best Contemporary R&B Album, Best Female R&B Vocal Performance for \"Dangerously in Love 2\", Best R&B Song and Best Rap/Sung Collaboration for \"Crazy in Love\", and Best R&B Performance by a Duo or Group with Vocals for \"The Closer I Get to You\" with Luther Vandross.'}\n"
     ]
    }
   ],
   "source": [
    "# Parsed wiki paragraph example\n",
    "print(wiki_articles[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OahFS1vhlTr0"
   },
   "source": [
    "### Download Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_nOLSOXlTr0"
   },
   "source": [
    "With our data ready to go, let's download, install, and configure Elasticsearch. We recommend running this notebook in Colab and executing the following code snippet for automatic setup. However, if you prefer to run locally, follow the instructions [here](https://www.elastic.co/downloads/elasticsearch). After completing the setup, we will have an Elasticsearch service running locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHmyF5RklTr1"
   },
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "\n",
    "# If using Colab - Start Elasticsearch from source\n",
    "! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\n",
    "! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\n",
    "! chown -R daemon:daemon elasticsearch-7.6.2\n",
    "\n",
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'],\n",
    "                   stdout=PIPE, stderr=STDOUT,\n",
    "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
    "                  )\n",
    "# wait until ES has started\n",
    "! sleep 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wy8Tzay-lTr3"
   },
   "source": [
    "### Getting Data into Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9cespYnhlTr4"
   },
   "source": [
    "We'll use the [official low-level Python client library](https://elasticsearch-py.readthedocs.io/en/master/) for interacting with Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "BMIa-bJHlTr4",
    "outputId": "95a011a7-4970-42ba-9d66-5f9b5581a87a"
   },
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "!pip install elasticsearch\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bpb2NmWClTr6"
   },
   "source": [
    "By default, Elasticsearch is launched locally on port 9200. We first need to instantiate an Elasticsearch client object and connect to the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DXvAmLwllTr7",
    "outputId": "dc593d7b-264e-468c-ff7e-77b18a4fc047"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "config = {'host':'localhost', 'port':9200}\n",
    "es = Elasticsearch([config])\n",
    "\n",
    "# test connection\n",
    "es.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fLI_1deTlTr9"
   },
   "source": [
    "Before we go further, let's introduce a few concepts that are specific to Elasticsearch and the process of indexing data. In Elasticsearch, an ***index*** is a collection of documents that have common characteristics (similar to a database schema in an RDBMS). ***Documents*** are JSON objects having their own set of key-value pairs consisting of various data types (similar to rows/fields in RDBMS). When we add a document into an index, the value for the document's text fields go through an analysis process prior to being indexed. This means that when executing a search query against an existing index, we are actually searching against the post-processed representation that is stored in the inverted index, not the raw input document itself.\n",
    "\n",
    "![Elasticsearch Index Process](https://github.com/fastforwardlabs/ff14_blog/blob/master/_notebooks/my_icons/elastic_index_process.png?raw=1)\n",
    "[Image Credit](https://codingexplained.com/coding/elasticsearch/understanding-analysis-in-elasticsearch-analyzers#:~:text=A%20Closer%20Look%20at%20Analyzers,documents%20when%20they%20are%20indexed.&text=An%20analyzer%20consists%20of%20three,them%20changing%20the%20input%20stream.)\n",
    "\n",
    "The anaysis process is a customizable pipeline carried out by a dedicated ***Analyzer***. Elasticsearch analyzers are composed of three sequential steps that form a processing pipeline: *character filters, a tokenizer, and token filters.* Each of these components modify the input stream of text according to some configurable settings. \n",
    "- **Character Filters:** First, character filters have the ability to add, remove, or replace specific items in the text field. A common application of this filter is to strip `html` markup from the raw input. \n",
    "- **Tokenizer:** After applying character filters, the transformed text is then passed to a tokenizer which breaks up the input string into individual tokens with a provided strategy. By default, the `standard` tokenizer splits tokens whenever it encounters a whitespace character, and also splits on most symbols (like commas, periods, semicolons, etc.)\n",
    "- **Token Filters:** Finally, the token stream is passed to a token filter which acts to add, remove, or modify tokens. Typical token filters include `lowercase` which converts all tokens to lowercase form, and `stop` which removes commonly occuring tokens called *stopwords*. \n",
    "\n",
    "Elasticsearch comes with several built-in Analyzers that satisfy common use cases and defaults to the `Standard Analyzer`. The Standard Analyzer doesn't contain any character filters, uses a `standard` tokenizer, and applies a `lowercase` token filter. Let's take a look at [this example](https://medium.com/elasticsearch/introduction-to-analysis-and-analyzers-in-elasticsearch-4cf24d49ddab) sentence as its passed through this pipeline:\n",
    "\n",
    "> \"I'm in the mood for drinking semi-dry red wine!\"\n",
    "\n",
    "![Elasticsearch Analyzer Pipeline](https://github.com/fastforwardlabs/ff14_blog/blob/master/_notebooks/my_icons/elasticsearch_standard_analyzer.png?raw=1)\n",
    "[Image Credit](https://codingexplained.com/coding/elasticsearch/understanding-analysis-in-elasticsearch-analyzers#:~:text=A%20Closer%20Look%20at%20Analyzers,documents%20when%20they%20are%20indexed.&text=An%20analyzer%20consists%20of%20three,them%20changing%20the%20input%20stream.)\n",
    "\n",
    "Crafting analyzers to your use case requires domain knowledge of the problem and dataset at hand, and doing so properly is key to optimizing relevance scoring for your search application. We found [this blog series](https://medium.com/elasticsearch/introduction-to-analysis-and-analyzers-in-elasticsearch-4cf24d49ddab) very useful in explaining the importance of analysis in Elasticsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_oaKhW1flTr9"
   },
   "source": [
    "#### Create an Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrlsaFgslTr-"
   },
   "source": [
    "Let's create a new index and add our Wikipedia articles to it. To do so, we provide a name and optionally some index configurations. Here we are specifying a set of `mappings` that indicate our anticipated index schema, data types, and how the text fields should be processed. If no `body` is passed, Elasticsearch will automatically infer fields and data types from incoming documents, as well as apply the `Standard Analyzer` to any text fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "vu6imTr6lTr-",
    "outputId": "6bfb7bfe-db9a-4ad5-878c-b23a56fc5960"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'index': 'squad-standard-index',\n",
       " 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_config = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"standard_analyzer\": {\n",
    "                    \"type\": \"standard\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"strict\", \n",
    "        \"properties\": {\n",
    "            \"document_title\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"},\n",
    "            \"document_text\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "index_name = 'squad-standard-index'\n",
    "es.indices.create(index=index_name, body=index_config, ignore=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3vqk73V3lTsB"
   },
   "source": [
    "#### Populate the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A11K4dsPlTsB"
   },
   "source": [
    "We can then loop through our list of Wikipedia titles & articles and add them to our newly created Elasticsearch index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pnd12zSHlTsB"
   },
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def populate_index(es_obj, index_name, evidence_corpus):\n",
    "    '''\n",
    "    Loads records into an existing Elasticsearch index\n",
    "\n",
    "    Args:\n",
    "        es_obj (elasticsearch.client.Elasticsearch) - Elasticsearch client object\n",
    "        index_name (str) - Name of index\n",
    "        evidence_corpus (list) - List of dicts containing data records\n",
    "\n",
    "    '''\n",
    "\n",
    "    for i, rec in enumerate(tqdm(evidence_corpus)):\n",
    "    \n",
    "        try:\n",
    "            index_status = es_obj.index(index=index_name, id=i, body=rec)\n",
    "        except:\n",
    "            print(f'Unable to load document {i}.')\n",
    "            \n",
    "    n_records = es_obj.count(index=index_name)['count']\n",
    "    print(f'Succesfully loaded {n_records} into {index_name}')\n",
    "\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "a493a4c07a2743899571330cf6476b74",
      "e94a6f98578743c096c9b045d9dfdf81",
      "28a305d13e584615b9ba3cd9a37bdd56",
      "a299f8fa902348b7807b4d97ebc6027d",
      "a44c2693aee14b89a4c6512c85142f51",
      "a8cebe4ccf004d84bdc37ce44d1192e8",
      "b04d963baf9a40789305d1372ffe1aab",
      "dca5b0a8c1014a66bc8c5ba988878a85"
     ]
    },
    "colab_type": "code",
    "id": "k44Drj_HlTsE",
    "outputId": "7e54768d-c113-4a39-ed76-b37a5b4d3578"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a493a4c07a2743899571330cf6476b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20239.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Succesfully loaded 20239 into squad-standard-index\n"
     ]
    }
   ],
   "source": [
    "all_wiki_articles = wiki_articles + wiki_articles_dev\n",
    "\n",
    "populate_index(es_obj=es, index_name='squad-standard-index', evidence_corpus=all_wiki_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hqCpXHL6lTsG"
   },
   "source": [
    "#### Search the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KsLqs2THlTsH"
   },
   "source": [
    "Wahoo! We now have some documents loaded into into an index. Elasticsearch provides a rich [query language](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html) that supports a diverse range of query types. For this example, we'll use the standard query for performing full-text search called a `match` query. By default, Elasticsearch sorts and returns a JSON reponse of search results based on a computed a [relevance score](https://qbox.io/blog/practical-guide-elasticsearch-scoring-relevancy#:~:text=Together%2C%20these%20combine%20into%20a,number%20known%20as%20the%20_score.) which indicates how well a given document matches the query. Along with the relevance score of each matched document, the search response also includes the amount of time the query took to run.\n",
    "\n",
    "Let's look at a simple `match` query used to search the `document_text` field in our newly created index.\n",
    "\n",
    "> Important: As previously mentioned, all documents in the index have gone through an analysis process prior to indexing - this is called *index time analysis*. To maintain consistency in matching text queries against the post-processed index tokens, the same Analyzer used on a given field at index time is automatically applied to the query text at search time. *Search time analysis* is applied depending on which query type is used - `match` queries apply search time analysis by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UfbzT03mlTsH"
   },
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "def search_es(es_obj, index_name, question_text, n_results):\n",
    "    '''\n",
    "    Execute an Elasticsearch query on a specified index\n",
    "    \n",
    "    Args:\n",
    "        es_obj (elasticsearch.client.Elasticsearch) - Elasticsearch client object\n",
    "        index_name (str) - Name of index to query\n",
    "        query (dict) - Query DSL\n",
    "        n_results (int) - Number of results to return\n",
    "        \n",
    "    Returns\n",
    "        res - Elasticsearch response object\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # construct query\n",
    "    query = {\n",
    "            'query': {\n",
    "                'match': {\n",
    "                    'document_text': question_text\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    res = es_obj.search(index=index_name, body=query, size=n_results)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1pePxDFlTsK"
   },
   "outputs": [],
   "source": [
    "question_text = 'Who was the first president of the Republic of China?'\n",
    "\n",
    "# execute query\n",
    "res = search_es(es_obj=es, index_name='squad-standard-index', question_text=question_text, n_results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "vEw77SallTsM",
    "outputId": "567623ea-649d-4d66-cd93-9a3cde84c28a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was the first president of the Republic of China?\n",
      "Query Duration: 74 milliseconds\n",
      "Title, Relevance Score:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Modern_history_54', 23.131157),\n",
       " ('Nanjing_18', 17.076923),\n",
       " ('Republic_of_the_Congo_10', 16.840765),\n",
       " ('Prime_minister_16', 16.137493),\n",
       " ('Korean_War_29', 15.801523),\n",
       " ('Korean_War_43', 15.586578),\n",
       " ('Qing_dynasty_52', 15.291815),\n",
       " ('Chinese_characters_55', 14.773873),\n",
       " ('Korean_War_23', 14.736045),\n",
       " ('2008_Sichuan_earthquake_48', 14.417962)]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Question: {question_text}')\n",
    "print(f'Query Duration: {res[\"took\"]} milliseconds')\n",
    "print('Title, Relevance Score:')\n",
    "[(hit['_source']['document_title'], hit['_score']) for hit in res['hits']['hits']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JloR3u_nlTsO"
   },
   "source": [
    "# Evaluating Retriever Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UsM5mdnHlTsP"
   },
   "source": [
    "Ok, so we now have a basic understanding of how to use Elasticsearch as an IR tool to return some results for a given question, but how do we know if it's working? How do we evaluate what a good IR tool looks like?\n",
    "\n",
    "To evaluate how well our Retriever is working, we'll need two things: some labeled examples (i.e. SQuAD2.0 question/answer pairs) and some performance metrics. In the conventional world of information retrieval, there are [many metrics](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)) used to quantify the relevance of query results - largely centered around the concepts of precision and recall. For IR in the context of question answering, we adapt some of these ideas into two commonly used evaluation metrics: *recall* and *mean average precision (mAP)*. Additionally, we evaluate the amount of time required to execute a query since the main point of having a two stage QA system is to efficiently narrow the large search space for our Reader.\n",
    "\n",
    "#### Recall\n",
    "\n",
    "Traditionally, recall in IR indicates the fraction of all relevant documents that are retrieved. In the context of end-to-end QA systems sitting on large corpora of documents, we are less concerned with finding *all* of the passages containing the answer (because it would take significant time to read through all of them anyway) and more concerned with the binary presence of a passage containing the correct answer being returned. In that light, we define a Retriever's recall across a set of questions as *the percentage of questions for which the answer segment appears in one of the top N pages returned by the search method.*\n",
    "\n",
    "#### Mean Average Precision\n",
    "\n",
    "While the *recall* metric focuses on the minimum viable result set to enable a Reader for success, we do still care about the composition of that result set. We want a metric that rewards a Retriever for a.) returning a lot of answer-containing documents in the result set (i.e. traditional meaning of precision) and b.) returning those answer-containing documents higher up in the result set than non-answer-containing documents (i.e. ranking them correctly). This is precisely (ðŸ™ƒ) what mean average precision (mAP) does for us. \n",
    "\n",
    "To explain mAP further, let's first break down the concept of average precision for information retrieval. If our Retriever is asked to return N documents and the total number of those N documents that actually contains the answer is m, then average precision (AP) is defined as:\n",
    "\n",
    "![](https://github.com/fastforwardlabs/ff14_blog/blob/master/_notebooks/my_icons/map_equation.png?raw=1)\n",
    "\n",
    "where *rel(k)* is just a binary indication of whether the *kth* passage contains the correct answer segment or not. Using a concrete example, consider retrieving *N=3* documents, of which one actually contains the correct answer segment. Here are three scenarios for how this could happen:\n",
    "\n",
    "![](https://github.com/fastforwardlabs/ff14_blog/blob/master/_notebooks/my_icons/map_example.png?raw=1)\n",
    "    \n",
    "Despite the fact that in each scenario we only have one document containing the correct answer, Scenario A is rewarded with the highest score because it was able to correctly rank the ground truth document relative to the others returned. Since average precision is calculated on a per query basis, the mean average precision is simply just *the average AP across all queries*. \n",
    "\n",
    "Now using our Wikipedia passage index, let's define a function called `evaluate_retriever` to loop through all quesion/answer examples from the SQuAD2.0 train set and see how well our Elasticsearch retriever peforms in terms of recall, mAP, and averege query duration when retrieving *N=3* passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yiakwwmklTsP"
   },
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def average_precision(binary_results):\n",
    "    \n",
    "    ''' Calculates the average precision for a list of binary indicators '''\n",
    "    \n",
    "    m = 0\n",
    "    precs = []\n",
    "\n",
    "    for i, val in enumerate(binary_results):\n",
    "        if val == 1:\n",
    "            m += 1\n",
    "            precs.append(sum(binary_results[:i+1])/(i+1))\n",
    "            \n",
    "    ap = (1/m)*np.sum(precs) if m else 0\n",
    "            \n",
    "    return ap\n",
    "\n",
    "\n",
    "def evaluate_retriever(es_obj, index_name, qa_records, n_results):\n",
    "    '''\n",
    "    This function loops through a set of question/answer examples from SQuAD2.0 and \n",
    "    evaluates Elasticsearch as a information retrieval tool in terms of recall, mAP, and query duration.\n",
    "    \n",
    "    Args:\n",
    "        es_obj (elasticsearch.client.Elasticsearch) - Elasticsearch client object\n",
    "        index_name (str) - name of index to query\n",
    "        qa_records (list) - list of qa_records from preprocessing steps\n",
    "        n_results (int) - the number of results ElasticSearch should return for a given query\n",
    "        \n",
    "    Returns:\n",
    "        test_results_df (pd.DataFrame) - a dataframe recording search results info for every example in qa_records\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, qa in enumerate(tqdm(qa_records)):\n",
    "        \n",
    "        ex_id = qa['example_id']\n",
    "        question = qa['question_text']\n",
    "        answer = qa['short_answer']\n",
    "        \n",
    "        # construct and execute query\n",
    "        query = {\n",
    "                'query': {\n",
    "                    'match': {\n",
    "                        'document_text': question\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "        \n",
    "        res = search_es(es_obj=es_obj, index_name=index_name, query=query, n_results=n_results)\n",
    "        \n",
    "        # calculate performance metrics from query response info\n",
    "        duration = res['took']\n",
    "        binary_results = [int(answer.lower() in doc['_source']['document_text'].lower()) for doc in res['hits']['hits']]\n",
    "        ans_in_res = int(any(binary_results))\n",
    "        ap = average_precision(binary_results)\n",
    "\n",
    "        rec = (ex_id, question, answer, duration, ans_in_res, ap)\n",
    "        results.append(rec)\n",
    "    \n",
    "    # format results dataframe\n",
    "    cols = ['example_id', 'question', 'answer', 'query_duration', 'answer_present', 'average_precision']\n",
    "    results_df = pd.DataFrame(results, columns=cols)\n",
    "    \n",
    "    # format results dict\n",
    "    metrics = {'Recall': results_df.answer_present.value_counts(normalize=True)[1],\n",
    "               'Mean Average Precision': results_df.average_precision.mean(),\n",
    "               'Average Query Duration':results_df.query_duration.mean()}\n",
    "    \n",
    "    return results_df, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "c2e8a34b484b47eca6cd2010052a6b44"
     ]
    },
    "colab_type": "code",
    "id": "N9jLAXDdlTsR",
    "outputId": "7c4aa880-a287-47a2-d6e9-ca03adbaa034"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e8a34b484b47eca6cd2010052a6b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=92749.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# combine train/dev examples and filter out SQuAD records that\n",
    "# do not have a short answer for the given question\n",
    "all_qa_records = qa_records+qa_records_dev\n",
    "qa_records_answerable = [record for record in all_qa_records if record['short_answer'] != '']\n",
    "\n",
    "# run evaluation\n",
    "results_df, metrics = evaluate_retriever(es_obj=es, index_name='squad-standard-index', qa_records=qa_records_answerable, n_results=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tOxeFQqFlTsT",
    "outputId": "6a8bf424-bb46-447d-b386-c1d7b9be2146"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Recall': 0.8226180336176131,\n",
       " 'Mean Average Precision': 0.7524133234140888,\n",
       " 'Average Query Duration': 3.0550841518506937}"
      ]
     },
     "execution_count": 135,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hU2Klvn-lTsU"
   },
   "source": [
    "# Improving Search Results with a Custom Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-8WrpnZlTsV"
   },
   "source": [
    "Identifying a correct passage in the Top 3 results for 82% of the SQuAD questions in ~3 milliseconds per question is not too bad! But that means that we've effectively limited our overall QA system to an 82% upper bound on performance. How can we improve upon this?\n",
    "\n",
    "One simple and obvious way to increase recall would be to just retrieve more passages. The following figure shows the effects of varying corpus size and result size on Elasticsearch retriever recall. As expected we see that the number of passages retrieved (i.e. *Top N*) has a dramatic impact on recall; a ~10-15 point jump from 1 to 3 passages returned, and ~5 point jump for each of the other tiers. We also see a gradual decrease in recall as corpus size increases, which isn't surprising.\n",
    "\n",
    "![Recall vs. Corpus Size](https://github.com/fastforwardlabs/ff14_blog/blob/master/_notebooks/my_icons/recall_v_corpussize.png?raw=1 \"Experimental results demonstrating the impact of increasing corpus size and number of results retrieved on Elasticsearch recall.\")\n",
    "\n",
    "While increasing the number of passages retrieved is effective, it also has implications on overall system performance as the already slow Reader now has to reason over more text. Instead, we can lean on best practices in the well explored domain of information retrieval.\n",
    "\n",
    "Optimizing full-text search is a battle between precision (returning as few irrelevant documents as possible) and recall (returning as many relevant documents as possible). Matching only exact words in the question query results in high precision, however it misses out on many passages that could possibly be considered relevant. Rather we can cast a wider net by searching for terms that are not exactly the same as in the question query, but are related in some way. Let's explore a few tips and tricks for widening the net.\n",
    "\n",
    "Earlier in the post we described Elasticsearch Analyzers as a way to deal with the nuances of human language. Analyzers provide a flexible and extensible method to tailor search for a given dataset. Here are a few custom Analyzer components Elasticsearch provides that can help cast a wider net.\n",
    "\n",
    "- **Stopwords**: Stopwords are the most frequently occuring words in the English language (ex. \"and\", \"the\", \"to\", etc.) and add minimal semantic value to a piece of text. Common practice in information retrieval is to identify and remove them from documents to decrease the size of the index and increase relevance of search results. \n",
    "\n",
    "- **Stemming**: The English language is inflected - words can alter their written form to express different meanings. For example, 'sing', 'sings', 'sang', and 'singing' are written with slight differences, but really mean the same thing. Stemming is a technique that attempts to reduce words to their root form and consequently improve retrievability. Stemming algorithms exploit the fact that search intent is *usually* word-form agnostic - which is not always the case. For example, the phrases \"fly fishing\" and \"flying fish\" both stem to \"fli fish\" which would provide a very incorrect result depending on the users intent. Despite that, stemming does increase search recall by relaxing query term inflection. We'll implement the [Snowball](https://snowballstem.org/) stemming algorithm as a token filter in our custom Analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284,
     "referenced_widgets": [
      "f8d18feb30b24de5bbbb869cc9a31ae8",
      "15f44b8cf87d459bb6a40b5e6b9db470",
      "f4d90214c67749168472a2ba3cb0d72a",
      "95923a713e324d18bc9fc82a466703c4",
      "40b3ea36c5ca407597e8ce6c738c9786",
      "9061d4497e4443029cbb21b77281cf31",
      "139cd5c5c43f436495dfddbdfc7d35c3",
      "ef2c1a3506c549b7878e3ea4a5cc565f"
     ]
    },
    "colab_type": "code",
    "id": "dPMz6_6ZlTsV",
    "outputId": "e4a16e49-387b-4913-adc6-db5def10d339"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d18feb30b24de5bbbb869cc9a31ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20239.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Succesfully loaded 20239 into squad-stop-stem-index\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fc4ce65eaa55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# evaluate retriever performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mstop_stem_results_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_stem_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes_obj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'squad-stop-stem-index'\u001b[0m\u001b[0;34m,\u001b[0m                                                             \u001b[0mqa_records\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqa_records_answerable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_retriever' is not defined"
     ]
    }
   ],
   "source": [
    "# create new index\n",
    "index_config = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"stop_stem_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\":[\n",
    "                        \"lowercase\",\n",
    "                        \"stop\",\n",
    "                        \"snowball\"\n",
    "                    ]\n",
    "                    \n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"strict\", \n",
    "        \"properties\": {\n",
    "            \"document_title\": {\"type\": \"text\", \"analyzer\": \"stop_stem_analyzer\"},\n",
    "            \"document_text\": {\"type\": \"text\", \"analyzer\": \"stop_stem_analyzer\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "es.indices.create(index='squad-stop-stem-index', body=index_config, ignore=400)\n",
    "\n",
    "# populate the index\n",
    "populate_index(es_obj=es, index_name='squad-stop-stem-index', evidence_corpus=all_wiki_articles)\n",
    "\n",
    "# evaluate retriever performance\n",
    "stop_stem_results_df, stop_stem_metrics = evaluate_retriever(es_obj=es, index_name='squad-stop-stem-index',\\\n",
    "                                                             qa_records=qa_records_answerable, n_results=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13ETw8oBlTsY",
    "outputId": "918b6bb6-1eff-4138-940c-e5784a39300b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Recall': 0.8501115914996388,\n",
       " 'Mean Average Precision': 0.7800892731997112,\n",
       " 'Average Query Duration': 0.7684287701215108}"
      ]
     },
     "execution_count": 137,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_stem_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4jt2vmgelTsa"
   },
   "source": [
    "Awesome - we've increased recall and mAP by ~3 points and reduced our average query duration by nearly 4x through simple pre-processing steps that just scratch the surface of tailored analysis in Elasticsearch. With that, there is no \"one size fits all\" recipe for optimizing search relevance and every implementation will be different. In addition to custom analysis, there are many other methods for increasing search recall like query expansion - that introduces additional tokens/phrases into a query at search time. We'll save that topics for another post and instead take a look at how the Retreiver's performance affects and end-to-end QA system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M4YOvX-ilTsb"
   },
   "source": [
    "# Impact of Retriever in End-to-End QA System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RAFJgQcjnbky"
   },
   "source": [
    "When evaluating the retriever, we wanted to use a large number of questions and documents in order to better understand the retriever's performance.   Having a significant number of documents is important when assessing retrievers because it increases the complexity of the problem and, while this is still a toy example, we want to make it similar to a real-world application which could have many tens of thousands of documents. It's easy to return a relevant document when there are only 10 to chose from!\n",
    "\n",
    "However, now we want to evaluate the full QA system in which our retriever passes relevant documents to a reader. BERT has been trained to understand the task of question-answering by learning the train set. Thus we cannot use those questions for evaluation -- BERT has already seen them all and would return inflated performance values. Instead, we'll evaluate the full QA system using only the questions from the dev set, which is how we evaluated BERT in our last post. We'll use the more efficient index that we developed in the last section in order to give our reader the best chance of receiving documents that contain the right answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BB_njF2And5p"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zc19RtHant-J"
   },
   "outputs": [],
   "source": [
    "from transformers.data.metrics.squad_metrics import squad_evaluate\n",
    "from transformers.data.processors import SquadV2Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ah5HxO2Gs7Gf"
   },
   "source": [
    "### Connecting the retriever to the reader\n",
    "\n",
    "There are multiple strategies for passing documents from the retriever to the reader. \n",
    "1. Pass each document to the reader individually then aggregate the resulting scores\n",
    "2. Concatenate all documents and pass to the reader simultaneously\n",
    "\n",
    "Each method has pros and cons and ultimately, we found that it was better to go with option 2. The reason lies in the aggregation step inherent to option 1. The reader returns answers and scores for each document. A series of heuristics must be developed to determine which answer is the best, and when the null answer should be returned -- made all the more challenging when 4 of your 5 documents return strong null answer scores (0.99) and one document returns a non-null answer with a weak score (0.34)!  Furthermore, due to the nature of how these scores are computed, it is difficult to compare them on the same scale -- an answer with a score of 0.78 from one document might not actually be inherently \"better\" than an answer with a score of .70 from another document! Digging into why this is so is beyond the scope of this article but we'd be happy to discuss it further with those who are interested! Finally, this option is slower because each article is passed individually leading to multiple BERT calls. \n",
    "\n",
    "Instead we opted for option 2. Creating one long context out of the top n articles solves many problems for us: \n",
    "1. all candidate answers are scored on the same scale,\n",
    "2. handling the null answer is more straightforward (we did that last time),\n",
    "3. and we can take advantage of faster compute as HF will chunk long documents for us and pass them through BERT in a batch. \n",
    "\n",
    "On the other hand, when concatenating multiple passages together, there's a good chance that BERT will see a mixed context: the end of one paragraph grafted onto the beginning of another, for example. This could make it more difficult for the model to correctly identify an answer in a potentially confusing context. Another drawback is that it's more difficult to backstrapolate which of the input documents the answer ultimately came from. \n",
    "\n",
    "\n",
    "We created a reader class that leverages the HF question-answering `pipeline` to do the brunt of the work for us (loading models and tokenizers, converting text to features, chunking, prediction, etc.). We then created two methods: `predict` and `predict_combine` corresponding to the two options above. In what follows we'll use `predict_combine` to concatenate the documents the retriever sends to the reader for a given question. \n",
    "\n",
    "The cell below will download the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eq4eKxJhnfnK"
   },
   "outputs": [],
   "source": [
    "!curl -L -O https://raw.githubusercontent.com/melaniebeck/question_answering/master/src/readers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "70d3vDGCsarf"
   },
   "source": [
    "We're going to do a SQuAD-style evaluation so we need the dev set in the proper format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ithvjPtSsG_f",
    "outputId": "418c49b2-30ad-4b31-ee8e-7cf410ad6fab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:03<00:00,  9.61it/s]\n"
     ]
    }
   ],
   "source": [
    "processor = SquadV2Processor()\n",
    "examples = processor.get_dev_examples(data_dir='data/squad/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AOTKaSZ4uRWu"
   },
   "source": [
    "The following cells implement two functions for our end-to-end QA system. \n",
    "\n",
    "The `query` function takes a question, retrieves the top k relevant documents, and passes those documents to the reader.  \n",
    "\n",
    "Given a list of SQuAD examples, the `evaluate_qasystem` function calls `query` for each example and extracts the top-scoring answer as the prediction. Predictions for each example are saved to an output file. Once all examples have predictions, both are fed to the HF `squad_evaluate` method that we used last time, which computes the EM and F1 scores for the dev set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zi9757o_qqC6"
   },
   "outputs": [],
   "source": [
    "def query(question, index_name, topk):\n",
    "    \"\"\"\n",
    "    Method to extract an answer from the full end-to-end QA system: \n",
    "        1) Passes the question to the search engine and retrieves the top N relevant documents \n",
    "        2) Massages those results into a suitable format for the reader\n",
    "        3) Passing the top N documents to the reader\n",
    "        4) Returns the top N answers for each of the N documents ()\n",
    "        calls the reader\n",
    "    \"\"\"\n",
    "    query = {\n",
    "        'query': {\n",
    "            'match': {\n",
    "                'document_text': question\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    retriever_results = search_es(es_obj=es, \n",
    "                                  index_name=index_name, \n",
    "                                  query=query, \n",
    "                                  n_results=topk)\n",
    "\n",
    "    passages = retriever_results['hits']['hits']\n",
    "    docs = []\n",
    "    for passage in passages:\n",
    "        doc = {\n",
    "            'id': passage['_id'],\n",
    "            'score': passage['_score'],\n",
    "            'text': passage['_source']['document_text'], \n",
    "            'title': passage['_source']['document_title'],\n",
    "        }\n",
    "        docs.append(doc)\n",
    "\n",
    "    answers = reader.predict_combined(question, docs, topk)\n",
    "    return answers\n",
    "\n",
    "def evaluate_qasystem(examples, \n",
    "                      index_name, \n",
    "                      topk, \n",
    "                      output_path='data/',\n",
    "                      save_output=True \n",
    "                      ):\n",
    "    \"\"\"\n",
    "    Evaluate an full QA system on the SQuAD2.0 dev set. \n",
    "    \n",
    "    Inputs\n",
    "        examples: the SQuAD2.0 dev set as loaded by the squad processors\n",
    "        \n",
    "    \n",
    "    Outputs\n",
    "        Saved to disk \n",
    "        predictions: Best answer for each SQuAD2.0 question \n",
    "        meta_predictions: Top N answers for each SQuAD question\n",
    "        \n",
    "        Returns\n",
    "        results: OrderedDict of results from the HF squad_evaluate method\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "\n",
    "    outfile = output_path+f\"predictions_{index_name}_{topk}.pkl\"\n",
    "\n",
    "    # if we've already computed predictions, load them for evaluation\n",
    "    if os.path.exists(outfile):\n",
    "        predictions = pickle.load(open(outfile, \"rb\"))\n",
    "    else:\n",
    "        predictions = {}\n",
    "        meta_predictions = {}\n",
    "        \n",
    "        for example in tqdm(examples):\n",
    "            # retrieve top N relevant documents from retriever\n",
    "            reader_results = query(example.question_text, index_name, topk)\n",
    "            # add best answer to predictions\n",
    "            answers = reader_results['answers']\n",
    "            predictions[example.qas_id] = answers[0]['answer_text']\n",
    "\n",
    "            # for debugging/explainability - save the full answer \n",
    "            # (not just text answer from top hit)\n",
    "            meta_predictions[example.qas_id] = answers\n",
    "\n",
    "        if save_output:\n",
    "            pickle.dump(predictions, open(outfile, \"wb\"))\n",
    "\n",
    "            meta_outfile = os.path.splitext(outfile)[0]+\"_meta.pkl\"\n",
    "            pickle.dump(meta_predictions, open(meta_outfile, \"wb\"))\n",
    "        \n",
    "    # compute evaluation with HF \n",
    "    results = squad_evaluate(examples, predictions)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrTUcbUkQonx"
   },
   "source": [
    "Running the cell below requires a gpu so execute with caution or Colab. Evaluation over the top 1 retrieved documents takes about 35 minutes to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287,
     "referenced_widgets": [
      "66e0efa9685248629e009c1e255bff6e",
      "f0858954bbfd447eb95a04a3caabf8a4",
      "f6c75626f2d54bfbaa7bad761af5b9c2",
      "62fb51c849b04bc78c629dd42f811deb",
      "26b9259fecf4445b8fdcb753ed6d09ef",
      "e91c7ba9a6314f0d905d08d0f822cbc1",
      "3d3ce5e897b84a218237582372bca2bb",
      "0f9413092ffe496d8f31c374732e870d"
     ]
    },
    "colab_type": "code",
    "id": "Eaiz_d3eDTkR",
    "outputId": "778f92a4-3c0c-4ffd-afc5-be2621cf8b1c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e0efa9685248629e009c1e255bff6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11873.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('exact', 57.16331171565737),\n",
       "             ('f1', 60.41835572408481),\n",
       "             ('total', 11873),\n",
       "             ('HasAns_exact', 50.64102564102564),\n",
       "             ('HasAns_f1', 57.16044829825517),\n",
       "             ('HasAns_total', 5928),\n",
       "             ('NoAns_exact', 63.666947014297726),\n",
       "             ('NoAns_f1', 63.666947014297726),\n",
       "             ('NoAns_total', 5945),\n",
       "             ('best_exact', 57.16331171565737),\n",
       "             ('best_exact_thresh', 0.0),\n",
       "             ('best_f1', 60.41835572408505),\n",
       "             ('best_f1_thresh', 0.0)])"
      ]
     },
     "execution_count": 104,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_qasystem(examples, index_name='squad-stop-stem-index', topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287,
     "referenced_widgets": [
      "be3cf3e6c6ba40d087f8dd27dd4f5e64",
      "d193d10e1ad94119a849d123c093f3cc",
      "705fa1214c044cbcae6f5d109b22802d",
      "3c0ffe3eeca741899ddbe1306e60ce39",
      "e30ff795eb1c4016be3967f190764399",
      "f04b80c35efb44b5bec078f4d7a57f3b",
      "7a1bbfb20bd84d4b8995584a37dabae1",
      "26ef4a7e1f76465ead7e51c1d9866c6f"
     ]
    },
    "colab_type": "code",
    "id": "Uv7CAhHuYbWw",
    "outputId": "128381b6-e6a1-4bd4-9f6f-133a38649b37"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3cf3e6c6ba40d087f8dd27dd4f5e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11873.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('exact', 39.15606839046576),\n",
       "             ('f1', 42.27172639000713),\n",
       "             ('total', 11873),\n",
       "             ('HasAns_exact', 49.42645074224021),\n",
       "             ('HasAns_f1', 55.666701657988256),\n",
       "             ('HasAns_total', 5928),\n",
       "             ('NoAns_exact', 28.915054667788056),\n",
       "             ('NoAns_f1', 28.915054667788056),\n",
       "             ('NoAns_total', 5945),\n",
       "             ('best_exact', 50.08843594710688),\n",
       "             ('best_exact_thresh', 0.0),\n",
       "             ('best_f1', 50.09054156489514),\n",
       "             ('best_f1_thresh', 0.0)])"
      ]
     },
     "execution_count": 105,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "evaluate_qasystem(examples, index_name='squad-stop-stem-index', topk=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the number of documents\n",
    "We saw the retriever's performance increased when it retrieved more documents -- more documents results in a higher chance of at least one of them containing the true answer. Does this hold true for a full QA system? \n",
    "\n",
    "Our initial findings suggest that, no, this might not hold true -- at least not in the context of evaluating on a standardized dev set like SQuAD2.0, where is it critical to return correct answers **and** correct null answers. \n",
    "\n",
    "We ran an experiment evaluating 1000 SQuAD2.0 examples while allowing the retriever to pass varying numbers of documents to the reader. Below we show the results along with the scores achieved during our \"standard\" evaluation on the SQuAD2.0 dev set (No Retriever). Recall that in that scenario the reader is given the *exact* paragraph it needs in order to answer the question (or not).\n",
    "\n",
    "![](my_icons/qa_eval_by_topk.png)\n",
    "\n",
    "What's going on here? \n",
    "\n",
    "We speculate that there are several things at work. \n",
    "\n",
    "1. In general, longer contexts are more difficult for standard Transformers\n",
    "2. This long context contains disparate information compiled from all kinds of random paragraphs\n",
    "\n",
    "\n",
    "Does this mean we shouldn't use QA systems like this? Of course not! We have to keep in mind that the goal of this type of evaluation is very strict and narrow. In a real-world example, no one really cares of the the answer exactly matched some preconceived notion -- what matters is that something made sense in at least one of the paragraphs. Thus, a potentially better way to evaluate a QA system through interaction using real-world questions and feedback mechanisms that identify how successful a model is at answering a user's question. \n",
    "\n",
    "[Melanie] I don't like this last paragraph yet but it's midnight and I'm tired. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_hC2kp1RT_R"
   },
   "source": [
    "# WRAPPING UP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0CNmwXot9GOy"
   },
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "class Reader:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name=\"twmkn9/distilbert-base-uncased-squad2\",\n",
    "        tokenizer_name=\"twmkn9/distilbert-base-uncased-squad2\",\n",
    "        use_gpu=True,\n",
    "        handle_impossible_answer=True\n",
    "        ):\n",
    "        \n",
    "        self.use_gpu = use_gpu\n",
    "        self.model = pipeline('question-answering', model=model_name, tokenizer=tokenizer_name, device=int(use_gpu)-1)\n",
    "        self.kwargs = {'handle_impossible_answer':handle_impossible_answer}\n",
    "\n",
    "    def predict(self, question, documents, topk=None):\n",
    "        # instead of having to deal with all the nitty gritty details of converting examples into features \n",
    "        # and creating sensible predictions, we can instead focus on predicting answers over a collection of docs\n",
    "        # as this is what we'll be getting from the retriever\n",
    "        self.kwargs['topk'] = topk\n",
    "\n",
    "        all_predictions = []\n",
    "        for doc in documents:            \n",
    "            inputs = {\"question\": question, \"context\": doc['text']}\n",
    "            predictions = self.model(inputs, **self.kwargs)\n",
    "            print(predictions)\n",
    "\n",
    "            # we want the best prediction from each document\n",
    "            if topk == 1:\n",
    "                best = predictions\n",
    "            else:\n",
    "                best = predictions[0]\n",
    "\n",
    "            answer = {\n",
    "                    'probability': best['score'],\n",
    "                    'answer_text': best['answer'],\n",
    "                    'start_index': best['start'],\n",
    "                    'end_index': best['end'],\n",
    "                    'doc_id': doc['id'],\n",
    "                    'title': doc['title']\n",
    "                }\n",
    "            all_predictions.append(answer)\n",
    "\n",
    "        # Simple heuristic: \n",
    "        # If the best prediction from each document is the null answer, return null\n",
    "        # Otherwise, return the highest scored non-null answer\n",
    "        null = True\n",
    "        for prediction in all_predictions:\n",
    "            if prediction['answer_text']:\n",
    "                null = False\n",
    "        \n",
    "        if not null:\n",
    "            # pull out and sort only non-null answers\n",
    "            non_null_predictions = [prediction for prediction in all_predictions if prediction['answer_text']]\n",
    "            sorted_non_null = sorted(non_null_predictions, key=lambda x: x['probability'], reverse=True)\n",
    "            \n",
    "            # append the null answers for completeness\n",
    "            null_predictions = [prediction for prediction in all_predictions if not prediction['answer_text']]\n",
    "            best_predictions = sorted_non_null + null_predictions\n",
    "        else:  \n",
    "            # sort null answers for funsies\n",
    "            best_predictions = sorted(all_predictions, key=lambda x: x[\"probability\"], reverse=True)[: self.kwargs[\"topk\"]]\n",
    "        \n",
    "        results = {\n",
    "            'question': question,\n",
    "            'answers': best_predictions\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def predict_combined(self, question, documents, topk=None):\n",
    "        \"\"\"\n",
    "        Compute text prediction for a question given a collection of documents\n",
    "\n",
    "        Inputs:\n",
    "            question: str, question string\n",
    "            documents: list of document dicts, each with the following format:\n",
    "                    {\n",
    "                        'text': context string,\n",
    "                        'id': document identification,\n",
    "                        'title': name of document \n",
    "                    }\n",
    "            topk (optional): int, if provided, overrides default topk\n",
    "        \n",
    "        Outputs:\n",
    "            results: dict with the following format:\n",
    "                {\n",
    "                    'question': str, question string,\n",
    "                    'answers': list of answer dicts, each including text answer, probability, \n",
    "                                start and end positions, and document metadata\n",
    "                }\n",
    "        \"\"\"\n",
    "        if topk:\n",
    "            self.kwargs['topk'] = topk\n",
    "\n",
    "        # combine all documents together into one long context\n",
    "        context = ''\n",
    "        for doc in documents:\n",
    "            context += doc['text'] + ' '\n",
    "\n",
    "        all_predictions = []\n",
    "        inputs = {\"question\": question, \"context\": context}\n",
    "        predictions = self.model(inputs, **self.kwargs)\n",
    "        \n",
    "        # we want the best prediction from each document\n",
    "        if self.kwargs['topk'] == 1:\n",
    "            predictions = [predictions]\n",
    "\n",
    "        for pred in predictions:\n",
    "            answer = {\n",
    "                    'probability': pred['score'],\n",
    "                    'answer_text': pred['answer'],\n",
    "                    'start_index': pred['start'],\n",
    "                    'end_index': pred['end'],\n",
    "                    #'doc_id': doc['id'],\n",
    "                    #'title': doc['title']\n",
    "                }\n",
    "            all_predictions.append(answer)\n",
    "\n",
    "        # sort and truncate predictions\n",
    "        best_predictions = sorted(all_predictions, key=lambda x: x[\"probability\"], reverse=True)[: self.kwargs[\"topk\"]]\n",
    "        \n",
    "        results = {\n",
    "            'question': question,\n",
    "            'answers': best_predictions\n",
    "        }\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "colab_type": "code",
    "id": "08Aof78MqhX_",
    "outputId": "feb0540a-770e-4239-a873-4a44d2524303"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "# pulled from the previous blog post - standard QA eval on distilbert\n",
    "predictions = json.load(open(\"predictions_.json\", \"rb\"))\n",
    "no_retriever = squad_evaluate(examples[:1000], predictions)\n",
    "\n",
    "results = pickle.load(open(\"qa_eval_results_1000squad_topk1_topk5.pkl\", \"rb\"))\n",
    "df = pd.DataFrame(data=[no_retriever] + results)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(df['topk'], df['f1'], label='F1')\n",
    "plt.text(0, 40, 'No Retriever', rotation=90, ha='center', va='top', color='white', fontsize=18)\n",
    "plt.xticks([0,1,2,3,4,5])\n",
    "\n",
    "plt.ylabel('F1 Score', fontsize=16)\n",
    "plt.xlabel('Top k retrieved documents', fontsize=16)\n",
    "plt.title(\"QA evaluation over 1000 SQuAD2.0 examples\", fontsize=18)\n",
    "\n",
    "plt.savefig('qa_eval_by_topk.png')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "In0YUapuBQ8A",
    "pKVJVU5hBQ8U",
    "t4s4Bx3LBQ8p"
   ],
   "machine_shape": "hm",
   "name": "2020-06-09-Evaluating_BERT_on_SQuAD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f9413092ffe496d8f31c374732e870d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "139cd5c5c43f436495dfddbdfc7d35c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "15f44b8cf87d459bb6a40b5e6b9db470": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26b9259fecf4445b8fdcb753ed6d09ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "26ef4a7e1f76465ead7e51c1d9866c6f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28a305d13e584615b9ba3cd9a37bdd56": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8cebe4ccf004d84bdc37ce44d1192e8",
      "max": 20239,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a44c2693aee14b89a4c6512c85142f51",
      "value": 20239
     }
    },
    "3c0ffe3eeca741899ddbe1306e60ce39": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26ef4a7e1f76465ead7e51c1d9866c6f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7a1bbfb20bd84d4b8995584a37dabae1",
      "value": " 11873/11873 [6:21:18&lt;00:00,  1.93s/it]"
     }
    },
    "3d3ce5e897b84a218237582372bca2bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40b3ea36c5ca407597e8ce6c738c9786": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "62fb51c849b04bc78c629dd42f811deb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f9413092ffe496d8f31c374732e870d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3d3ce5e897b84a218237582372bca2bb",
      "value": " 11873/11873 [7:01:01&lt;00:00,  2.13s/it]"
     }
    },
    "66e0efa9685248629e009c1e255bff6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f6c75626f2d54bfbaa7bad761af5b9c2",
       "IPY_MODEL_62fb51c849b04bc78c629dd42f811deb"
      ],
      "layout": "IPY_MODEL_f0858954bbfd447eb95a04a3caabf8a4"
     }
    },
    "705fa1214c044cbcae6f5d109b22802d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f04b80c35efb44b5bec078f4d7a57f3b",
      "max": 11873,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e30ff795eb1c4016be3967f190764399",
      "value": 11873
     }
    },
    "7a1bbfb20bd84d4b8995584a37dabae1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9061d4497e4443029cbb21b77281cf31": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95923a713e324d18bc9fc82a466703c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef2c1a3506c549b7878e3ea4a5cc565f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_139cd5c5c43f436495dfddbdfc7d35c3",
      "value": " 20239/20239 [02:38&lt;00:00, 127.56it/s]"
     }
    },
    "a299f8fa902348b7807b4d97ebc6027d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dca5b0a8c1014a66bc8c5ba988878a85",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b04d963baf9a40789305d1372ffe1aab",
      "value": " 20239/20239 [04:14&lt;00:00, 79.57it/s]"
     }
    },
    "a44c2693aee14b89a4c6512c85142f51": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a493a4c07a2743899571330cf6476b74": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_28a305d13e584615b9ba3cd9a37bdd56",
       "IPY_MODEL_a299f8fa902348b7807b4d97ebc6027d"
      ],
      "layout": "IPY_MODEL_e94a6f98578743c096c9b045d9dfdf81"
     }
    },
    "a8cebe4ccf004d84bdc37ce44d1192e8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b04d963baf9a40789305d1372ffe1aab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be3cf3e6c6ba40d087f8dd27dd4f5e64": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_705fa1214c044cbcae6f5d109b22802d",
       "IPY_MODEL_3c0ffe3eeca741899ddbe1306e60ce39"
      ],
      "layout": "IPY_MODEL_d193d10e1ad94119a849d123c093f3cc"
     }
    },
    "d193d10e1ad94119a849d123c093f3cc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dca5b0a8c1014a66bc8c5ba988878a85": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e30ff795eb1c4016be3967f190764399": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e91c7ba9a6314f0d905d08d0f822cbc1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e94a6f98578743c096c9b045d9dfdf81": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef2c1a3506c549b7878e3ea4a5cc565f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f04b80c35efb44b5bec078f4d7a57f3b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0858954bbfd447eb95a04a3caabf8a4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4d90214c67749168472a2ba3cb0d72a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9061d4497e4443029cbb21b77281cf31",
      "max": 20239,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_40b3ea36c5ca407597e8ce6c738c9786",
      "value": 20239
     }
    },
    "f6c75626f2d54bfbaa7bad761af5b9c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e91c7ba9a6314f0d905d08d0f822cbc1",
      "max": 11873,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_26b9259fecf4445b8fdcb753ed6d09ef",
      "value": 11873
     }
    },
    "f8d18feb30b24de5bbbb869cc9a31ae8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f4d90214c67749168472a2ba3cb0d72a",
       "IPY_MODEL_95923a713e324d18bc9fc82a466703c4"
      ],
      "layout": "IPY_MODEL_15f44b8cf87d459bb6a40b5e6b9db470"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
