{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl0KyQomBQ7l"
   },
   "source": [
    "# Evaluating the Retriever & End-to-End System\n",
    "> A review of Information Retrieval and the role it plays in a QA system\n",
    "\n",
    "- title: \"Evaluating the Retriever & End-to-End System\"\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- hide: true\n",
    "- permalink: /hidden/\n",
    "- search_exclude: false\n",
    "- categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our last post, [Evaluating QA: Metrics, Predictions, and the Null Response](https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html), we took a deep dive look at how to asses the quality of a BERT-like Reader for Question Answering (QA) using the Hugging Face framework. In this post, we'll focus on the former component of an end-to-end QA system - the Retriever. Specifically, we'll introduce Elasticsearch as a powerful and efficient Information Retrieval (IR) tool that can be used to scour through large corpora and retrieve relevant documents. Through the post, we'll explain how to implement and evaluate a Retriever in the context of Question Answering and demonstrate the impact it has on an end-to-end QA system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "np9SZdgqBQ7m"
   },
   "source": [
    "### Prerequisites\n",
    "* a basic understanding of Information Retrieval (IR) & Search\n",
    "* a basic understanding of IR based QA systems (see previous posts)\n",
    "* a basic understanding of Transformers and PyTorch\n",
    "* a basic understanding of the SQuAD2.0 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving the right document is important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](my_icons/michael_scott_quote.jpg \"You miss 100% of the shots you don't take\")\n",
    "\n",
    "\n",
    "We believe that what Michael Scott really mean to say is:\n",
    "\n",
    "> \"***You miss 100% of the questions if the answer doesn't appear in the input context***\"\n",
    "\n",
    "[Andrew] - Find a better intro ^^^\n",
    "\n",
    "\n",
    "As we have discussed throughout this blog series, many modern QA systems take a two-staged approach to answering questions. In the first stage, a document retriever selects $N$ potentially relevant documents from a given corpus. Subsequently, a machine comprehension model processes each of the $N$ documents to determine an answer to the input question. Because of recent advances in NLP and deep learning (i.e. flashy Transformer models), the machine comprehension component of question answering has typically been the main focus of evaluation for these systems. Stage one of these systems has recieved limited attention despite its obvious importance...stage two is bounded by performance at stage one. Let's get more specific.\n",
    "\n",
    "We [recently explained methods]() that enable BERT-like models to produce robust answers given a question and context passage by selectively processing predictions and by refraining from answering certain questions at all. While the ability to properly comprehend a passage and produce a correct answer is a very important feature of any QA tool, the success of the overall system is highly dependent on first providing a correct passage to read through. Without being fed a context passage that actually contains the ground-truth answer to a given question, the overall system's performance is limited to how well it can predict no-answer questions. To demonstrate, we'll revisit an example from our [second blog post](https://qa.fastforwardlabs.com/pytorch/hugging%20face/wikipedia/bert/transformers/2020/05/19/Getting_Started_with_QA.html) where three questions were asked of the Wikipedia search engine based QA system:\n",
    "\n",
    "```\n",
    "**Example 1: Incorrect**\n",
    "Question: When was Barack Obama born?\n",
    "Top wiki result: <WikipediaPage 'Barack Obama Sr.'>\n",
    "Answer: 18 June 1936 / February 2 , 1961 / \n",
    "\n",
    "**Example 2: Correct**\n",
    "Question: Why is the sky blue?\n",
    "Top wiki result: <WikipediaPage 'Diffuse sky radiation'>\n",
    "Answer: Rayleigh scattering / \n",
    "\n",
    "**Example 3: Correct**\n",
    "Question: How many sides does a pentagon have?\n",
    "Top wiki result: <WikipediaPage 'The Pentagon'>\n",
    "Answer: five / \n",
    "```\n",
    "\n",
    "In Example 1, the Reader had no chance of producing the correct answer because of its outright absence from the context article served up by the Retriever. Namely, the Retriever erroneously provided a page about Barack Obama Sr. instead of his son, the former US President. In this case, the only way the Reader could have possibly produced the correct answer was if the correct answer was actually not to answer at all. On the flip side, in Example 3, the Retriever did not identify the globally \"correct\" document - it returned an article about \"The Pentagon\" instead of a page about geometry - but nonetheless, it provided enough context for the Reader to succeed.\n",
    "\n",
    "These quick examples illustrate  why an effective Retriever is critical for an end-to-end QA system. Now let's take a deeper look at a classic tool used for information retrieval - Elasticsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elasticsearch as an IR Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](my_icons/elasticsearch-logo.png \"Elasticsearch\")\n",
    "\n",
    "Modern QA systems employ a variety of techniques for the task of information retrieval ranging from traditional sparse vector word matching (ex. Elasticsearch) to [novel approaches](https://arxiv.org/pdf/2004.04906.pdf) using dense representations of encoded passages combined with [efficient search capabilities](https://github.com/facebookresearch/faiss). Despite the flurry of contemporary research efforts in this area, the traditional sparse vector approach performs very well overall and has only recently been overtaken by embedding-based systems for end-to-end QA retrieval tasks. For that reason, we'll explore Elasticsearch as a simple and easy to use framework for document retrieval. So, what exactly is Elasticsearch?\n",
    "\n",
    "Elasticsearch is a powerful open-source search and analytics engine built on the [Apache Lucene](https://lucene.apache.org/) library that is capable of handling all types of data including textual, numerical, geospatial, structrured, and unstructured. It is built to scale with a robust set of features, rich ecosystem, and diverse set of client libraries making it easy to integrate and use. In the context of information retrieval for automated question answering, we are keenly interested in the features surrounding full-text search. Elasticsearch provides a convenient way to index documents so they can easily be queried for nearest neighbor search using a TF-IDF based similarity metric. Specifically, it uses [BM25](https://opensourceconnections.com/blog/2015/10/16/bm25-the-next-generation-of-lucene-relevation/) term weighting to represent question and context passages as high-dimensional, sparse vectors that are efficiently searched in an inverted index. Let's unpack those ideas a bit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of an inverted index is to store text in a data structure that allows for efficient and fast full-text searches. An inverted index is essentially just a mapping between unique terms and documents which contain those terms. For example, let's consider the following two documents and a depiction of an inverted index built from them:\n",
    "1. \"Elasticsearch is a powerful technique for search!\"\n",
    "2. \"Manual search is a slow technique.\"\n",
    "\n",
    "|      Term     | Document 1 | Document 2 |\n",
    "|:-------------:|:----------:|:----------:|\n",
    "|       a       |      x     |      x     |\n",
    "| elasticsearch |      x     |            |\n",
    "|      for      |      x     |            |\n",
    "|       is      |      x     |      x     |\n",
    "|     manual    |            |      x     |\n",
    "|    powerful   |      x     |            |\n",
    "|     search    |      x     |      x     |\n",
    "|      slow     |            |      x     |\n",
    "|   technique   |      x     |      x     |\n",
    "\n",
    "Notice that the unique set of terms from both documents are contained in the index, and we can easily lookup which document contains which terms. Searching this inverted index for the phrase \"search technique\" would return both documents because both terms are present in each document, while searching for the phrase \"powerful technique\" would return only Document 1. Search itself is quite a bit more complicated than the boolean logic depicted here as it involves relevance scoring (among other query dependent logic), however this oversimplification is intended to demonstrate the quick and powerful nature of the inverted index data structure.\n",
    "\n",
    "> Note: In the example above, all tokens have been lowercased and punctuation removed. This happens as part of an important preprocessing pipeline that we'll explain in more detail later in the post.\n",
    "\n",
    "The inverted index representation is considered *sparse* by construction because for each document, you end up with a vector containing few terms relative to all terms in the index. This indexing process is what allows Elasticsearch to search large collections of text documents orders of magnitude faster than traditional SQL databases. While the exact match nature of search in this data structure is powerful and effective, it isn't without flaws. The word matching approach is limited in its ability to take semantically related concepts into search consideration. For example, consider the following question and context:\n",
    "\n",
    "> **Question:** \"Who is the bad guy in lord of the rings?\"\\\n",
    "> **Context:** \"Sala Baker is an actor and stuntman from New Zealand. He is best known for portraying the villain Sauron in the Lord of the Rings trilogy...\"\n",
    "\n",
    "A exact match based system like Elasticsearch would struggle to retrieve this supporting context passage because it lacks the ability to relate the concepts of \"bad guy\" and \"villain\". Modern document retrieval systems that take advantage of learned, dense representations of text would perform better in this situation.\n",
    "\n",
    "[ANDREW] - this example ^ was taken from [DPR](https://arxiv.org/pdf/2004.04906.pdf). Either cite it or come up with different example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Elasticsearch with SQuAD2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this basic understanding of how Elasticsearch works, let's dive in and build our own Document Retrieval system by indexing a set of Wikipedia article paragraphs that support questions and answers in the SQuAD2.0 dataset. Before we get started, we'll need to download and prepare data from the SQuAD2.0 train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Prepare SQUAD2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "\n",
    "# Download the SQuAD2.0 train set\n",
    "!wget -P data/squad/ https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "    \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `parse_qa_records` function will extract question/answer examples, as well as article content from the train set. A common practice in IR for QA is to segment large articles into smaller passages before indexing for two reasons:\n",
    "\n",
    "1. Transformer based Readers are very slow - providing an entire Wikipedia article to BERT for processing takes a considerable amount of time and also defeats the purpose of using an IR tool to narrow the search space for the Reader.\n",
    "2. Smaller passages reduce noise - by identifying a more concise context passage for BERT to read through, we reduce the chance of BERT getting lost.\n",
    "\n",
    "Of course the chunking method proposed here doesn't come without a cost. Larger document size means each document contains more information. By reducing passage size, we are potentially trading off system recall for speed - though there are techniques to alleviate this as we will disucss later in the post. \n",
    "\n",
    "With our chunking approach, each article paragraph will be prepended with the article title and collectively serve as a corpus of documents for which our Elasticsearch Retriever will search over. In practice, open-domain QA systems sit atop massive collections of documents (think all of Wikipedia) to provide a breadth of information to answer general-knowledge questions from. For the purposes of demonstrating Elasticsearch functionality, we will limit our corpus to only the Wikipedia articles supporting SQuAD2.0 train questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_qa_records(data):\n",
    "    '''\n",
    "    Loop through SQuAD2.0 dataset and parse out question/answer examples and unique article paragraphs\n",
    "    \n",
    "    Returns:\n",
    "        qa_records (list) - question/answer examples as list of dictionaries\n",
    "        wiki_articles (list) - unique Wikipedia titles and article paragraphs recreated from SQuAD data\n",
    "    \n",
    "    '''\n",
    "    num_with_ans = 0\n",
    "    num_without_ans = 0\n",
    "    qa_records = []\n",
    "    wiki_articles = {}\n",
    "    \n",
    "    for article in data:\n",
    "        \n",
    "        for i, paragraph in enumerate(article['paragraphs']):\n",
    "            \n",
    "            wiki_articles[article['title']+f'_{i}'] = article['title'] + ' ' + paragraph['context']\n",
    "            \n",
    "            for questions in paragraph['qas']:\n",
    "                \n",
    "                qa_record = {}\n",
    "                qa_record['example_id'] = questions['id']\n",
    "                qa_record['document_title'] = article['title']\n",
    "                qa_record['question_text'] = questions['question']\n",
    "                \n",
    "                try: \n",
    "                    qa_record['short_answer'] = questions['answers'][0]['text']\n",
    "                    num_with_ans += 1\n",
    "                except:\n",
    "                    qa_record['short_answer'] = \"\"\n",
    "                    num_without_ans += 1\n",
    "                    \n",
    "                qa_records.append(qa_record)\n",
    "        \n",
    "        \n",
    "    wiki_articles = [{'document_title':title, 'document_text': text}\\\n",
    "                         for title, text in wiki_articles.items()]\n",
    "                \n",
    "    print(f'Data contains {num_with_ans} question/answer pairs with a short answer, and {num_without_ans} without.'+\n",
    "          f'\\nThere are {len(wiki_articles)} unique wikipedia article paragraphs.')\n",
    "                \n",
    "    return qa_records, wiki_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data contains 86821 question/answer pairs with a short answer, and 43498 without.\n",
      "There are 19035 unique wikipedia article paragraphs.\n"
     ]
    }
   ],
   "source": [
    "# load and parse data\n",
    "train_file = \"data/squad/train-v2.0.json\"\n",
    "train = json.load(open(train_file, 'rb'))\n",
    "\n",
    "qa_records, wiki_articles = parse_qa_records(train['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_id': '56d43c5f2ccc5a1400d830ab',\n",
       " 'document_title': 'Beyoncé',\n",
       " 'question_text': 'What was the first album Beyoncé released as a solo artist?',\n",
       " 'short_answer': 'Dangerously in Love'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show parsed record example\n",
    "qa_records[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document_title': 'Beyoncé_10', 'document_text': 'Beyoncé Beyoncé\\'s first solo recording was a feature on Jay Z\\'s \"\\'03 Bonnie & Clyde\" that was released in October 2002, peaking at number four on the U.S. Billboard Hot 100 chart. Her first solo album Dangerously in Love was released on June 24, 2003, after Michelle Williams and Kelly Rowland had released their solo efforts. The album sold 317,000 copies in its first week, debuted atop the Billboard 200, and has since sold 11 million copies worldwide. The album\\'s lead single, \"Crazy in Love\", featuring Jay Z, became Beyoncé\\'s first number-one single as a solo artist in the US. The single \"Baby Boy\" also reached number one, and singles, \"Me, Myself and I\" and \"Naughty Girl\", both reached the top-five. The album earned Beyoncé a then record-tying five awards at the 46th Annual Grammy Awards; Best Contemporary R&B Album, Best Female R&B Vocal Performance for \"Dangerously in Love 2\", Best R&B Song and Best Rap/Sung Collaboration for \"Crazy in Love\", and Best R&B Performance by a Duo or Group with Vocals for \"The Closer I Get to You\" with Luther Vandross.'}\n"
     ]
    }
   ],
   "source": [
    "# Show example of parsed wiki article paragraph\n",
    "print(wiki_articles[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data ready to go, let's download, install, and configure Elasticsearch using one of the two following methods (Colab recommended). After executing the setup, we will have an Elasticsearch service running locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.\r\n",
      "See 'docker run --help'.\r\n"
     ]
    }
   ],
   "source": [
    "# If running locally - Run Elasticsearch using Docker (assumes Docker is installed)\n",
    "!docker run -d -p 9200:9200 -e \"discovery.type=single-node\" elasticsearch:7.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "\n",
    "# If using Colab - Start Elasticsearch from source\n",
    "! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\n",
    "! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\n",
    "! chown -R daemon:daemon elasticsearch-7.6.2\n",
    "\n",
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'],\n",
    "                   stdout=PIPE, stderr=STDOUT,\n",
    "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
    "                  )\n",
    "# wait until ES has started\n",
    "! sleep 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data into Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the [official low-level Python client library](https://elasticsearch-py.readthedocs.io/en/master/) for interacting with Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "\n",
    "!pip install elasticsearch\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Elasticsearch is launched locally on port 9200. We first need to instantiate an Elasticsearch client object and connect to the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "config = {'host':'localhost', 'port':9200}\n",
    "es = Elasticsearch([config])\n",
    "\n",
    "# test connection\n",
    "es.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go further, let's introduce a few concepts that are specific to Elasticsearch and the process of indexing data. In Elasticsearch, an ***index*** is a collection of documents that have common characteristics (similar to a database schema in an RDBMS). ***Documents*** are JSON objects having their own set of key-value pairs consisting of various data types (similar to rows/fields in RDBMS). When we add a document into an index, the value for the document's text fields go through an analysis process prior to being indexed. This means that when executing a search query against an existing index, we are actually searching against the post-processed representation that is stored in the inverted index, not the raw input document itself.\n",
    "\n",
    "![Elasticsearch Index Process](my_icons/elastic_index_process.png)\n",
    "\n",
    "The anaysis process is a customizable pipeline carried out by a dedicated ***Analyzer***. Elasticsearch analyzers are comprised of three components that make up the processing pipeline: *character filters, a tokenizer, and token filters.* Each of these components modify the input stream of text according to some configurable settings. \n",
    "- **Character Filters:** First, character filters have the ability to add, remove, or replace specific items in the text field. A common application of this filter is to strip `html` tags from the raw input. \n",
    "- **Tokenizer:** After applying character filters, the transformed text is then passed to a tokenizer which breaks up the input string into individual tokens with a provided strategy. By default, the `standard` tokenizer splits tokens whenever it encounters a whitespace character, and also splits on most symbols (like commas, periods, semicolons, etc.)\n",
    "- **Token Filters:** Finally, the token stream is passed to a token filter which acts to add, remove, or modify tokens. Typical token filters include `lowercase` which converts all tokens to lowercase form, and `stop` which removes commonly occuring tokens called stopwords. \n",
    "\n",
    "Elasticsearch comes with several built-in Analyzers that satisfy common use cases with the default being the `Standard Analyzer`. The Standard Analyzer doesn't contain any character filters, uses a `standard` tokenizer, and applies a `lowercase` token filter. Let's take a look at one of the example sentences from above as its passed through this pipeline.\n",
    "\n",
    "[ANDREW] - recreate these images/flows on my own with my own examples. Include a blurb saying \"we found this blog post helpful in understanding Analyzers\"\n",
    "\n",
    "![Elasticsearch Analyzer Pipeline](my_icons/elasticsearch_standard_analyzer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new index and add our Wikipedia articles to it. To create an index, we provide a name and optionally some index configurations. Here we are specifying a set of `mappings` that indicate our anticipated index schema, data types, and how the text fields should be processed. If no `body` is passed, Elasticsearch will automatically infer fields and data types from incoming documents, as well as apply the `Standard Analyzer` to any text fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'squad-standard-index'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_config = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"standard_analyzer\": {\n",
    "                    \"type\": \"standard\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"strict\", \n",
    "        \"properties\": {\n",
    "            \"document_title\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"},\n",
    "            \"document_text\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "index_name = 'squad-standard-index'\n",
    "es.indices.create(index=index_name, body=index_config, ignore=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populate the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then loop through our list of Wikipedia titles & articles and add them to our newly created Elasticsearch index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def populate_index(es_obj, index_name, evidence_corpus):\n",
    "    '''\n",
    "    Loads records into an existing Elasticsearch index\n",
    "\n",
    "    Args:\n",
    "        es_obj (elasticsearch.client.Elasticsearch)\n",
    "        index_name (str)\n",
    "        evidence_corpus (list) - list of dicts containing data records\n",
    "\n",
    "    '''\n",
    "\n",
    "    for i, rec in enumerate(tqdm(evidence_corpus)):\n",
    "    \n",
    "        try:\n",
    "            index_status = es_obj.index(index=index_name, id=i, body=rec)\n",
    "        except:\n",
    "            print(f'Unable to load document {i}.')\n",
    "            \n",
    "    n_records = es_obj.count(index=index_name)['count']\n",
    "    print(f'Succesfully loaded {n_records} into {index_name}')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e2655bf86340b08460fd1c27209075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Succesfully loaded 19035 into squad-standard-index\n"
     ]
    }
   ],
   "source": [
    "populate_index(es_obj=es, index_name=index_name, evidence_corpus=wiki_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wahoo! We now have some documents loaded into into an index. Elasticsearch provides a rich [query language](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html) that supports a diverse range of query types. For this example, we'll use the standard query for performing full-text search called a \"match\" query. By default, Elasticsearch sorts and returns a JSON reponse of search results based on a computed a [relevance score](https://qbox.io/blog/practical-guide-elasticsearch-scoring-relevancy#:~:text=Together%2C%20these%20combine%20into%20a,number%20known%20as%20the%20_score.) which indicates how well a given document matches the query. Along with the relevance score of each matched document, the search response also includes the amount of time the query took to run.\n",
    "\n",
    "Let's look at a simple match query used to search the `document_text` field in our newly created index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_es(es_obj, index_name, query, n_results):\n",
    "    '''\n",
    "    Execute an Elasticsearch query on a specified index\n",
    "    \n",
    "    Args:\n",
    "        es_obj - Elasticsearch client object\n",
    "        index_name (str) - Name of index to query\n",
    "        query (dict) - Query DSL\n",
    "        n_results (int) - Number of results to return\n",
    "        \n",
    "    Returns\n",
    "        res - Elasticsearch response object\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    res = es_obj.search(index=index_name, body=query, size=n_results)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_text = 'Who was the first president of the Republic of China?'\n",
    "\n",
    "# construct query\n",
    "query = {\n",
    "        'query': {\n",
    "            'match': {\n",
    "                'document_text': question_text\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "# execute query\n",
    "res = search_es(es_obj=es, index_name='squad-standard-index', query=query, n_results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was the first president of the Republic of China?\n",
      "Query Duration: 9 milliseconds\n",
      "Title, Relevance Score:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Modern_history_54', 23.007595),\n",
       " ('Nanjing_18', 16.980503),\n",
       " ('Republic_of_the_Congo_10', 16.721775),\n",
       " ('Prime_minister_16', 16.045874),\n",
       " ('Korean_War_29', 15.702344),\n",
       " ('Korean_War_43', 15.4945345),\n",
       " ('Qing_dynasty_52', 15.195876),\n",
       " ('Chinese_characters_55', 14.69559),\n",
       " ('Korean_War_23', 14.64461),\n",
       " ('2008_Sichuan_earthquake_48', 14.349867)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Question: {question_text}')\n",
    "print(f'Query Duration: {res[\"took\"]} milliseconds')\n",
    "print('Title, Relevance Score:')\n",
    "[(hit['_source']['document_title'], hit['_score']) for hit in res['hits']['hits']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check that all questions are answerable\n",
    "\n",
    "wiki_dict = {rec['document_title']:rec['document_text'] for rec in wiki_articles}\n",
    "any([ex['short_answer'] not in wiki_dict[ex['document_title']] for ex in qa_records])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Retriever Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we now have a basic understanding of how to use Elasticsearch as an IR tool to return some results for a given question, but how do we know if it's working? How do we evaluate what a good IR tool looks like? Like we pointed out in the introduction of this post, if the Retriever component of our QA system doesn't provide the correct passage to the Reader, we are doomed from the start.\n",
    "\n",
    "To evaluate how well our Retriever is working, we'll need two things: some labeled examples (i.e. SQuAD2.0 question/answer pairs) and some performance metrics. In the traditional world of information retrieval, there are [many metrics](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)) used to quantify the relevance of a set of query results that are largely centered around the concepts of precision and recall. For IR in the context of question answering, we adapt some of these ideas into two metrics: recall and mean average precision (mAP). Additionally, we evaluate the amount of time required to execute a query since the main point of having a two stage QA system is to efficiently narrow the large search space for our machine comprehension Reader.\n",
    "\n",
    "#### Recall\n",
    "\n",
    "In a traditional sense of IR, recall indicates the fraction of retrieved documents that are relevant to the query. In the context of end-to-end QA systems sitting on large corpora of documents, we are less concerned with finding *all* of the passages containing the answer (because it would take significant time to read through all of them anyway) and more concerned with the binary presence of a passage containing the correct answer being returned. In that light, we define a Retriever's recall as the *percentage of questions for which the answer segment appears in one of the top N pages returned by the search method.*\n",
    "\n",
    "#### Mean Average Precision\n",
    "\n",
    "While the recall metric focuses on the minimum viable result set to enable the Reader for success, we do still care about the composition of that result set. We want a metric that rewards a Retriever for a.) returning a lot of answer-containing documents in the result set (i.e. traditional meaning of precision) and b.) returning those answer-containing documents higher up in the result set than non-answer-containing documents (i.e. ranking them correctly). This is precisely what mean average precision (mAP) does for us. \n",
    "\n",
    "To explain mAP further, let's first break down the concept of average precision. If our Retriever is asked to return N documents and the total number of those N documents that actually contains the answer is m, then average precision (AP) is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "AP@N = \\frac{1}{m} \\sum_{k=1}^N (P(k) \\text{ if the }k^{th} \\text{ item contains the answer)} = \\frac{1}{m} \\sum_{k=1}^N P(k)*rel(k)\n",
    "\\end{equation*}\n",
    "\n",
    "where $rel(k)$ is just a binary indication of whether the $k^{th}$ item contains the correct segment or not. Using a concrete example, consider retrieving $N=3$ documents, of which one actually contains the correct answer segment. Here are three scenarios for how this could happen:\n",
    "\n",
    "| Scenario | Binary Indication | Precision @k's |     Average Precision @N     |\n",
    "|:--------:|:-----------------:|:--------------:|:----------------------------:|\n",
    "|     A    |     [1, 0, 0]     |   [1/1, 0, 0]  |   (1/1)*[(1/1) + 0 + 0] = 1  |\n",
    "|     B    |     [0, 1, 0]     |   [0, 1/2, 0]  |  (1/1)*[0 + (1/2) + 0] = 0.5 |\n",
    "|     C    |     [0, 0, 1]     |   [0, 0, 1/3]  | (1/1)*[0 + 0 + (1/3)] = 0.33 |\n",
    "\n",
    "Despite the fact that in each scenario we only have one document containing the correct answer, Scenario A is rewarded with the highest score because it was able to correctly rank the ground truth document relative to the others returned. Since average precision is calculated on a per query basis, the mean average precision is simply just the average AP across all queries. Now using our Wikipedia article index, let's define a function called `evaluate_retriever` to loop through all quesion/answer examples from the SQuAD2.0 train set and see how well our Elasticsearch retriever peforms in terms of recall, mAP, and averege query duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def average_precision(binary_results):\n",
    "    \n",
    "    ''' Calculates the average precision for a list of binary indicators '''\n",
    "    \n",
    "    m = 0\n",
    "    precs = []\n",
    "\n",
    "    for i, val in enumerate(binary_results):\n",
    "        if val == 1:\n",
    "            m += 1\n",
    "            precs.append(sum(binary_results[:i+1])/(i+1))\n",
    "            \n",
    "    ap = (1/m)*np.sum(precs) if m else 0\n",
    "            \n",
    "    return ap\n",
    "\n",
    "\n",
    "def evaluate_retriever(es_obj, index_name, qa_records, n_results):\n",
    "    '''\n",
    "    This function loops through a set of question/answer examples from SQuAD2.0 and \n",
    "    evaluates Elasticsearch as a information retrieval tool in terms of recall, mAP, and query duration.\n",
    "    \n",
    "    Args:\n",
    "        es_obj\n",
    "        index_name (str)\n",
    "        qa_records (list) - list of qa_records from preprocessing steps\n",
    "        n_results (int) - the number of results ElasticSearch should return for a given query\n",
    "        \n",
    "    Returns:\n",
    "        test_results_df (pd.DataFrame) - a dataframe recording search results info for every example in qa_records\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, qa in enumerate(tqdm(qa_records)):\n",
    "        \n",
    "        ex_id = qa['example_id']\n",
    "        question = qa['question_text']\n",
    "        answer = qa['short_answer']\n",
    "        \n",
    "        # construct and execute query\n",
    "        query = {\n",
    "                'query': {\n",
    "                    'match': {\n",
    "                        'document_text': question\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "        \n",
    "        res = search_es(es_obj=es_obj, index_name=index_name, query=query, n_results=n_results)\n",
    "        \n",
    "        # calculate performance metrics from query response info\n",
    "        duration = res['took']\n",
    "        binary_results = [int(answer.lower() in doc['_source']['document_text'].lower()) for doc in res['hits']['hits']]\n",
    "        ans_in_res = int(any(binary_results))\n",
    "        ap = average_precision(binary_results)\n",
    "\n",
    "        rec = (ex_id, question, answer, duration, ans_in_res, ap)\n",
    "        results.append(rec)\n",
    "    \n",
    "    # format results\n",
    "    cols = ['example_id', 'question', 'answer', 'query_duration', 'answer_present', 'average_precision']\n",
    "    \n",
    "    results_df = pd.DataFrame(results, columns=cols)\n",
    "    \n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea761efe2783461dbab22d67bf2c29ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86821.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# filter out SQuAD records that do not have a short answer for the given question\n",
    "qa_records_answerable = [record for record in qa_records if record['short_answer'] != '']\n",
    "\n",
    "results_df = evaluate_retriever(es_obj=es, index_name=index_name, qa_records=qa_records_answerable, n_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>query_duration</th>\n",
       "      <th>answer_present</th>\n",
       "      <th>average_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>2003</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 example_id  \\\n",
       "0  56be85543aeaaa14008c9063   \n",
       "1  56be85543aeaaa14008c9065   \n",
       "2  56be85543aeaaa14008c9066   \n",
       "3  56bf6b0f3aeaaa14008c9601   \n",
       "4  56bf6b0f3aeaaa14008c9602   \n",
       "\n",
       "                                            question               answer  \\\n",
       "0           When did Beyonce start becoming popular?    in the late 1990s   \n",
       "1  What areas did Beyonce compete in when she was...  singing and dancing   \n",
       "2  When did Beyonce leave Destiny's Child and bec...                 2003   \n",
       "3      In what city and state did Beyonce  grow up?        Houston, Texas   \n",
       "4         In which decade did Beyonce become famous?           late 1990s   \n",
       "\n",
       "   query_duration  answer_present  average_precision  \n",
       "0               1               0                0.0  \n",
       "1               6               0                0.0  \n",
       "2               4               1                1.0  \n",
       "3               4               0                0.0  \n",
       "4               4               0                0.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall / mAP / Average Duration\n",
      "0.8601951140853019 0.7523912705451445 3.207461328480437\n"
     ]
    }
   ],
   "source": [
    "print('Recall / mAP / Average Duration')\n",
    "print(results_df.answer_present.value_counts(normalize=True)[1],\\\n",
    "results_df.average_precision.mean(),\n",
    "results_df.query_duration.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Search Results with Custom Analyzers & Query Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***PLACEHOLDER CONTENT***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different approaches to improving the Retreiver component of a QA system: (http://staffwww.dcs.shef.ac.uk/people/M.Greenwood/nlp/pubs/gaizauskas_sigirforum_2004d.pdf)\n",
    "1. preprocessing the question in creating the IR query;\n",
    "2. preprocessing the collection to identify significant information that can be included in the indexation for retrieval;\n",
    "3. adapting the similarity metric used in selecting documents;\n",
    "4. modifying the form of retrieval return, e.g. to deliver passages rather than whole documents.\n",
    "5. Re-ranking passages fed to the retriever (https://www.aclweb.org/anthology/D18-1053.pdf) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to modify for improvement:\n",
    "- custom analyzer - stopword removal\n",
    "- mulitmatch query on title and body (maybe weighted)\n",
    "- NER + phrase match in custom query\n",
    "- MAYBE: synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ElasticSearch comes with several built-in Analyzers that satisfy common use cases. However, custom Analyzers can also be crafted by combining specific character filters, tokenizers, and token filters to best suit any unique dataset. As explained above, Analyzers are applied at index time to pre-process documents before indexing. In addition, Analyzers can also be applied at search time to process text queries according to the same logic the candidate documents were processed with. Search time analysis can be customized and is only applied to certain query types such as `match` queries. Lets take a closer look at how Analyzers work with ElasticSearch's Analyze API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build new index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'squad-standard-stopwords-index'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_config = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"stop_analyzer\": {\n",
    "                    \"type\": \"standard\",\n",
    "                    \"stopwords\": \"_english_\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"strict\", \n",
    "        \"properties\": {\n",
    "            \"document_title\": {\"type\": \"text\", \"analyzer\": \"stop_analyzer\"},\n",
    "            \"document_text\": {\"type\": \"text\", \"analyzer\": \"stop_analyzer\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "index_name = 'squad-standard-stopwords-index'\n",
    "es.indices.create(index=index_name, body=index_config, ignore=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5686d1fb1543b3832ca9129b3d7fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Succesfully loaded 19035 into squad-standard-stopwords-index\n"
     ]
    }
   ],
   "source": [
    "populate_index(es_obj=es, index_name='squad-standard-stopwords-index', evidence_corpus=wiki_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import Q, Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "def search_es2(es_obj, index_name, question_text, n_results):\n",
    "    '''\n",
    "    Execute an Elasticsearch query on a specified index\n",
    "    \n",
    "    Args:\n",
    "        es_obj - Elasticsearch client object\n",
    "        index_name (str) - Name of index to query\n",
    "        query (dict) - Query DSL\n",
    "        n_results (int) - Number of results to return\n",
    "        \n",
    "    Returns\n",
    "        res - Elasticsearch response object\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    sub_queries = []\n",
    "\n",
    "    # define question subquery to match on document title and document text\n",
    "    qq = Q('multi_match',\n",
    "            query=question_text,\n",
    "            type='most_fields',\n",
    "            fields=['document_title', 'document_text'])\n",
    "    \n",
    "    sub_queries.append(qq)\n",
    "    \n",
    "    \n",
    "    # extract entities from question using spacy\n",
    "    doc = nlp(question)\n",
    "    entity_list = [entity.text.lower() for entity in doc.ents]\n",
    "    \n",
    "    # define entity subqueries to do exact phrase match on any entities in the question\n",
    "    for ent in entity_list:\n",
    "        eq = Q('multi_match',\n",
    "                query=ent,\n",
    "                type='phrase',\n",
    "                fields=['document_title','document_text'])\n",
    "\n",
    "        sub_queries.append(eq)\n",
    "    \n",
    "    # combine subqueries into bool query\n",
    "    q = Q('bool', should=[*sub_queries])\n",
    "    \n",
    "    # execute query\n",
    "    s = Search(using=es_obj, index=index_name)\n",
    "    res = s.query(q).execute()\n",
    "    \n",
    "    return res, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'When did Kendrick Lamars first album come out?'\n",
    "\n",
    "res, query = search_es2(es_obj=es, index_name='squad-standard-index', question_text=question, n_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bool': {'should': [{'multi_match': {'query': 'When did Kendrick Lamars first album come out?',\n",
       "     'type': 'most_fields',\n",
       "     'fields': ['document_title', 'document_text']}},\n",
       "   {'multi_match': {'query': 'kendrick lamars',\n",
       "     'type': 'phrase',\n",
       "     'fields': ['document_title', 'document_text']}},\n",
       "   {'multi_match': {'query': 'first',\n",
       "     'type': 'phrase',\n",
       "     'fields': ['document_title', 'document_text']}}]}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Result(squad-standard-index/_doc/6097): {'document_title': 'Hard_rock_2', 'document_text': \"Hard_roc...}>,\n",
       " <Result(squad-standard-index/_doc/720): {'document_title': 'Kanye_West_26', 'document_text': 'Kanye_...}>,\n",
       " <Result(squad-standard-index/_doc/9146): {'document_title': 'Queen_(band)_37', 'document_text': 'Quee...}>,\n",
       " <Result(squad-standard-index/_doc/6112): {'document_title': 'Hard_rock_17', 'document_text': 'Hard_ro...}>,\n",
       " <Result(squad-standard-index/_doc/999): {'document_title': 'American_Idol_77', 'document_text': 'Ame...}>,\n",
       " <Result(squad-standard-index/_doc/712): {'document_title': 'Kanye_West_18', 'document_text': 'Kanye_...}>,\n",
       " <Result(squad-standard-index/_doc/18051): {'document_title': 'Party_leaders_of_the_United_States_House...}>,\n",
       " <Result(squad-standard-index/_doc/9115): {'document_title': 'Queen_(band)_6', 'document_text': 'Queen...}>,\n",
       " <Result(squad-standard-index/_doc/9116): {'document_title': 'Queen_(band)_7', 'document_text': \"Queen...}>,\n",
       " <Result(squad-standard-index/_doc/701): {'document_title': 'Kanye_West_7', 'document_text': 'Kanye_W...}>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retriever3(es_obj, index_name, qa_records, n_results):\n",
    "    '''\n",
    "    This function loops through a set of question/answer examples from SQuAD2.0 and \n",
    "    evaluates Elasticsearch as a information retrieval tool in terms of recall, mAP, and query duration.\n",
    "    \n",
    "    Args:\n",
    "        es_obj\n",
    "        index_name (str)\n",
    "        qa_records (list) - list of qa_records from preprocessing steps\n",
    "        n_results (int) - the number of results ElasticSearch should return for a given query\n",
    "        \n",
    "    Returns:\n",
    "        test_results_df (pd.DataFrame) - a dataframe recording search results info for every example in qa_records\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, qa in enumerate(tqdm(qa_records)):\n",
    "        \n",
    "        ex_id = qa['example_id']\n",
    "        question = qa['question_text']\n",
    "        answer = qa['short_answer']\n",
    "        \n",
    "        res, qry = search_es2(es_obj=es_obj, index_name=index_name, question_text=question, n_results=n_results)\n",
    "        \n",
    "        # calculate performance metrics from query response info\n",
    "        duration = res.took\n",
    "        binary_results = [int(answer.lower() in doc['document_text'].lower()) for doc in res.hits]\n",
    "        ans_in_res = int(any(binary_results))\n",
    "        ap = average_precision(binary_results)\n",
    "\n",
    "        rec = (ex_id, question, answer, duration, ans_in_res, ap)\n",
    "        results.append(rec)\n",
    "    \n",
    "    # format results\n",
    "    cols = ['example_id', 'question', 'answer', 'query_duration', 'answer_present', 'average_precision']\n",
    "    \n",
    "    results_df = pd.DataFrame(results, columns=cols)\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc85642f08774064a7a06049a1b07fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86821.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ner_results = evaluate_retriever3(es_obj=es, index_name='squad-standard-index', qa_records=qa_records_answerable, n_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall / mAP / Average Duration\n",
      "0.8938044943043734 0.7315383558286108 4.893228596768063\n"
     ]
    }
   ],
   "source": [
    "print('Recall / mAP / Average Duration')\n",
    "print(ner_results.answer_present.value_counts(normalize=True)[1],\\\n",
    "ner_results.average_precision.mean(),\n",
    "ner_results.query_duration.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.took"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hard_rock Bon Jovi's third album, Slippery When Wet (1986), mixed hard rock with a pop sensitivity and spent a total of 8 weeks at the top of the Billboard 200 album chart, selling 12 million copies in the US while becoming the first hard rock album to spawn three top 10 singles — two of which reached number one. The album has been credited with widening the audiences for the genre, particularly by appealing to women as well as the traditional male dominated audience, and opening the door to MTV and commercial success for other bands at the end of the decade. The anthemic The Final Countdown (1986) by Swedish group Europe was an international hit, reaching number eight on the US charts while hitting the top 10 in nine other countries. This era also saw more glam-infused American hard rock bands come to the forefront, with both Poison and Cinderella releasing their multi-platinum début albums in 1986. Van Halen released 5150 (1986), their first album with Sammy Hagar on lead vocals, which was number one in the US for three weeks and sold over 6 million copies. By the second half of the decade, hard rock had become the most reliable form of commercial popular music in the United States.\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.hits[0]['document_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c08e66a3468490da280254308d67a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86821.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stop_results_df = evaluate_retriever(es_obj=es, index_name=index_name, qa_records=qa_records_answerable, n_results=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9262851153522765"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall\n",
    "results_df.answer_present.value_counts(normalize=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8226511059152355"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_results_df.average_precision.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact of Retriever in End-to-End QA System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I do chunk into smaller passages, we could just do all evaluation on this same dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'squad-standard-paragraph-index'}"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_config = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"standard_analyzer\": {\n",
    "                    \"type\": \"standard\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"strict\", \n",
    "        \"properties\": {\n",
    "            \"document_title\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"},\n",
    "            \"document_text\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "index_name = 'squad-standard-paragraph-index'\n",
    "es.indices.create(index=index_name, body=index_config, ignore=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dec0090386d4e3b8f786dc5d8bbb178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Succesfully loaded 19035 into squad-standard-paragraph-index\n"
     ]
    }
   ],
   "source": [
    "populate_index(es_obj=es, index_name=index_name, evidence_corpus=wiki_articles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2719df53fa74717a8228c1165156365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86821.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "paragraph_results_df = evaluate_retriever(es_obj=es, index_name='squad-standard-paragraph-index', qa_records=qa_records_answerable, n_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8601951140853019 0.7523912705451445 3.080591101231269\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(paragraph_results_df.answer_present.value_counts(normalize=True)[1],\\\n",
    "paragraph_results_df.average_precision.mean(),\n",
    "paragraph_results_df.query_duration.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'squad-standardstop-paragraph-index'}"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_config = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"stop_analyzer\": {\n",
    "                    \"type\": \"standard\",\n",
    "                    \"stopwords\": \"_english_\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"strict\", \n",
    "        \"properties\": {\n",
    "            \"document_title\": {\"type\": \"text\", \"analyzer\": \"stop_analyzer\"},\n",
    "            \"document_text\": {\"type\": \"text\", \"analyzer\": \"stop_analyzer\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "index_name = 'squad-standardstop-paragraph-index'\n",
    "es.indices.create(index=index_name, body=index_config, ignore=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b3db77275347e7b241a4146d3f8559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Succesfully loaded 19035 into squad-standardstop-paragraph-index\n"
     ]
    }
   ],
   "source": [
    "populate_index(es_obj=es, index_name='squad-standardstop-paragraph-index', evidence_corpus=wiki_articles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2e119666e74ec7bc611e78245aaea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86821.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "paragraph_stop_results_df = evaluate_retriever(es_obj=es, index_name='squad-standardstop-paragraph-index', qa_records=qa_records_answerable, n_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.857914559841513 0.749556190962504 1.066170626922058\n"
     ]
    }
   ],
   "source": [
    "print(paragraph_stop_results_df.answer_present.value_counts(normalize=True)[1],\\\n",
    "paragraph_stop_results_df.average_precision.mean(),\n",
    "paragraph_stop_results_df.query_duration.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0202d41d11284afb9ecc6608d275a820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86821.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RequestError",
     "evalue": "RequestError(400, 'parsing_exception', \"[match] query doesn't support multiple fields, found [document_text] and [cutoff_frequency]\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRequestError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-362-a0b2967faca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparagraph_commonterms_results_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes_obj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'squad-standard-paragraph-index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa_records\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqa_records_answerable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-361-7cbca25798e3>\u001b[0m in \u001b[0;36mevaluate_retriever\u001b[0;34m(es_obj, index_name, qa_records, n_results)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 }\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mes_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# calculate performance metrics from query response info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qa-retriever-eval/lib/python3.7/site-packages/elasticsearch/client/utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qa-retriever-eval/lib/python3.7/site-packages/elasticsearch/client/__init__.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, body, index, doc_type, params, headers)\u001b[0m\n\u001b[1;32m   1621\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m         )\n\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qa-retriever-eval/lib/python3.7/site-packages/elasticsearch/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    360\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m                 )\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qa-retriever-eval/lib/python3.7/site-packages/elasticsearch/connection/http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             )\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         self.log_request_success(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qa-retriever-eval/lib/python3.7/site-packages/elasticsearch/connection/base.py\u001b[0m in \u001b[0;36m_raise_error\u001b[0;34m(self, status_code, raw_data)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         raise HTTP_EXCEPTIONS.get(status_code, TransportError)(\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         )\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRequestError\u001b[0m: RequestError(400, 'parsing_exception', \"[match] query doesn't support multiple fields, found [document_text] and [cutoff_frequency]\")"
     ]
    }
   ],
   "source": [
    "paragraph_commonterms_results_df = evaluate_retriever(es_obj=es, index_name='squad-standard-paragraph-index', qa_records=qa_records_answerable, n_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(index='squad-standard-paragraph-index', ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['corpus2', 'nq_wiki_data_test', 'squad_corpus1', 'corpus0', 'nq_wiki_data', 'corpus4', 'squad_corpus2', 'cases', 'squad_corpus3', 'demo_index_fullsys', 'corpus3', 'baseline', 'squad_corpus0', 'squad_corpus4', 'corpus1', 'demo_index'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.get_alias('*').keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "In0YUapuBQ8A",
    "pKVJVU5hBQ8U",
    "t4s4Bx3LBQ8p"
   ],
   "name": "2020-06-09-Evaluating_BERT_on_SQuAD.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
