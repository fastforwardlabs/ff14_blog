{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating QA: Metrics, Predictions, and the Null Response\n",
    "> A deep dive into computing QA predictions and when to tell BERT to zip it! \n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [PyTorch, Hugging Face, Wikipedia, BERT, Transformers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "* A basic understanding of Transformers and PyTorch\n",
    "* A Transformer fine-tuned on SQuAD2.0\n",
    "* The SQuAD2.0 dev set\n",
    "\n",
    "### What you'll learn\n",
    "* Metrics for evaluating QA performance\n",
    "* How to evaluate BERT on SQuAD2.0\n",
    "* How to handle the Null Response -- when a question doesn't have answer in the passage\n",
    "* Implementing a more robust answering method for your QA system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our last post, [Building a QA System with BERT on Wikipedia](https://qa.fastforwardlabs.com/pytorch/hugging%20face/wikipedia/bert/transformers/2020/05/19/Getting_Started_with_QA.html), we used the HuggingFace framework to train BERT on the SQuAD2.0 dataset and built a simple QA system on top of the Wikipedia search engine. This time, we'll look at the quality of BERT for Question Answering. We'll cover what metrics are used to quantify quality, how to evaluate BERT with the Hugging Face framework and the importance of the \"null response\" -- when a question does not have an answer -- for both improved performance and more realistic QA output. \n",
    "\n",
    "\n",
    "# Answering questions is complicated\n",
    "Quantifying the success of answering a question is a tricky task. When you or I ask a question, the correct answer could take multiple forms. Let's look at an example. \n",
    "\n",
    "In our previous post, BERT answered the question, \"Why is the sky blue?\" with \"Rayleigh scattering\" but another correct answer would be \n",
    "\n",
    "\"The Earth's atmosphere scatters short-wavelength light more efficiently than that of longer wavelengths. Because its wavelengths are shorter, blue light is more strongly scattered than the longer-wavelength lights, red or green. Hence the result that when looking at the sky away from the direct incident sunlight, the human eye perceives the sky to be blue.\" \n",
    "\n",
    "Both of these passages can be found in the Wikipedia article [Diffuse Sky Radiation](https://en.wikipedia.org/wiki/Diffuse_sky_radiation) and both are correct. However, I've also had a model return \"because its wavelengths are shorter\" as the answer, which is close but not really a correct answer: the sky itself doesn't have a wavelength -- this answer is missing too much context to be useful. \n",
    "\n",
    "How should we judge a model when there can be multiple correct answers and even more incorrect answers? To properly assess the quality of a model we need a gold standard set of questions and answers! Let's turn back to the SQuAD dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More about SQuAD\n",
    "The SQuAD dataset comes in two flavors: SQuAD1.1 and SQuAD2.0. The latter contains the same questions and answers as the former but also includes additional questions that _cannot_ be answered by the accompanying passage. This is intended to provide a more realistic question answering task because often times there really won't be an answer to the question in the document we're parsing. This ability to properly identify when a question does not have an answer is much more challenging for Transformer models and it's why we focused on this dataset rather than SQuAD1.1. \n",
    "\n",
    "SQuAD2.0 consists of more than 130k questions, of which a full third do not have an answer that can be found in the associated passage. The dev set in particular contains a 50/50 split of answerable questions. SQuAD examples consist of question - context pairs. The context is a single paragraph from a Wikipedia article. Each paragraph will have several questions (both answerable and unanswerable) associated with it. Paragraphs are drawn from 35 Wikipedia articles. Every paragraph in the article has at least one question associated with it. The full SQuAD stats are shown below from the paper XXX. \n",
    "\n",
    "![](my_icons/squad_datasets.png \"SQuAD Stats\")\n",
    "\n",
    "Last time we fine-tuned the model on the SQuAD2.0 train set. This time we'll evaluate the model on the SQuAD2.0 dev set.\n",
    "\n",
    "\n",
    "Use the hidden cells below to get set set up, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "\n",
    "# use this cell to install packages if needed\n",
    "!pip install torch  torchvision -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# This is the directory in which we'll store all evaluation output\n",
    "model_dir = \"models/distilbert/twmkn9_distilbert-base-uncased-squad2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "\n",
    "# Download the SQuAD2.0 dev set\n",
    "!wget -P data/squad/ https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the SQuAD 2.0 dev set using HF data processors\n",
    "\n",
    "HuggingFace provide the [Processors](https://huggingface.co/transformers/main_classes/processors.html) library for fascilitating basic processing tasks with some canonical NLP datasets. The processors can be used for loading datasets and converting their examples to features for direct use in the model. We'll be using the [SQuAD processors](https://huggingface.co/transformers/main_classes/processors.html#squad). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:05<00:00,  6.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers.data.processors.squad import SquadV2Processor\n",
    "\n",
    "# This processor loads the SQuAD2.0 dev set examples\n",
    "processor = SquadV2Processor()\n",
    "examples = processor.get_dev_examples(\"data/squad/\", filename=\"dev-v2.0.json\")\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `examples` is a list, most other tasks we'll work with use a unique identifier - one for each question in the dev set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some maps to help us identify examples of interest\n",
    "qid_to_example_index = {example.qas_id: i for i, example in enumerate(examples)}\n",
    "qid_to_has_answer = {example.qas_id: bool(example.answers) for example in examples}\n",
    "answer_qids = [qas_id for qas_id, has_answer in qid_to_has_answer.items() if has_answer]\n",
    "no_answer_qids = [qas_id for qas_id, has_answer in qid_to_has_answer.items() if not has_answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_example(qid):    \n",
    "    idx = qid_to_example_index[qid]\n",
    "    q = examples[idx].question_text\n",
    "    c = examples[idx].context_text\n",
    "    a = [answer['text'] for answer in examples[idx].answers]\n",
    "    \n",
    "    print(f'Example {idx} of {len(examples)}\\n---------------------')\n",
    "    print(f\"Q: {q}\\n\")\n",
    "    print(f\"Context: \\n{c}\\n\")\n",
    "    print(f\"True Answers:\\n{a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A SQuAD example \n",
    "\n",
    "Approximately 50% of the examples in the dev set are questions that have answers contained within their corresponding passage. In these cases, up to five possible correct answers are provided (questions and answers were generated and identified by crowd-sourced workers). Answers must be direct excerpts from the passage but we can see there are several ways to have a \"correct\" answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2548 of 11873\n",
      "---------------------\n",
      "Q: Where on Earth is free oxygen found?\n",
      "\n",
      "Context: \n",
      "Free oxygen also occurs in solution in the world's water bodies. The increased solubility of O\n",
      "2 at lower temperatures (see Physical properties) has important implications for ocean life, as polar oceans support a much higher density of life due to their higher oxygen content. Water polluted with plant nutrients such as nitrates or phosphates may stimulate growth of algae by a process called eutrophication and the decay of these organisms and other biomaterials may reduce amounts of O\n",
      "2 in eutrophic water bodies. Scientists assess this aspect of water quality by measuring the water's biochemical oxygen demand, or the amount of O\n",
      "2 needed to restore it to a normal concentration.\n",
      "\n",
      "True Answers:\n",
      "['water', \"in solution in the world's water bodies\", \"the world's water bodies\"]\n"
     ]
    }
   ],
   "source": [
    "display_example(answer_qids[1300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A SQuAD negative example\n",
    "\n",
    "The other half of the questions in dev set do not have an answer in the corresponding passage. These questions were generated by crowd-sourced workers to be related and relevant to the passage but unanswerable by that passage. There are thus no True Answers associated with these questions as we see in the example below. \n",
    "\n",
    "Note: In this case, the question is a trick -- the numbers are reoriented in a way that no longer holds true. Will the model pick up on that? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2564 of 11873\n",
      "---------------------\n",
      "Q: What happened 3.7-2 billion years ago?\n",
      "\n",
      "Context: \n",
      "Free oxygen gas was almost nonexistent in Earth's atmosphere before photosynthetic archaea and bacteria evolved, probably about 3.5 billion years ago. Free oxygen first appeared in significant quantities during the Paleoproterozoic eon (between 3.0 and 2.3 billion years ago). For the first billion years, any free oxygen produced by these organisms combined with dissolved iron in the oceans to form banded iron formations. When such oxygen sinks became saturated, free oxygen began to outgas from the oceans 3–2.7 billion years ago, reaching 10% of its present level around 1.7 billion years ago.\n",
      "\n",
      "True Answers:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "display_example(no_answer_qids[1254])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for QA\n",
    "\n",
    "There are two dominant metrics used by many question answering datasets: exact match (EM) and F1 score. These scores are computed on individual question+answer pairs. When multiple correct answers are possible for a given question (as in the previous example), the maximum score over all possible correct answers is computed. A model is given overall EM and F1 scores by averaging over the individual example scores.  \n",
    "\n",
    "\n",
    "### Exact Match\n",
    "This metric is as simple as it sounds. For each question+answer pair, if the characters of the model's prediction exactly match the characters of the known answer, EM = 1, otherwise EM = 0. This is a strict all-or-nothing metric; being off by a single character would result in a score of 0 for that prediciton. When assessing against a negative example, if the model predicts any text at all it automatically receives a 0 for that example. \n",
    "\n",
    "### F1 \n",
    "This metric can be found in many analyses so it shouldn't be a surprise to see here. F1 is the harmonic mean of the precision and recall. In this case, it's computed over the individual words in the prediction against those in the true answer. The number of shared words provides the basis of the f1 score.\n",
    "\n",
    "\n",
    "![](my_icons/f1score.png \"F1 score\")\n",
    "\n",
    "Let's see how these metrics work in practice. We'll load up a fine-tuned model and its tokenizer and compare our predictions against the True Answers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Transformer model fine-tuned on SQuAD 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"twmkn9/distilbert-base-uncased-squad2\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"twmkn9/distilbert-base-uncased-squad2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `get_prediction` method is essentially identical to what we used last time in our simple QA system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(qid):\n",
    "    # given a question id (qas_id or qid), load the example, get the model outputs and generate an answer\n",
    "    question = examples[qid_to_example_index[qid]].question_text\n",
    "    context = examples[qid_to_example_index[qid]].context_text\n",
    "\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    answer_start = torch.argmax(outputs[0])  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(outputs[1]) + 1 \n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some functions we'll need to compute our quality metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are heavily influenced by the HF squad_metrics.py script\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    \n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "def get_gold_answers(example):\n",
    "    \"\"\"helper function that retrieves all possible true answers from a squad2.0 example\"\"\"\n",
    "    \n",
    "    gold_answers = [answer[\"text\"] for answer in example.answers if answer[\"text\"]]\n",
    "\n",
    "    # if gold_answers doesn't exist it's because this is a negative example - \n",
    "    # the only correct answer is an empty string\n",
    "    if not gold_answers:\n",
    "        gold_answers = [\"\"]\n",
    "        \n",
    "    return gold_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we start by computing EM and F1 for our first example - the one that has several True Answers associated with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where on Earth is free oxygen found?\n",
      "Prediction: water bodies\n",
      "True Answers: ['water', \"in solution in the world's water bodies\", \"the world's water bodies\"]\n",
      "EM: 0 \t F1: 0.8\n"
     ]
    }
   ],
   "source": [
    "prediction = get_prediction(answer_qids[1300])\n",
    "example = examples[qid_to_example_index[answer_qids[1300]]]\n",
    "\n",
    "gold_answers = get_gold_answers(example)\n",
    "\n",
    "em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
    "f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
    "\n",
    "print(f\"Question: {example.question_text}\")\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"True Answers: {gold_answers}\")\n",
    "print(f\"EM: {em_score} \\t F1: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our prediction is actually quite close to some of the True Answers resulting in a respectable F1 score. However, it does not exactly match any of them so our EM score is 0. \n",
    "\n",
    "Let's try with our negative example now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What happened 3.7-2 billion years ago?\n",
      "Prediction: [CLS] what happened 3 . 7 - 2 billion years ago ? [SEP] free oxygen gas was almost nonexistent in earth ' s atmosphere before photosynthetic archaea and bacteria evolved , probably about 3 . 5 billion years ago . free oxygen first appeared in significant quantities during the paleoproterozoic eon ( between 3 . 0 and 2 . 3 billion years ago ) . for the first billion years , any free oxygen produced by these organisms combined with dissolved iron in the oceans to form banded iron formations . when such oxygen sinks became saturated , free oxygen began to outgas from the oceans\n",
      "True Answers: ['']\n",
      "EM: 0 \t F1: 0\n"
     ]
    }
   ],
   "source": [
    "prediction = get_prediction(no_answer_qids[1254])\n",
    "example = examples[qid_to_example_index[no_answer_qids[1254]]]\n",
    "\n",
    "gold_answers = get_gold_answers(example)\n",
    "\n",
    "em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
    "f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
    "\n",
    "print(f\"Question: {example.question_text}\")\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"True Answers: {gold_answers}\")\n",
    "print(f\"EM: {em_score} \\t F1: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. Both or metrics are zero because this model does not correctly asses that this question is unanswerable! Even worse, it seems to have catastrophically failed, including the entire question as part of the answer.  \n",
    "\n",
    "Now that we have the basics of computing QA metrics on a couple of examples, we need to assess the model on the entire dev set. Luckily, there's a script for that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a model on the SQuAD2.0 dev set with HF\n",
    "\n",
    "The same `run_squad.py` script we used to fine-tune a Transformer for question answering can also be used to evaluate the model! Below are the arguments you'll need to properly evaluate a fine-tuned model for question answering on the SQuAD dev set. Because we using SQuAD2.0 it is **crucial** that you include the `--version_2_with_negative` flag!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-02 19:14:41.314546: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2020-06-02 19:14:41.314635: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2020-06-02 19:14:41.314648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "06/02/2020 19:14:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "06/02/2020 19:14:42 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/twmkn9/distilbert-base-uncased-squad2/config.json from cache at /home/melanie/.cache/torch/transformers/a3920afc5879111d09b0c1e74e7e674b82a7cd96109c2094b1abd8e57f50a126.eba6ca264db629858c65c1b534a0393d738537aad02120c2ac4fa8dd94820d0b\n",
      "06/02/2020 19:14:42 - INFO - transformers.configuration_utils -   Model config DistilBertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"dim\": 768,\n",
      "  \"do_sample\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tie_weights_\": true,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522,\n",
      "  \"xla_device\": null\n",
      "}\n",
      "\n",
      "06/02/2020 19:14:42 - INFO - transformers.tokenization_utils -   Model name 'twmkn9/distilbert-base-uncased-squad2' not found in model shortcut name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad, distilbert-base-cased, distilbert-base-cased-distilled-squad, distilbert-base-german-cased, distilbert-base-multilingual-cased). Assuming 'twmkn9/distilbert-base-uncased-squad2' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "06/02/2020 19:14:43 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/twmkn9/distilbert-base-uncased-squad2/vocab.txt from cache at /home/melanie/.cache/torch/transformers/a6078a0000db15a7f37a4c934bf07641f2ce917d7e6d19e8250b7b087948d0bb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "06/02/2020 19:14:43 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/twmkn9/distilbert-base-uncased-squad2/added_tokens.json from cache at None\n",
      "06/02/2020 19:14:43 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/twmkn9/distilbert-base-uncased-squad2/special_tokens_map.json from cache at /home/melanie/.cache/torch/transformers/112632b7c7669e4ebb3bbda33d84526efe839633b66db00007aa1e616521262f.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "06/02/2020 19:14:43 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/twmkn9/distilbert-base-uncased-squad2/tokenizer_config.json from cache at /home/melanie/.cache/torch/transformers/ff910ad3f01a29bfc80c754346974acfe80b3ec108facc01c68a5b86f282bdf0.071a6c67df33940f411dddb070d8c2e69b425ca06b9d813035dae545ab90c616\n",
      "06/02/2020 19:14:43 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/twmkn9/distilbert-base-uncased-squad2/pytorch_model.bin from cache at /home/melanie/.cache/torch/transformers/3d3d52586249e1f364d8f19dd13d14226dffdeaa18bb3ff1625bd5389db027b0.a9ad8185141adee62a5e0039ff540a444c26257146ac66a7ce69b7ec018b7a7c\n",
      "06/02/2020 19:14:48 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='data/squad', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=5e-05, local_rank=-1, logging_steps=500, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='twmkn9/distilbert-base-uncased-squad2', model_type='distilbert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='models/distilbert/twmkn9_distilbert-base-uncased-squad2', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=8, predict_file='dev-v2.0.json', save_steps=500, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=True, warmup_steps=0, weight_decay=0.0)\n",
      "06/02/2020 19:14:48 - INFO - __main__ -   Loading checkpoint twmkn9/distilbert-base-uncased-squad2 for evaluation\n",
      "06/02/2020 19:14:48 - INFO - __main__ -   Evaluate the following checkpoints: ['twmkn9/distilbert-base-uncased-squad2']\n",
      "06/02/2020 19:14:48 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/twmkn9/distilbert-base-uncased-squad2/config.json from cache at /home/melanie/.cache/torch/transformers/a3920afc5879111d09b0c1e74e7e674b82a7cd96109c2094b1abd8e57f50a126.eba6ca264db629858c65c1b534a0393d738537aad02120c2ac4fa8dd94820d0b\n",
      "06/02/2020 19:14:48 - INFO - transformers.configuration_utils -   Model config DistilBertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"dim\": 768,\n",
      "  \"do_sample\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tie_weights_\": true,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522,\n",
      "  \"xla_device\": null\n",
      "}\n",
      "\n",
      "06/02/2020 19:14:49 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/twmkn9/distilbert-base-uncased-squad2/pytorch_model.bin from cache at /home/melanie/.cache/torch/transformers/3d3d52586249e1f364d8f19dd13d14226dffdeaa18bb3ff1625bd5389db027b0.a9ad8185141adee62a5e0039ff540a444c26257146ac66a7ce69b7ec018b7a7c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/02/2020 19:14:50 - INFO - __main__ -   Creating features from dataset file at data/squad\n",
      "100%|███████████████████████████████████████████| 35/35 [00:05<00:00,  6.54it/s]\n",
      "convert squad examples to features: 100%|█| 11873/11873 [02:02<00:00, 97.08it/s]\n",
      "add example index and unique id: 100%|█| 11873/11873 [00:00<00:00, 461404.92it/s\n",
      "06/02/2020 19:16:59 - INFO - __main__ -   Saving features into cached file data/squad/cached_dev_distilbert-base-uncased-squad2_384\n",
      "06/02/2020 19:17:20 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "06/02/2020 19:17:20 - INFO - __main__ -     Num examples = 12232\n",
      "06/02/2020 19:17:20 - INFO - __main__ -     Batch size = 12\n",
      "Evaluating: 100%|███████████████████████████| 1020/1020 [02:23<00:00,  7.13it/s]\n",
      "06/02/2020 19:19:43 - INFO - __main__ -     Evaluation done in total 143.126918 secs (0.011701 sec per example)\n",
      "06/02/2020 19:19:43 - INFO - transformers.data.metrics.squad_metrics -   Writing predictions to: models/distilbert/twmkn9_distilbert-base-uncased-squad2/predictions_.json\n",
      "06/02/2020 19:19:43 - INFO - transformers.data.metrics.squad_metrics -   Writing nbest to: models/distilbert/twmkn9_distilbert-base-uncased-squad2/nbest_predictions_.json\n",
      "06/02/2020 19:20:46 - INFO - __main__ -   Results: {'exact': 66.25958056093658, 'f1': 69.66994428499025, 'total': 11873, 'HasAns_exact': 68.91025641025641, 'HasAns_f1': 75.74076391627662, 'HasAns_total': 5928, 'NoAns_exact': 63.61648444070648, 'NoAns_f1': 63.61648444070648, 'NoAns_total': 5945, 'best_exact': 66.25958056093658, 'best_exact_thresh': 0.0, 'best_f1': 69.66994428499046, 'best_f1_thresh': 0.0}\n"
     ]
    }
   ],
   "source": [
    "!python run_squad.py  \\\n",
    "    --model_type distilbert   \\\n",
    "    --model_name_or_path twmkn9/distilbert-base-uncased-squad2  \\\n",
    "    --output_dir models/distilbert/twmkn9_distilbert-base-uncased-squad2 \\\n",
    "    --data_dir data/squad   \\\n",
    "    --predict_file dev-v2.0.json   \\\n",
    "    --do_eval   \\\n",
    "    --version_2_with_negative \\\n",
    "    --do_lower_case  \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've evaluated a `distilbert` model that was fine-tuned on SQuAD2.0 by a member of the NLP community. When running the evaluation we see a number of steps performed: \n",
    "\n",
    "1. the dev set is loaded from disk \n",
    "2. the examples are converted to features that can be directly fed to the model\n",
    "3. these features are cached to disk\n",
    "4. Evaluation proceeds in batches of 12 and finishes in about 2.5 minutes (this is because distilBERT is much faster and more lightweight than BERT)\n",
    "5. a slew of prediction outputs are written to disk\n",
    "6. Overall model results are displayed\n",
    "\n",
    "We'll focus on this last piece of information: the overall model results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = {\n",
    "    # a) Scores averaged over all examples in the dev set\n",
    "    'exact': 66.25958056093658,         \n",
    "    'f1': 69.66994428499025,            \n",
    "    'total': 11873,  # number of examples in the dev set\n",
    "    \n",
    "    # b) Scores averaged over only positive examples (have answers)\n",
    "    'HasAns_exact': 68.91025641025641,  \n",
    "    'HasAns_f1': 75.74076391627662,     \n",
    "    'HasAns_total': 5928, # number of positive examples\n",
    "    \n",
    "    # c) Scores averaged over only negative examples (no answers)\n",
    "    'NoAns_exact': 63.61648444070648, \n",
    "    'NoAns_f1': 63.61648444070648, \n",
    "    'NoAns_total': 5945, # number of negative examples\n",
    "    \n",
    "    # d) Given probabilities of no-answer for each example, what would the best scores and thresholds be?\n",
    "    'best_exact': 66.25958056093658, \n",
    "    'best_exact_thresh': 0.0, \n",
    "    'best_f1': 69.66994428499046, \n",
    "    'best_f1_thresh': 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three blocks of the `Results` output are pretty straightforward. EM and F1 scores are reported over a) the full dev set, b) the set of positive examples, and c) the set of negative examples. This can give you some insight into whether your model is performing adequately on both answer and no-answer questions (this particular model is pretty bad at no-answer questions). \n",
    "\n",
    "However, what's going on with that fourth block? This portion of the output is not useful unless you supply the evaluation method with additional information. And for that we'll need to dig a bit deeper into the evaluation process because it turns out that we need to compute more than just a prediction for an answer - we must also compute a prediction for NO answer and we must score both predictions!\n",
    "\n",
    "The following section will walk through the details of computing robust predictions on SQuAD2.0 examples, including how to score an answer and the null answer, and how to determine which one should be the \"correct\" prediction for a given example. Feel free to jump ahead to the next section if you want to get to the punchline, however, for those of you considering building a QA system of your own, this information will be invaluable for proper prediction and assessment. \n",
    "\n",
    "\n",
    "### Computing predictions [highly technical]\n",
    "\n",
    "When the tokenized question+context is passed to the model, the output consists of two sets of logits: one for the start of the answer span, the other for the end of the answer span. These logits represent the likelihood of any given token being the start or end of the answer. Every token passed to the model is assigned a logit, including special tokens (e.g, [CLS], [SEP]), and tokens corresponding to the question itself.  \n",
    "\n",
    "Let's walk through the process using our last example (Q: Why is the theory of evolution so complex?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus(example.question_text, example.context_text, return_tensors='pt')\n",
    "start_logits, end_logits = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  6.4914,  -9.1416,  -8.4068,  -7.5684,  -9.9081,  -9.4256, -10.1625,\n",
       "          -9.2579, -10.0554,  -9.9653,  -9.2002,  -8.8657,  -9.1162,   0.6481,\n",
       "          -2.5947,  -4.5072,  -8.1189,  -6.5871,  -5.8973, -10.8619, -11.0953,\n",
       "         -10.2294,  -9.3660,  -7.6017, -10.8009, -10.8197,  -6.1258,  -8.3507,\n",
       "          -4.2463, -10.0987, -10.2659,  -8.8490,  -6.7346,  -8.6513,  -9.7573,\n",
       "          -5.7496,  -5.5851,  -8.9483,  -7.0652,  -6.1369,  -5.7810,  -9.4366,\n",
       "          -8.7670,  -9.6743,  -9.7446,  -7.7905,  -7.4541,  -1.5963,  -3.8540,\n",
       "          -7.3450,  -8.1854,  -9.5566,  -8.3416,  -8.9553,  -8.3144,  -6.4132,\n",
       "          -4.2285,  -9.4427,  -9.5111,  -9.2931,  -8.9154,  -9.3930,  -8.2111,\n",
       "          -8.9774,  -9.0274,  -7.2652,  -7.4511,  -9.8597,  -9.5869,  -9.9735,\n",
       "          -7.0526,  -9.7560,  -8.7788,  -9.5117,  -9.6391,  -8.6487,  -9.5994,\n",
       "          -7.8213,  -5.1754,  -4.3561,  -4.3913,  -7.8499,  -7.7522,  -8.9651,\n",
       "          -3.5229,  -0.8312,  -2.7668,  -7.9180, -10.0320,  -8.7797,  -4.5965,\n",
       "          -5.9465,  -9.9442,  -3.2135,  -5.0734,  -8.3462,  -7.5366,  -3.7073,\n",
       "          -7.0968,  -4.3325,  -1.3691,  -4.1477,  -5.3794,  -7.6138,   1.3183,\n",
       "          -3.4190,   3.1457,  -3.0152,  -0.4102,  -2.4606,  -3.5971,   6.4519,\n",
       "          -0.5654,   0.9829,  -1.6682,   3.3549,  -4.7847,  -2.8024,  -3.3160,\n",
       "          -0.5868,  -0.9617,  -8.1925,  -4.3299,  -7.3923,  -5.0875,  -5.3880,\n",
       "          -5.3676,  -3.0878,  -4.3427,   4.3975,   1.8860,  -5.4661,  -9.1565,\n",
       "          -3.6369,  -3.5462,  -4.1448,  -2.0250,  -2.4492,  -8.7015,  -7.3292,\n",
       "          -7.7616,  -7.0786,  -4.6668,  -4.4089,  -9.1182]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at how large the logit is in the [CLS] position (index 0)!  \n",
    "# Strong possibility that this question has no answer... but our prediction returned an answer anyway!\n",
    "start_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our simple QA system, we predicted the best answer by selecting the start and end tokens with the largest logits, but that's not very robust. In fact, the original BERT paper suggested considering any sensible start+end combination as a possible answer to the question. These combinations would then be scored, and the one with the highest score would be considered the best answer. A possible (candidate) answer is scored as the sum of it's start and end logits. Let's see it in practice. \n",
    "\n",
    "\n",
    "To mimic this behavior we'll start by taking the _n_ largest `start_logits` and the _n_ largest `end_logits` as candidates. Any sensible combination of these start + end tokens is considered a candidate answer, however, several consistency checks must first be performed. For example, an answer wherein the end token falls _before_ the start token should be excluded because that just doesn't make sense. Candidate answers wherein the start or end tokens are associated with question tokens are also excluded because the answer to the question should obviously not be in the question itself! It is important to note that the [CLS] token and its corresonding logits are **not** removed because this token indicates the null answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 6.491387367248535), (111, 6.451895713806152), (129, 4.397505760192871), (115, 3.354909658432007), (106, 3.1457457542419434)]\n",
      "[(119, 6.33292293548584), (0, 6.084450721740723), (135, 4.417276382446289), (116, 4.3764214515686035), (112, 4.125303268432617)]\n"
     ]
    }
   ],
   "source": [
    "# We can sort our list of start_logits by logit score and keep track of which token they're associated with\n",
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()\n",
    "\n",
    "# convert our start and end logit tensors to lists\n",
    "start_logits = to_list(start_logits)[0]\n",
    "end_logits = to_list(end_logits)[0]\n",
    "\n",
    "# sort our start and end logits from largest to smallest, keeping track of the index\n",
    "start_idx_and_logit = sorted(enumerate(start_logits), key=lambda x: x[1], reverse=True)\n",
    "end_idx_and_logit = sorted(enumerate(end_logits), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# select the top n (in this case, 5)\n",
    "print(start_idx_and_logit[:5])\n",
    "print(end_idx_and_logit[:5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null answer token (index 0) is in the top five of both the start and end logit lists. \n",
    "\n",
    "In order to eventually predict a text answer (or empty string), we need to keep track of the indexes which can be used to pull the corresponding token ids later on. We'll also need to identify which indexes correspond to the question tokens so that we can ensure we don't allow a nonsensical prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_indexes = [idx for idx, logit in start_idx_and_logit[:5]]\n",
    "end_indexes = [idx for idx, logit in end_idx_and_logit[:5]]\n",
    "\n",
    "# convert the token ids from a tensor to a list\n",
    "tokens = to_list(inputs['input_ids'])[0]\n",
    "\n",
    "# question tokens are defined as those between the CLS token (101, at position 0) and first SEP (102) token \n",
    "question_indexes = [i+1 for i, token in enumerate(tokens[1:tokens.index(102)])]\n",
    "question_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll loop through all combinations of the start and end token indexes, excluding nonsensical combinations. We'll save candidate predictions to a list for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# keep track of all preliminary predictions\n",
    "PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "    \"PrelimPrediction\", [\"start_index\", \"end_index\", \"start_logit\", \"end_logit\"]\n",
    ")\n",
    "\n",
    "prelim_preds = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # throw out invalid predictions\n",
    "        if start_index in question_indexes:\n",
    "            continue\n",
    "        if end_index in question_indexes:\n",
    "            continue\n",
    "        if end_index < start_index:\n",
    "            continue\n",
    "        prelim_preds.append(\n",
    "            PrelimPrediction(\n",
    "                start_index = start_index,\n",
    "                end_index = end_index,\n",
    "                start_logit = start_logits[start_index],\n",
    "                end_logit = end_logits[end_index]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a list of sensible candidate predictions, it's time to score them. \n",
    "\n",
    "For a candidate answer, score = `start_logit` + `end_logit`. Below we sort our candidate predictions by their score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PrelimPrediction(start_index=0, end_index=119, start_logit=6.491387367248535, end_logit=6.33292293548584),\n",
       " PrelimPrediction(start_index=111, end_index=119, start_logit=6.451895713806152, end_logit=6.33292293548584),\n",
       " PrelimPrediction(start_index=0, end_index=0, start_logit=6.491387367248535, end_logit=6.084450721740723),\n",
       " PrelimPrediction(start_index=0, end_index=135, start_logit=6.491387367248535, end_logit=4.417276382446289),\n",
       " PrelimPrediction(start_index=111, end_index=135, start_logit=6.451895713806152, end_logit=4.417276382446289),\n",
       " PrelimPrediction(start_index=0, end_index=116, start_logit=6.491387367248535, end_logit=4.3764214515686035),\n",
       " PrelimPrediction(start_index=111, end_index=116, start_logit=6.451895713806152, end_logit=4.3764214515686035),\n",
       " PrelimPrediction(start_index=0, end_index=112, start_logit=6.491387367248535, end_logit=4.125303268432617),\n",
       " PrelimPrediction(start_index=111, end_index=112, start_logit=6.451895713806152, end_logit=4.125303268432617),\n",
       " PrelimPrediction(start_index=115, end_index=119, start_logit=3.354909658432007, end_logit=6.33292293548584),\n",
       " PrelimPrediction(start_index=106, end_index=119, start_logit=3.1457457542419434, end_logit=6.33292293548584),\n",
       " PrelimPrediction(start_index=129, end_index=135, start_logit=4.397505760192871, end_logit=4.417276382446289),\n",
       " PrelimPrediction(start_index=115, end_index=135, start_logit=3.354909658432007, end_logit=4.417276382446289),\n",
       " PrelimPrediction(start_index=115, end_index=116, start_logit=3.354909658432007, end_logit=4.3764214515686035),\n",
       " PrelimPrediction(start_index=106, end_index=135, start_logit=3.1457457542419434, end_logit=4.417276382446289),\n",
       " PrelimPrediction(start_index=106, end_index=116, start_logit=3.1457457542419434, end_logit=4.3764214515686035),\n",
       " PrelimPrediction(start_index=106, end_index=112, start_logit=3.1457457542419434, end_logit=4.125303268432617)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort preliminary predictions by their score\n",
    "prelim_preds = sorted(prelim_preds, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\n",
    "prelim_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to convert our preliminary predictions into actual text (or the empty string if null). We'll also trim this list down to the best 5 predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of all best predictions\n",
    "BestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "    \"BestPrediction\", [\"text\", \"start_logit\", \"end_logit\"]\n",
    ")\n",
    "\n",
    "nbest = []\n",
    "seen_predictions = []\n",
    "for pred in prelim_preds:\n",
    "    \n",
    "    # For now we only care about the top 5 best predictions\n",
    "    if len(nbest) >= 5: \n",
    "        break\n",
    "        \n",
    "    # loop through predictions according to their start index\n",
    "    if pred.start_index > 0: # non-null answers have start_index > 0\n",
    "\n",
    "        text = tokenizer.convert_tokens_to_string(\n",
    "            tokenizer.convert_ids_to_tokens(\n",
    "                tokens[pred.start_index:pred.end_index+1]\n",
    "            )\n",
    "        )\n",
    "        # Clean whitespace\n",
    "        text = text.strip()\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "    if text in seen_predictions:\n",
    "        continue\n",
    "        \n",
    "    # flag this text as being seen -- if we see it again, no need to add it to the list\n",
    "    seen_predictions.append(text) \n",
    "    \n",
    "    # add this text prediction to a pruned list of the top 5 best predictions\n",
    "    nbest.append(BestPrediction(text=text, start_logit=pred.start_logit, end_logit=pred.end_logit))\n",
    "\n",
    "\n",
    "# And don't forget -- include the null answer!\n",
    "nbest.append(BestPrediction(text=\"\", start_logit=start_logits[0], end_logit=end_logits[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null answer is scored as the sum of the  start_logit and end_logit associated with the [CLS] token. \n",
    "\n",
    " \n",
    "At this point, we have a neat list of the top 5 best predictions for this question. The number of best predictions for each example is adjustable with the `--n_best_size` argument of the `run_squad.py` script.  The `nbest` predictions for _every question_ in the dev set are saved to disk under `nbest_predictions_.json` in `--output_dir`. This is a great resource if you need to dig into how your model is behaving. Let's take a look at our `nbest` predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BestPrediction(text='free oxygen', start_logit=6.491387367248535, end_logit=6.33292293548584),\n",
       " BestPrediction(text='free oxygen began to outgas from the oceans', start_logit=6.451895713806152, end_logit=6.33292293548584),\n",
       " BestPrediction(text='free oxygen began to outgas from the oceans 3 – 2 . 7 billion years ago , reaching 10 % of its present level', start_logit=6.451895713806152, end_logit=4.417276382446289),\n",
       " BestPrediction(text='free oxygen began to outgas', start_logit=6.451895713806152, end_logit=4.3764214515686035),\n",
       " BestPrediction(text='outgas from the oceans', start_logit=3.354909658432007, end_logit=6.33292293548584),\n",
       " BestPrediction(text='', start_logit=6.491387367248535, end_logit=6.084450721740723)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: UPDATE THIS PARAGRAPH\n",
    "Sure enough, we see that the best prediction is identical to the one we originally had: \"uncertainties of fossilization\". In this case, the best answer indexes didn't yield a nonsensical prediction so taking the max(`start_logit`) and the max(`end_logit`) wasn't an issue. However, we know it's still incorrect. Let's keep going. \n",
    "\n",
    "\n",
    "The last step is to compute the the null score -- more specifically, the difference between the null score and the best non-null score as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2484722137451172"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the null score as the sum of the [CLS] token logits\n",
    "score_null = start_logits[0] + end_logits[0]\n",
    "\n",
    "# compute the difference between the null score and the best non-null score\n",
    "score_diff = score_null - nbest[0].start_logit - nbest[0].end_logit\n",
    "\n",
    "score_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `score_diff` is computed for every example in the dev set and these scores are saved to disk in the `null_odds_.json`. Let's pull up the score stored for the example we're using and see how we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2090005874633789"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = model_dir + 'null_odds_.json'\n",
    "null_odds = json.load(open(filename, 'rb'))\n",
    "\n",
    "null_odds[example.qas_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're a few hundredths of a logit off but not too bad. (In the full HF version there even more checks and layers of sophistication which I have stripped here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `--null_score_diff_threshold`\n",
    "\n",
    "In the previous section we computed robust prediction scores for both the best possible text answer and the null answer for questions in the SQuAD2.0 dev set. We also computed the difference between these scores and learned that `run_squad.py` saves these score differences for each example in the `null_odds_.json` file. With can now start to make sense of the fourth block of the results output!\n",
    "\n",
    "According to the original BERT paper, \n",
    "\n",
    "> We predict a non-null answer when sˆi,j > s_null + τ , where the threshold τ is selected on the dev set to maximize F1.  \n",
    "\n",
    "In other words, the authors are saying that one should predict a null answer for a given example if that example's score difference is above a certain threshold. What should that threshold be? How should we compute it? They give us a recipe: fine tune the score to maximize F1. \n",
    "\n",
    "Rather then rerunning `run_squad.py`, we can import the specific method that computes SQuAD evalutation, the aptly named `squad_evaluate`. You can take a look at the code for yourself [here](https://github.com/huggingface/transformers/blob/5856999a9f2926923f037ecd8d27b8058bcf9dae/src/transformers/data/metrics/squad_metrics.py#L211-L239). To use it we'll need \n",
    "\n",
    "* the original examples (because that's where the True Answers are stored),\n",
    "* the predictions (which we already computed when we evaluated using `run_squad.py`), \n",
    "* the `null_odds_.json` (also already computed),\n",
    "* and a threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.metrics.squad_metrics import squad_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the predictions we generated earlier\n",
    "filename = model_dir + 'predictions_.json'\n",
    "preds = json.load(open(filename, 'rb'))\n",
    "\n",
    "# Load the null score differences we generated earlier\n",
    "filename = model_dir = 'null_odds_.json'\n",
    "null_odds = json.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-evaluate our model on SQuAD2.0 using the `squad_evaluate` method. This method uses the score differences for each example in the dev set to determine thresholds that maximize either the EM score or the F1 score. It then recomputes the best possible EM score and F1 score associated with that null threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default threshold is set to 1.0 -- we'll leave it there for now\n",
    "results_default_thresh = squad_evaluate(examples, \n",
    "                                        preds, \n",
    "                                        no_answer_probs=null_odds, \n",
    "                                        no_answer_probability_threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('exact', 66.25958056093658),\n",
       "             ('f1', 69.66994428499025),\n",
       "             ('total', 11873),\n",
       "             ('HasAns_exact', 68.91025641025641),\n",
       "             ('HasAns_f1', 75.74076391627662),\n",
       "             ('HasAns_total', 5928),\n",
       "             ('NoAns_exact', 63.61648444070648),\n",
       "             ('NoAns_f1', 63.61648444070648),\n",
       "             ('NoAns_total', 5945),\n",
       "             ('best_exact', 68.36519834919565),\n",
       "             ('best_exact_thresh', -4.189256191253662),\n",
       "             ('best_f1', 71.1144383018176),\n",
       "             ('best_f1_thresh', -3.767639636993408)])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_default_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three blocks have identical values as our first evaluation because they are based on the default threshold (which is currently 1.0). However, the values in the fourth block have been updated by taking into account the `null_odds` information.  When a given example's `score_diff` is greater than the threshold, the prediction is flipped to a null answer which affects the overall EM and F1 scores. \n",
    "\n",
    "Let's use the `best_f1_thresh` and run the evaluation once more one more so that we can see a breakdown of how our model does on `HasAns` and `NoAns` examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1_thresh = -3.767639636993408\n",
    "results_f1_thresh = squad_evaluate(examples, \n",
    "                                   preds, \n",
    "                                   no_answer_probs=null_odds, \n",
    "                                   no_answer_probability_threshold=best_f1_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('exact', 68.31466352227744),\n",
       "             ('f1', 71.11443830181771),\n",
       "             ('total', 11873),\n",
       "             ('HasAns_exact', 61.53846153846154),\n",
       "             ('HasAns_f1', 67.14604014127524),\n",
       "             ('HasAns_total', 5928),\n",
       "             ('NoAns_exact', 75.07148864592094),\n",
       "             ('NoAns_f1', 75.07148864592094),\n",
       "             ('NoAns_total', 5945),\n",
       "             ('best_exact', 68.36519834919565),\n",
       "             ('best_exact_thresh', -4.189256191253662),\n",
       "             ('best_f1', 71.1144383018176),\n",
       "             ('best_f1_thresh', -3.767639636993408)])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_f1_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we used the default threshold of 1.0, we saw that our `NoAns_f1` score was a mere 63.6 but when we use the `best_f1_thresh`, we now get a `NoAns_f1` score of 75! Nearly a 12 point jump! The downside is that we lose some ground in how well our model correctly predicts `HasAns` examples. Overall, however, we see a net increase of a couple points in both EM and F1 scores. This demonstrates that the computing null scores and properly using a null threshold  significantly increases QA performance on the SQuAD2.0 dev set with almost no additional work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "Below we have a new method that will select more robust predictions, compute scores for the best text predictions as well as for the null prediction, and will use these scores along with a threshold to determine whether the question should be answered. As a bonus, this method also computes and returns the probability of the answer which is often easier to interpret than a logit score. The probability of a prediction will depend on `nbest` since they are computed with a softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def get_robust_prediction(example, nbest=10, null_threshold=1.0):\n",
    "    # given a question id (qas_id or qid), load the example, get the model outputs and generate an answer\n",
    "    question = example.question_text\n",
    "    context = example.context_text\n",
    "\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "\n",
    "    start_logits, end_logits = model(**inputs)\n",
    "\n",
    "    # convert our start and end logit tensors to lists\n",
    "    start_logits = to_list(start_logits)[0]\n",
    "    end_logits = to_list(end_logits)[0]\n",
    "\n",
    "    # sort our start and end logits from largest to smallest, keeping track of the index\n",
    "    start_idx_and_logit = sorted(enumerate(start_logits), key=lambda x: x[1], reverse=True)\n",
    "    end_idx_and_logit = sorted(enumerate(end_logits), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    start_indexes = [idx for idx, logit in start_idx_and_logit[:5]]\n",
    "    end_indexes = [idx for idx, logit in end_idx_and_logit[:5]]\n",
    "\n",
    "    # convert the token ids from a tensor to a list\n",
    "    tokens = to_list(inputs['input_ids'])[0]\n",
    "\n",
    "    # question tokens are defined as those between the CLS token (101, at position 0) and first SEP (102) token \n",
    "    question_indexes = [i+1 for i, token in enumerate(tokens[1:tokens.index(102)])]\n",
    "    question_indexes\n",
    "    \n",
    "    # keep track of all preliminary predictions\n",
    "    PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"PrelimPrediction\", [\"start_index\", \"end_index\", \"start_logit\", \"end_logit\"]\n",
    "    )\n",
    "\n",
    "    prelim_preds = []\n",
    "    for start_index in start_indexes:\n",
    "        for end_index in end_indexes:\n",
    "            # throw out invalid predictions\n",
    "            if start_index in question_indexes:\n",
    "                continue\n",
    "            if end_index in question_indexes:\n",
    "                continue\n",
    "            if end_index < start_index:\n",
    "                continue\n",
    "            prelim_preds.append(\n",
    "                PrelimPrediction(\n",
    "                    start_index = start_index,\n",
    "                    end_index = end_index,\n",
    "                    start_logit = start_logits[start_index],\n",
    "                    end_logit = end_logits[end_index]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # keep track of all best predictions\n",
    "    # This will be the pool from which answer probabilities are computed \n",
    "    BestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"BestPrediction\", [\"text\", \"start_logit\", \"end_logit\"]\n",
    "    )\n",
    "\n",
    "    nbest_predictions = []\n",
    "    seen_predictions = []\n",
    "    for pred in prelim_preds:\n",
    "        if len(nbest_predictions) >= nbest: \n",
    "            break\n",
    "        if pred.start_index > 0: # non-null answers have start_index > 0\n",
    "            text = tokenizer.convert_tokens_to_string(\n",
    "                tokenizer.convert_ids_to_tokens(\n",
    "                    tokens[pred.start_index:pred.end_index+1]\n",
    "                )\n",
    "            )\n",
    "            # Clean whitespace\n",
    "            text = text.strip()\n",
    "            text = \" \".join(text.split())\n",
    "\n",
    "            if text in seen_predictions:\n",
    "                continue\n",
    "\n",
    "            # flag this text as being seen -- if we see it again, no need to add it to the list\n",
    "            seen_predictions.append(text) \n",
    "\n",
    "            # add this text prediction to a pruned list of the top nbest predictions\n",
    "            nbest_predictions.append(BestPrediction(text=text, \n",
    "                                                    start_logit=pred.start_logit, \n",
    "                                                    end_logit=pred.end_logit))\n",
    "        \n",
    "    # Add the null prediction\n",
    "    nbest_predictions.append(BestPrediction(text=\"\", start_logit=start_logits[0], end_logit=end_logits[0]))\n",
    "\n",
    "    # Compute the probability of each prediction - this is not needed but nice to have\n",
    "    all_scores = [pred.start_logit+pred.end_logit for pred in nbest_predictions] \n",
    "    probabilities = softmax(np.array(all_scores))\n",
    "        \n",
    "    # compute the null score as the sum of the [CLS] token logits\n",
    "    score_null = nbest_predictions[-1].start_logit + nbest_predictions[-1].end_logit\n",
    "    score_non_null = nbest_predictions[0].start_logit + nbest_predictions[0].end_logit\n",
    "\n",
    "    # compute the difference between the null score and the best non-null score\n",
    "    score_diff = score_null - score_non_null\n",
    "\n",
    "    # If that difference is greater than the null threshold, return the null answer\n",
    "    if score_diff > null_threshold:\n",
    "        return \"\", probabilities[-1]\n",
    "    else:\n",
    "        return nbest_predictions[0].text, probabilities[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we now get the right answer (an empty string) for that tricky no-answer example we were working with? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', 0.34837593510791465)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_robust_prediction(example, nbest=10, null_threshold=best_f1_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo!! We got the right answer this time!! \n",
    "\n",
    "Even if we didn't have the best threshold in place, our additional checks still allow us to output more sensible looking answers, rejecting predictions that include the question tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('free oxygen began to outgas from the oceans', 0.4293458323979202)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_robust_prediction(example, nbest=10, null_threshold=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if it hadn't been a trick question, this would be the correct answer! Seems like distilBERT could use some improvement in number understanding. \n",
    "\n",
    "Using a robust prediction method like the above will do more than allow your model to perform better on a curated dev set, though this is an important first step. It will also provide your model with a slightly better ability to refrain from answering questions that simply don't have an answer in the associated passage. This is a crucial feature for QA models because it's not enough to get an answer if that answer doesn't make sense for the question we ask. We want our models to tell us something useful -- and sometimes that means telling us nothing at all. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.1 64-bit ('venv': virtualenv)",
   "language": "python",
   "name": "python36164bitvenvvirtualenv0c1267fd5e5f4f5ba18905a319acff6d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
