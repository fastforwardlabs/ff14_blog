{
  
    
        "post0": {
            "title": "How to Build a QA System with BERT on Wikipedia",
            "content": "So you&#39;ve decided to build a QA system. . You want to start with something general and straightforward so you plan to make it open domain using Wikipedia as a corpus for answering questions. You&#39;re going to use an IR-based design (see previous post) since you&#39;re working with a large collection of unstructured text. You want to use the best NLP that your compute resources allow (you&#39;re lucky enough to have access to a GPU) so you&#39;re going to focus on the big, flashy Transformer models that are all the rage these days. . Sounds like a plan! So where do you start? . This was our thought process when we first set out on this research path and in this post we&#39;ll discuss what you need to know to get going! . installing libraries and setting up an environment | understanding Huggingface&#39;s run_squad.py training script | Understanding the basic ins and outs of a BERT-esque model | getting BERT to accept a full Wikipedia article as context for a question | . Setting up your virtual environment . A virtual environment is always best practice and we&#39;re using venv (though Melanie is also partial to conda). Here&#39;s the bare minimum that you&#39;ll need to do what I did. For this project we&#39;ll be using Pytorch (though everything we do can also be accomplished in Tensorflow). Pytorch handles the heavy lifting of deep differentiable learning. Transformers is a library by Huggingface that provides super easy to use implementations (in torch) of all the popular Transformer architectures (more on this later). . PyTorch | Transformers | Wikipedia | . TODO - torch for gpu vs no gpu . We used venv for our virtual environment. It comes standard with Python installations. Other great options include virtualenv and Anaconda. You can recreate our env with the following commands in your command line (linux/MacOS users) . $ python3 -m venv myenv $ source myenv/bin/activate $ pip install torch $ pip install transformers $ pip install wikipedia . import torch import wikipedia as wiki . Parenthetical note: our GPU machine sports an older version of CUDA (9.2 -- we&#39;re getting around to updating that). In the meantime, this requires us to use an older version of PyTorch for the necessary CUDA support. The HF script we&#39;re using for training (run_squad.py) requires some specific packages. More recent versions of PyTorch include these packages; however, older versions do not and thus may require that you also install TensorboardX (see the hidden code cell below). . # collapse-hide # line 69 of `run_squad.py` script shows why you might need to install # tensorboardX if you have an older version of torch try: from torch.utils.tensorboard import SummaryWriter except ImportError: from tensorboardX import SummaryWriter . . HuggingFace Transformers . I&#39;m new to PyTorch and even newer to HuggingFace (HF) but I&#39;m quickly becoming a convert! The HuggingFace Transformers package provides state-of-the-art general-purpose architecures for natural language understanding and natural language generation. They host dozens of pre-trained models (like BERT) operating in over 100 languages that you can use right out of the box. All of these models come with deep interoperability between PyTorch and Tensorflow 2.0, which means you can move a model from TF2.0 to PyTorch and back again with a line or two of code! . If you&#39;re new to Hugging Face, we strongly recommend working through the HF Quickstart guide as well as their excellent Transformer Notebooks (we did!), as we won&#39;t cover that material in this notebook. We&#39;ll be using HF AutoClasses, which serve as a wrapper around pretty much any of the base Transformer classes. So if we want to work with several models we don&#39;t have to import transformers.BertModel, and transformers.XLNetModel, and transformers.RobertaModel, etc. We can just import transformers.AutoModel and feed it the appropriate model name or path for each architecture we care about. . from transformers import AutoModel, AutoTokenizer, AutoModelForQuestionAnswering . Training a Transformer model for Question Answering . Not every Transformer architecture lends itself naturally to the task of Question Answering (GPT, for instance, does not do QA; similarly BERT does not do machine translation!). . You can identify likely model families by ... Once you&#39;ve found a model you&#39;d like to work with (in a future post we&#39;ll go over the model families in more depth), the next step is to train it on some data! . One of the canonical datasets for QA is the Stanford Question Answering Dataset, or SQuAD, which comes in two flavors: SQuAD 1.1 and SQuAD 2.0. HF helpfully provide a script that trains a Transformer model on one of the datasets, called run_squad.py. You can grab the script here. This script is pretty complicated and in a later post we&#39;ll go through some of the details for those who really want to get into the nuts and bolts of fine-tuning. For now, however, we&#39;ll walk through the command to train BERT on SQuAD 1.1 or 2.0 datasets (or both, in succession!) . # Set paths %env DATA_DIR=./data/squad %env MODEL_DIR=./models # we&#39;ll store trained models here . env: DATA_DIR=./data/squad env: MODEL_DIR=./models # we&#39;ll store trained models here . # Download the data def download_squad(version=1): if version == 1: !wget -P $SQUAD_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json !wget -P $SQUAD_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json else: !wget -P $SQUAD_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json !wget -P $SQUAD_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json download_squad(version=2) . --2020-05-08 15:11:59-- https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.109.153, 185.199.110.153, 185.199.111.153, ... Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.109.153|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 42123633 (40M) [application/json] Saving to: ‘./data/squad/train-v2.0.json.2’ train-v2.0.json.2 100%[===================&gt;] 40.17M 16.6MB/s in 2.4s 2020-05-08 15:12:01 (16.6 MB/s) - ‘./data/squad/train-v2.0.json.2’ saved [42123633/42123633] --2020-05-08 15:12:02-- https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ... Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 4370528 (4.2M) [application/json] Saving to: ‘./data/squad/dev-v2.0.json’ dev-v2.0.json 100%[===================&gt;] 4.17M 4.84MB/s in 0.9s 2020-05-08 15:12:03 (4.84 MB/s) - ‘./data/squad/dev-v2.0.json’ saved [4370528/4370528] . # Training your own model to do QA using HF&#39;s `run_squad.py` # Turn flags on and off according to the model you&#39;re training cmd = [ &#39;python&#39;, # &#39;-m torch.distributed.launch --nproc_per_node 2&#39;, # use this to perform distributed training over multiple GPUs &#39;run_squad.py&#39;, &#39;--model_type&#39;, &#39;bert&#39;, # model type &#39;--model_name_or_path&#39;, &#39;bert-base-uncased&#39;, # specific model name of the given model type &#39;--output_dir&#39;, &#39;$DATA_DIR/bert/bbu_squad2&#39;, # directory for model checkpoints and predictions # &#39;--overwrite_output_dir&#39;, # use when adding output to a directory that is non-empty &#39;--do_train&#39;, &#39;--train_file&#39;, &#39;$SQUAD_DIR/train-v2.0.json&#39;, &#39;--version_2_with_negative&#39;, # MUST use this flag if training on SQuAD 2.0 dataset! &#39;--do_lower_case&#39;, # Set this flag if using an uncased model &#39;--do_eval&#39;, # evaluate the model on the dev set after fine-tuning complete &#39;--predict_file&#39;, &#39;$SQUAD_DIR/dev-v2.0.json&#39;, &#39;--eval_all_checkpoints&#39;, # evaluate the model on the dev set at each checkpoint &#39;--num_train_epochs&#39;, &#39;3&#39;, # model hyperparameters &#39;--learning_rate&#39;, &#39;3e-5&#39;, &#39;--max_seq_length&#39;, &#39;384&#39;, &#39;--doc_stride&#39;, &#39;128&#39;, &#39;--per_gpu_eval_batch_size&#39;, &#39;12&#39;, &#39;--per_gpu_train_batch_size&#39;, &#39;12&#39;, &#39;--save_steps&#39;, &#39;10000&#39;, # How often checkpoints (complete model snapshot) are saved &#39;--threads&#39;, &#39;8&#39; # num of CPU threads to use for converting examples to features ] . # Don&#39;t run this cell unless you&#39;re rocking at least one GPU from subprocess import PIPE, STDOUT, Popen # Live output from run_squad.py is through stderr (rather than stdout). The following command runs the process # and ports stderr to stdout p = Popen(cmd, stdout=PIPE, stderr=STDOUT) # Default behavior when using bash cells is that you won&#39;t see the live output in the cell -- you can only see # output once the entire process has finished and then you get it all at once. This is terrible when training # models that can take hours or days of compute time! # This command combined with the above allows you to see the live output feed, though it is a bit asynchronous. for line in iter(p.stdout.readline, b&#39;&#39;): print(&quot;&gt;&gt;&gt; &quot; + line.decode().rstrip()) . &gt;&gt;&gt; 2020-05-08 16:02:51.791650: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library &#39;libnvinfer.so.6&#39;; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64: &gt;&gt;&gt; 2020-05-08 16:02:51.791724: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library &#39;libnvinfer_plugin.so.6&#39;; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64: &gt;&gt;&gt; 2020-05-08 16:02:51.791733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. &gt;&gt;&gt; 05/08/2020 16:02:52 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False &gt;&gt;&gt; 05/08/2020 16:02:53 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at /home/melanie/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c &gt;&gt;&gt; 05/08/2020 16:02:53 - INFO - transformers.configuration_utils - Model config DistilBertConfig { &gt;&gt;&gt; &#34;_num_labels&#34;: 2, &gt;&gt;&gt; &#34;activation&#34;: &#34;gelu&#34;, &gt;&gt;&gt; &#34;architectures&#34;: [ &gt;&gt;&gt; &#34;DistilBertForMaskedLM&#34; &gt;&gt;&gt; ], &gt;&gt;&gt; &#34;attention_dropout&#34;: 0.1, &gt;&gt;&gt; &#34;bad_words_ids&#34;: null, &gt;&gt;&gt; &#34;bos_token_id&#34;: null, &gt;&gt;&gt; &#34;decoder_start_token_id&#34;: null, &gt;&gt;&gt; &#34;dim&#34;: 768, &gt;&gt;&gt; &#34;do_sample&#34;: false, &gt;&gt;&gt; &#34;dropout&#34;: 0.1, &gt;&gt;&gt; &#34;early_stopping&#34;: false, &gt;&gt;&gt; &#34;eos_token_id&#34;: null, &gt;&gt;&gt; &#34;finetuning_task&#34;: null, &gt;&gt;&gt; &#34;hidden_dim&#34;: 3072, &gt;&gt;&gt; &#34;id2label&#34;: { &gt;&gt;&gt; &#34;0&#34;: &#34;LABEL_0&#34;, &gt;&gt;&gt; &#34;1&#34;: &#34;LABEL_1&#34; &gt;&gt;&gt; }, &gt;&gt;&gt; &#34;initializer_range&#34;: 0.02, &gt;&gt;&gt; &#34;is_decoder&#34;: false, &gt;&gt;&gt; &#34;is_encoder_decoder&#34;: false, &gt;&gt;&gt; &#34;label2id&#34;: { &gt;&gt;&gt; &#34;LABEL_0&#34;: 0, &gt;&gt;&gt; &#34;LABEL_1&#34;: 1 &gt;&gt;&gt; }, &gt;&gt;&gt; &#34;length_penalty&#34;: 1.0, &gt;&gt;&gt; &#34;max_length&#34;: 20, &gt;&gt;&gt; &#34;max_position_embeddings&#34;: 512, &gt;&gt;&gt; &#34;min_length&#34;: 0, &gt;&gt;&gt; &#34;model_type&#34;: &#34;distilbert&#34;, &gt;&gt;&gt; &#34;n_heads&#34;: 12, &gt;&gt;&gt; &#34;n_layers&#34;: 6, &gt;&gt;&gt; &#34;no_repeat_ngram_size&#34;: 0, &gt;&gt;&gt; &#34;num_beams&#34;: 1, &gt;&gt;&gt; &#34;num_return_sequences&#34;: 1, &gt;&gt;&gt; &#34;output_attentions&#34;: false, &gt;&gt;&gt; &#34;output_hidden_states&#34;: false, &gt;&gt;&gt; &#34;output_past&#34;: true, &gt;&gt;&gt; &#34;pad_token_id&#34;: 0, &gt;&gt;&gt; &#34;prefix&#34;: null, &gt;&gt;&gt; &#34;pruned_heads&#34;: {}, &gt;&gt;&gt; &#34;qa_dropout&#34;: 0.1, &gt;&gt;&gt; &#34;repetition_penalty&#34;: 1.0, &gt;&gt;&gt; &#34;seq_classif_dropout&#34;: 0.2, &gt;&gt;&gt; &#34;sinusoidal_pos_embds&#34;: false, &gt;&gt;&gt; &#34;task_specific_params&#34;: null, &gt;&gt;&gt; &#34;temperature&#34;: 1.0, &gt;&gt;&gt; &#34;tie_weights_&#34;: true, &gt;&gt;&gt; &#34;top_k&#34;: 50, &gt;&gt;&gt; &#34;top_p&#34;: 1.0, &gt;&gt;&gt; &#34;torchscript&#34;: false, &gt;&gt;&gt; &#34;use_bfloat16&#34;: false, &gt;&gt;&gt; &#34;vocab_size&#34;: 30522, &gt;&gt;&gt; &#34;xla_device&#34;: null &gt;&gt;&gt; } &gt;&gt;&gt; &gt;&gt;&gt; 05/08/2020 16:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/melanie/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084 &gt;&gt;&gt; 05/08/2020 16:02:53 - INFO - transformers.modeling_utils - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-pytorch_model.bin from cache at /home/melanie/.cache/torch/transformers/7b8a8f0b21c4e7f6962451c9370a5d9af90372a5f64637a251f2de154d0fc72c.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5 . KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-48-0a58a906027f&gt; in &lt;module&gt; 12 13 # This command combined with the above allows you to see the live output feed, though it is a bit asynchronous. &gt; 14 for line in iter(p.stdout.readline, b&#39;&#39;): 15 print(&#34;&gt;&gt;&gt; &#34; + line.decode().rstrip()) KeyboardInterrupt: . Training Output . Successful completion of the run_squad.py script by Hugging Face outputs a slew of tanglibles, including the model weights, tokenizer, config, xxx, xxx, etc. These will all be found in the --output_dir directory. When loading your model for future use, this is the directory you&#39;ll point at and the HF API will take care of the rest. . Files for the model&#39;s tokenizer, which converts text into tokens in a way that can be read by the model . tokenizer_config.json | vocab.txt | special_tokens_map.json | . Files for the model itself . pytorch_model.bin: these are the actual model weights (this file can be quite large) | config.json: details of the model architecture | . Binary representation of the command line arguments used to train this model . training_args.bin | . If you include --do_eval, you&#39;ll also see these files . predictions_.json: the official best answer for each example | nbest_predictions_.json: the top n best answers for each example | . Using a pre-trained model from the Hugging Face repository . If you don&#39;t have access to GPUs or don&#39;t have the time to fiddle and train models, you&#39;re in luck! Hugging Face is not just a slick API for Transformers -- it also hosts a repository for pre-trained and fine-tuned models contributed from the wide community of NLP practitioners. Searching for &quot;squad&quot; brings up a list of 55 models. . . Clicking one of these links gives explicit code for using the model, and, in some cases, information on how it was trained and what results were achieved. . # command for importing/downloading one of these pre-fine-tuned models from HF from transformers import AutoTokenizer, AutoModelForQuestionAnswering # Executing these commands for the first time initiates a download of the # model weights to ~/.cache/torch/transformers/ tokenizer = AutoTokenizer.from_pretrained(&quot;deepset/bert-base-cased-squad2&quot;) #&quot;ahotrod/xlnet_large_squad2_512&quot; model = AutoModelForQuestionAnswering.from_pretrained(&quot;deepset/bert-base-cased-squad2&quot;) #&quot;deepset/bert-base-cased-squad2&quot; . Toggle the field below to see what the model looks like in terms of its architecture. . # collapse-hide model . . BertForQuestionAnswering( (bert): BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(28996, 768, padding_idx=0) (position_embeddings): Embedding(512, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) (qa_outputs): Linear(in_features=768, out_features=2, bias=True) ) . Let&#39;s try our model! . question = &quot;Who ruled Macedonia&quot; text = &quot;&quot;&quot;Macedonia was an ancient kingdom on the periphery of Archaic and Classical Greece, and later the dominant state of Hellenistic Greece. The kingdom was founded and initially ruled by the Argead dynasty, followed by the Antipatrid and Antigonid dynasties. Home to the ancient Macedonians, it originated on the northeastern part of the Greek peninsula. Before the 4th century BC, it was a small kingdom outside of the area dominated by the city-states of Athens, Sparta and Thebes, and briefly subordinate to Achaemenid Persia.&quot;&quot;&quot; inputs = tokenizer.encode_plus(question, text, return_tensors=&quot;pt&quot;) answer_start_scores, answer_end_scores = model(**inputs) answer_start = torch.argmax(answer_start_scores) # Get the most likely beginning of answer with the argmax of the score answer_end = torch.argmax(answer_end_scores) + 1 # Get the most likely end of answer with the argmax of the score tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;][0][answer_start:answer_end])) . &#39;the Argead dynasty&#39; . QA on Wikipedia pages . We saw our model work on some short questions with a few small snippets of context. But what if we want to search for answers in much longer documents? A typical Wikipedia page is much longer than any of the snippet examples presented above and it takes a bit of massaging befor we can use our model on these longer contexts. . import wikipedia as wiki import pprint as pp question = &#39;What is the wingspan of an albatross?&#39; results = wiki.search(question) print(&quot;Wikipedia search results for our question: n&quot;) pp.pprint(results) page = wiki.page(results[0]) text = page.content print(f&quot; nThe {results[0]} Wikipedia article contains {len(text)} characters.&quot;) . Wikipedia search results for our question: [&#39;Albatross&#39;, &#39;List of largest birds&#39;, &#39;Black-browed albatross&#39;, &#39;Argentavis&#39;, &#39;Pterosaur&#39;, &#39;Mollymawk&#39;, &#39;Largest body part&#39;, &#39;List of birds by flight speed&#39;, &#39;Pelican&#39;, &#39;Aspect ratio (aeronautics)&#39;] The Albatross Wikipedia article contains 38200 characters. . inputs = tokenizer.encode_plus(question, text, return_tensors=&#39;pt&#39;) print(f&quot;This translates into {len(inputs[&#39;input_ids&#39;][0])} tokens.&quot;) . Token indices sequence length is longer than the specified maximum sequence length for this model (10 &gt; 512). Running this sequence through the model will result in indexing errors . This translates into 8824 tokens. . # collapse-hide answer_start_scores, answer_end_scores = model(**inputs) . . RuntimeError Traceback (most recent call last) &lt;ipython-input-88-57152e2c4feb&gt; in &lt;module&gt; 1 # collapse-hide -&gt; 2 answer_start_scores, answer_end_scores = model(**inputs) /home/ryan/work/ff14/venv/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 545 result = self._slow_forward(*input, **kwargs) 546 else: --&gt; 547 result = self.forward(*input, **kwargs) 548 for hook in self._forward_hooks.values(): 549 hook_result = hook(self, input, result) /home/ryan/work/ff14/transformers/src/transformers/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions) 1478 position_ids=position_ids, 1479 head_mask=head_mask, -&gt; 1480 inputs_embeds=inputs_embeds, 1481 ) 1482 /home/ryan/work/ff14/venv/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 545 result = self._slow_forward(*input, **kwargs) 546 else: --&gt; 547 result = self.forward(*input, **kwargs) 548 for hook in self._forward_hooks.values(): 549 hook_result = hook(self, input, result) /home/ryan/work/ff14/transformers/src/transformers/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask) 781 782 embedding_output = self.embeddings( --&gt; 783 input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds 784 ) 785 encoder_outputs = self.encoder( /home/ryan/work/ff14/venv/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 545 result = self._slow_forward(*input, **kwargs) 546 else: --&gt; 547 result = self.forward(*input, **kwargs) 548 for hook in self._forward_hooks.values(): 549 hook_result = hook(self, input, result) /home/ryan/work/ff14/transformers/src/transformers/modeling_bert.py in forward(self, input_ids, token_type_ids, position_ids, inputs_embeds) 172 if inputs_embeds is None: 173 inputs_embeds = self.word_embeddings(input_ids) --&gt; 174 position_embeddings = self.position_embeddings(position_ids) 175 token_type_embeddings = self.token_type_embeddings(token_type_ids) 176 /home/ryan/work/ff14/venv/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 545 result = self._slow_forward(*input, **kwargs) 546 else: --&gt; 547 result = self.forward(*input, **kwargs) 548 for hook in self._forward_hooks.values(): 549 hook_result = hook(self, input, result) /home/ryan/work/ff14/venv/lib/python3.6/site-packages/torch/nn/modules/sparse.py in forward(self, input) 112 return F.embedding( 113 input, self.weight, self.padding_idx, self.max_norm, --&gt; 114 self.norm_type, self.scale_grad_by_freq, self.sparse) 115 116 def extra_repr(self): /home/ryan/work/ff14/venv/lib/python3.6/site-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse) 1465 # remove once script supports set_grad_enabled 1466 _no_grad_embedding_renorm_(weight, input, max_norm, norm_type) -&gt; 1467 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) 1468 1469 RuntimeError: index out of range: Tried to access index 512 out of table with 511 rows. at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:237 . The tokenizer takes the input and returns tokens. In general, tokenizers convert words or pieces of words into a model-ingestable format. The specific tokens and format are dependent on the type of model. For example, BERT tokenizes words differently from RoBERTa. This is why you must always use the associated tokenizer appropriate for your model. . In this case, the tokenizer converts our text into 8824 tokens, but this far exceeds the maximum number of tokens that can be fed to the model at one time. Most BERT-esque models can only accept 512 tokens at once (check out the error cell above to see what happens when you try to exceed that). This means we&#39;ll have to split our input into chunks and each chunk must not exceed 512 tokens in total. . When working with Question Answering, it&#39;s crucial that each chunk follows this format: . [CLS] question tokens [SEP] context tokens [SEP] . This means that, for each segment of the Wikipedia article, we must prepend the original question, followed by the next &quot;chunk&quot; of article tokens. . # Time to chunk! from collections import OrderedDict # identify question tokens (token_type_ids = 0) qmask = inputs[&#39;token_type_ids&#39;].lt(1) qt = torch.masked_select(inputs[&#39;input_ids&#39;], qmask) print(f&quot;The question consists of {qt.size()[0]} tokens.&quot;) chunk_size = model.config.max_position_embeddings - qt.size()[0] - 1 # the &quot;-1&quot; accounts for # having to add an ending [SEP] token to the end print(f&quot;Each chunk will contain {chunk_size - 2} tokens of the Wikipedia article.&quot;) # create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input chunked_input = OrderedDict() for k,v in inputs.items(): q = torch.masked_select(v, qmask) c = torch.masked_select(v, ~qmask) chunks = torch.split(c, chunk_size) for i, chunk in enumerate(chunks): if i not in chunked_input: chunked_input[i] = {} thing = torch.cat((q, chunk)) if i != len(chunks)-1: if k == &#39;input_ids&#39;: thing = torch.cat((thing, torch.tensor([102]))) else: thing = torch.cat((thing, torch.tensor([1]))) chunked_input[i][k] = torch.unsqueeze(thing, dim=0) . The question consists of 12 tokens. Each chunk will contain 497 tokens of the Wikipedia article. . for i in range(len(chunked_input.keys())): print(f&quot;Number of tokens in chunk {i}: {len(chunked_input[i][&#39;input_ids&#39;].tolist()[0])}&quot;) . Number of tokens in chunk 0: 512 Number of tokens in chunk 1: 512 Number of tokens in chunk 2: 512 Number of tokens in chunk 3: 512 Number of tokens in chunk 4: 512 Number of tokens in chunk 5: 512 Number of tokens in chunk 6: 512 Number of tokens in chunk 7: 512 Number of tokens in chunk 8: 512 Number of tokens in chunk 9: 512 Number of tokens in chunk 10: 512 Number of tokens in chunk 11: 512 Number of tokens in chunk 12: 512 Number of tokens in chunk 13: 512 Number of tokens in chunk 14: 512 Number of tokens in chunk 15: 512 Number of tokens in chunk 16: 512 Number of tokens in chunk 17: 341 . Each of these chunks (except for the last one) has the structure: . [CLS], 10 question tokens, [SEP], 497 tokens of the Wikipedia article, [SEP] token = 512 tokens . Each of these chunks can now be fed to the model without causing indexing errors. We&#39;ll get an answer for each chunk. Most times, the chunk will not contain the answer (not every segment of a Wikipedia article is generally informative for our question). When the model determines that the context does not answer the question it returns the [CLS] token. . def convert_ids_to_string(tokenizer, input_ids): return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids)) answer = &#39;&#39; # Now we iterate over our chunks, looking for the best answer from each chunk for k, chunk in chunked_input.items(): answer_start_scores, answer_end_scores = model(**chunk) answer_start = torch.argmax(answer_start_scores) answer_end = torch.argmax(answer_end_scores) + 1 ans = convert_ids_to_string(tokenizer, chunk[&#39;input_ids&#39;][0][answer_start:answer_end]) if ans != &#39;[CLS]&#39;: answer += ans + &quot; / &quot; print(answer) . 3 . 7 m / . Put it all together . Time to put it all together now. We&#39;re using wikipedia&#39;s information retrieval system (search engine) to return a list of candidate documents that we then feed into our Document Reader (in this case, BERT fine-tuned on SQuAD 2.0). We&#39;ll now use Streamlit to create a simple interface for our app. In order to make the app code easier to read, we&#39;ll first package our Document Reader into a class. . from transformers import AutoTokenizer, AutoModelForQuestionAnswering class DocumentReader: def __init__(self, pretrained_model_name_or_path=&#39;bert-large-uncased&#39;): # &#39;bert-base-uncased&#39; self.READER_PATH = pretrained_model_name_or_path self.tokenizer = AutoTokenizer.from_pretrained(self.READER_PATH) self.model = AutoModelForQuestionAnswering.from_pretrained(self.READER_PATH) self.max_len = self.model.config.max_position_embeddings self.chunked = False def tokenize(self, question, text): self.inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=&quot;pt&quot;) self.input_ids = self.inputs[&quot;input_ids&quot;].tolist()[0] if len(self.input_ids) &gt; self.max_len: self.inputs = self.chunkify() self.chunked = True def chunkify(self): &quot;&quot;&quot; Break up a long article into chunks that fit within the max token requirement for that Transformer model. Calls to BERT / RoBERTa / ALBERT require the following format: [CLS] question tokens [SEP] context tokens [SEP] &quot;&quot;&quot; # TODO: generalize this because not all models include token_type_ids (distilBERT) # create question mask based on token_type_ids # value is 0 for question tokens, 1 for context tokens qmask = self.inputs[&#39;token_type_ids&#39;].lt(1) qt = torch.masked_select(self.inputs[&#39;input_ids&#39;], qmask) chunk_size = self.max_len - qt.size()[0] - 1 # the &quot;-1&quot; accounts for # having to add an ending [SEP] token to the end # create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input chunked_input = OrderedDict() for k,v in self.inputs.items(): q = torch.masked_select(v, qmask) c = torch.masked_select(v, ~qmask) chunks = torch.split(c, chunk_size) for i, chunk in enumerate(chunks): if i not in chunked_input: chunked_input[i] = {} thing = torch.cat((q, chunk)) if i != len(chunks)-1: if k == &#39;input_ids&#39;: thing = torch.cat((thing, torch.tensor([102]))) else: thing = torch.cat((thing, torch.tensor([1]))) chunked_input[i][k] = torch.unsqueeze(thing, dim=0) return chunked_input def get_answer(self): if self.chunked: answer = &#39;&#39; for k, chunk in self.inputs.items(): answer_start_scores, answer_end_scores = self.model(**chunk) answer_start = torch.argmax(answer_start_scores) answer_end = torch.argmax(answer_end_scores) + 1 ans = self.convert_ids_to_string(chunk[&#39;input_ids&#39;][0][answer_start:answer_end]) if ans != &#39;[CLS]&#39;: answer += ans + &quot; / &quot; return answer else: answer_start_scores, answer_end_scores = self.model(**self.inputs) answer_start = torch.argmax(answer_start_scores) # Get the most likely beginning of answer with the argmax of the score answer_end = torch.argmax(answer_end_scores) + 1 # Get the most likely end of answer with the argmax of the score return self.convert_ids_to_string(self.inputs[&#39;input_ids&#39;][0][ answer_start:answer_end]) def convert_ids_to_string(self, input_ids): return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids)) . import os cwd = os.getcwd() MODEL_PATHS = { &#39;default_bert_base_uncased&#39;: &#39;bert-base-uncased&#39;, &#39;bert_base_uncased_squad1&#39;: cwd+&quot;/models/bert/bert-base-uncased-tuned-squad-1.0&quot;, &#39;bert_base_cased_squad2&#39;: cwd+&quot;/models/bert/bert-base-cased-tuned-squad-2.0/&quot; } . questions = [ &#39;When was Barack Obama born?&#39;, &#39;Why is the sky blue?&#39;, &#39;How many sides does a pentagon have?&#39; ] reader = DocumentReader(&quot;deepset/bert-base-cased-squad2&quot;) # if you trained your own model using the training cell earlier you can access it with this: #reader = DocumentReader(&quot;./models/bert/bbu_squad2&quot;) for question in questions: results = wiki.search(question) page = wiki.page(results[0]) print(f&quot;Top result: {page}&quot;) text = page.content reader.tokenize(question, text) print(f&quot;Question: {question}&quot;) print(reader.get_answer()) . Top result: &lt;WikipediaPage &#39;Barack Obama Sr.&#39;&gt; Question: When was Barack Obama born? 18 June 1936 / August 1961 / 4 August 1961 / 6 May 2011 . = = See also = = Family / . #hidden-cell import logging logging.getLogger(&quot;transformers.tokenization_utils&quot;).setLevel(logging.ERROR) . Wrapping Up . There we have it! A working QA system on Wikipedia articles. This is great but it&#39;s admittedly not very sophisticated. Furthermore there are still a lot of unanswered questions: . Why the SQuAD dataset and not something else? What other options are there? | Why did we train BERT the way we did? Are there ways to make it better? What&#39;s the &quot;best&quot; BERT can be? should we train on more than just SQuAD? | If so, what other datasets should we train on? | How much does this increase performance? | . | Why BERT and not another Transformer model? What&#39;s the difference between all these Transformer models anyway? | . | How can we make our get_answer method more sophisticated and realistic? | How can we improve our chunkify method? | How to train Transformers with GPUs from your jupyter notebook . How to make sense of the SQuAD (and other QA) datasets . How to evaluate a Transformer model on a QA dataset . How to chose the right Transformer model for your QA system . How to create your own QA dataset .",
            "url": "https://qa.fastforwardlabs.com/hidden/",
            "relUrl": "/hidden/",
            "date": " • May 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Intro to Automated Question Answering",
            "content": "Welcome to the first edition of the Cloudera Fast Forward blog on Natural Language Processing for Question Answering! Throughout this series, we’ll build a Question Answering (QA) system with off-the-shelf algorithms and libraries and blog about our process and what we find along the way. We hope to wind up with a beginning-to-end documentary that provides: . insight into QA as a tool, | useful context to make decisions for those who might build their own QA system, | tips and tricks we pick up as we go, and | sample code and commentary. | . We’re trying a new thing here. In the past, we’ve documented our work in discrete reports at the end of our research process. We hope this new format suits the above goals and makes the topic more accessible, while ultimately being useful. . To kick off the series, this introductory post will discuss what QA is and isn’t, where this technology is being employed, and what techniques are used to accomplish this natural language task. . Question Answering in a Nutshell . Question Answering is a human-machine interaction to extract information from data using natural language queries. Machines do not inherently understand human languages any more than the average human understands machine language. A well-developed QA system bridges the gap between the two, allowing humans to extract knowledge from data in a way that is natural to us, i.e., asking questions. . QA systems accept questions in the form of natural language (typically text based, although you are probably also familiar with systems that accept speech input, such as Amazon’s Alexa or Apple’s Siri), and output a concise answer. Google’s search engine product adds a form of question answering in addition to its traditional search results, as illustrated here: . . Google took our question and returned a set of 1.3 million documents (not shown) relevant to the search terms, i.e., documents about Abraham Lincoln. Google also used what it knows about the contents of some of those documents to provide a “snippet” that answered our question in one word, presented above a link to the most pertinent website and keyword-highlighted text. . This goes beyond the standard capabilities of a search engine, which typically only return a list of relevant documents or websites. Google recently explained how they are using state-of-the-art NLP to enhance some of their search results. We’ll revisit this example in a later section and discuss how this technology works in practice and how we can (and will!) build our own QA system. . Why Question Answering? . Sophisticated Google searches with precise answers are fun, but how useful are QA systems in general? It turns out that this technology is maturing rapidly. Gartner recently identified natural language processing and conversational analytics as one of the top trends poised to make a substantial impact in the next three to five years. These technologies will provide increased data access, ease of use, and wider adoption of analytics platforms - especially to mainstream users. QA systems specifically will be a core part of the NLP suite, and are already seeing adoption in several areas. . Business Intelligence (BI) platforms are beginning to use Machine Learning (ML) to assist their users in exploring and analyzing their data through ML-augmented data preparation and insight generation. One of the key ways that ML is augmenting BI platforms is through the incorporation of natural language query functionality, which allows users to more easily query systems, and retrieve and visualize insights in a natural and user-friendly way, reducing the need for deep expertise in query languages, such as SQL. . Another area where QA systems will shine is in corporate and general use chatbots. Chatbots have been around for several years, but they mostly rely on hand-tailored responses. QA systems can augment this existing technology, providing a deeper understanding to improve user experience. For example, a QA system with knowledge of a company’s FAQs can streamline customer experience, while QA systems built atop internal company documentation could provide employees easier access to logs, reports, financial statements, or design docs. . The success of these systems will vary based on the use case, implementation, and richness of data. The field of QA is just starting to become commercially viable and it’s picking up speed. We think it’s a field worth exploring in order to understand what uses it might (and might not) have. So how does this technology work? . Designing a Question Answerer . As explained above, question answering systems process natural language queries and output concise answers. This general capability can be implemented in dozens of ways. How a QA system is designed depends, in large part, on three key elements: the knowledge provided to the system, the types of questions it can answer, and the structure of the data supporting the system. . Domain . QA systems operate within a domain, constrained by the data that is provided to them. The domain represents the embodiment of all the knowledge the system can know. There are two domain paradigms: open and closed. Closed domain systems are narrow in scope and focus on a specific topic or regime. Open domain systems are broad, answering general knowledge questions. . The BASEBALL system is an early example of a closed domain QA system. Built in the 1960s, it was limited to answering questions surrounding one year’s worth of baseball facts and statistics. Not only was this domain constrained to the topic of baseball, it was also constrained in the timeframe of data at its proverbial fingertips. A contemporary example of closed domain QA systems are those found in some BI applications. Generally, their domain is scoped to whatever data the user supplies, so they can only answer questions on the specific datasets to which they have access. . By contrast, open domain QA systems rely on knowledge supplied from vast resources - such as Wikipedia or the World Wide Web - to answer general knowledge questions. These systems can even answer general trivia. One example of such a system is IBM’s Watson, which won on Jeopardy! in 2011 (perhaps Watson was more of an Answer Questioner? We like jokes). Google’s QA capability as demonstrated above would also be considered open domain. . Question Type . Once you’ve decided the scope of knowledge your QA system will cover, you must also determine what types of questions it can answer. The vast majority of all QA systems answer factual questions: those that start with who, what, where, when, and how many. These types of questions tend to be straightforward enough for a machine to comprehend, and can be built directly atop structural databases or ontologies, as well as being extracted directly from unstructured text. . However, research is emerging that would allow QA systems to answer hypothetical questions, cause-effect questions, confirmation (yes/no) questions, and inferential questions (questions whose answers can be inferred from one or more pieces of evidence). Much of this research is still in its infancy, however, as the requisite natural language understanding is (for now) beyond the capabilities of most of today’s algorithms. . Implementation . There’s more than one way to cuddle a cat, as the saying goes. Question answering seeks to extract information from data and, generally speaking, data come in two broad formats: structured and unstructured. QA algorithms have been developed to harness the information from either paradigm: knowledge-based systems for structured data and information retrieval-based systems for unstructured (text) data. Some QA systems exploit a hybrid design that harvests information from both data types; IBM’s Watson is a famous example. In this section, we’ll highlight some of the most widely used techniques in each data regime - concentrating more on those for unstructured data, since this will be the focus of our applied research. Because we’ll be discussing explicit methods and techniques, the following sections are more technical. And we’ll note that, while we provide an overview here, an even more comprehensive discussion can be found in the Question Answering chapter of Jurafsky and Martin’s Speech and Language Processing (a highly accessible textbook). . Knowledge-Based Systems . A large quantity of data is encapsulated in structured formats, e.g., relational databases. The goal of knowledge-based QA systems is to map questions to these structured entities through semantic parsing algorithms. Semantic parsing techniques convert text strings to symbolic logic or query languages, e.g., SQL. . . Semantic parsing algorithms are highly tailored to their specific domain and database, and utilize templates as well as supervised learning approaches. Templates are handwritten rules, useful for frequently observed logical relationships. For example, an employee database might have a start-date template consisting of handwritten rules that search for when and hired since “when was Employee Name hired” would likely be a common query. . Supervised methods generalize this approach and are used when there exists a dataset of question-logical form pairs, such as in the figure above. These algorithms process the question, creating a parse tree that then maps the relevant parts of speech (nouns, verbs, and modifiers) to the appropriate logical form. Many algorithms begin with simple relationship mapping: matching segments from the question parse tree to a logical relation, as in the two examples below. . . The algorithm then bootstraps from simple relationship logic to incorporate more specific information from the parse tree, mapping it to more sophisticated logical queries like this birth-year example below. . . These systems can be made more robust by providing lexicons that capture the semantics and variations of natural language. For instance, in our employee database example, a question might contain the word “employed” rather than “hired,” but the intention is the same. . Information Retrieval-Based Systems: Retrievers and Readers . . Information retrieval-based question answering (IR QA) systems find and extract a text segment from a large collection of documents. The collection can be as vast as the entire web (open domain) or as specific as a company’s Confluence documents (closed domain). Contemporary IR QA systems first identify the most relevant documents in the collection, and then extract the answer from the contents of those documents. To illustrate this approach, let’s revisit our Google example from the introduction, only this time we’ll include some of the search results! . . We already talked about how the snippet box acts like a QA system. The search results below the snippet illustrate some of the reasons why an IR QA system can be more useful than a search engine alone. The relevant links vary from what is essentially advertising (study.com) to making fun of Lincoln’s ears (Reddit at its finest) to a discussion of color blindness (answers.com without the answer we want) to an article about all presidents’ eye colors (getting warmer, Chicago Tribune) to the very last link (answers.yahoo.com, which is on-topic - and narrowly scoped to Lincoln - but gives an ambiguous answer). Without the snippet box at the top, a user would have to skim each of these links to locate their answer - with varying degrees of success. . IR QA systems are not just search engines, which take general natural language terms and provide a list of relevant documents. IR QA systems perform an additional layer of processing on the most relevant documents to deliver a pointed answer, based on the contents of those documents (like the snippet box). While we won’t hazard a guess at exactly how Google extracted “gray” from these search results, we can examine how an IR QA system could exhibit similar functionality in a real world (e.g., non-Google) implementation. . Below we illustrate the workflow of a generic IR-based QA system. These systems generally have two main components: the document retriever and the document reader. . . The document retriever functions as the search engine, ranking and retrieving relevant documents to which it has access. It supplies a set of candidate documents that could answer the question (often with mixed results, per the Google search shown above). The document reader consists of reading comprehension algorithms built with core NLP techniques. This component processes the candidate documents and extracts from one of them an explicit span of text that best satisfies the query. Let’s dive deeper into each of these components. . Document Retriever . The document retriever has two core jobs: process the question for use in an IR engine, and use this IR query to retrieve the most appropriate documents and passages. Query processing can be as simple as no processing at all, and instead passing the entire question to the search engine. However, if the question is long or complicated, it often pays to process the query through various techniques - such as stop word removal, removing wh-words, converting to n-grams, or extracting named entities as keywords. . Some systems also extract contextual information from the query, e.g., the focus of the question and the expected answer type, which can then be used in the Document Reader during the answer extraction phase. The focus of a question is the string within the query that the user is looking to fill. The answer type is categorical, e.g., person, location, time, etc. In our earlier example, “when was Employee Name hired?”, the focus would be “when” and the answer type might be a numeric date-time. . The IR query is then passed to an IR algorithm. These algorithms search over all documents often using standard tf-idf cosine matching to rank documents by relevance. The simplest implementations would pass the top n most relevant documents to the document reader for answer extraction but this, too, can be made more sophisticated by breaking documents into their respective passages or paragraphs and filtering them (based on named entity matching or answer type, for example) to narrow down the number of passages sent to the document reader. . Document Reader . Once we have a selection of relevant documents or passages, it’s time to extract the answer. The sole purpose of the document reader is to apply reading comprehension algorithms to text segments for answer extraction. Modern reading comprehension algorithms come in two broad flavors: feature-based and neural-based. . Feature-based answer extraction can include rule-based templates, regex pattern matching, or a suite of NLP models (such as parts-of-speech tagging and named entity recognition) designed to identify features that will allow a supervised learning algorithm to determine whether a span of text contains the answer. One useful feature is the answer type identified by the document retriever during query processing. Other features could include the number of matched keywords in the question, the distance between the candidate answer and the query keywords, and the location of punctuation around the candidate answer. This type of QA works best when the answers are short and when the domain is narrow. . Neural-based reading comprehension approaches capitalize on the idea that the question and the answer are semantically similar. Rather than relying on keywords, these methods use extensive datasets that allow the model to learn semantic embeddings for the question and the passage. Similarity functions on these embeddings provide answer extraction. . Neural network models that perform well in this arena are Seq2Seq models and Transformers. (For a detailed dive into these architectures, interested readers should check out these excellent posts for Seq2Seq and Transformers.) The Transformer architecture in particular is currently revolutionizing the entire field of NLP. Models builts on this architecture include BERT (and its myriad off-shoots: RoBERTa, ALBERT, distilBERT, etc.), XLNet, GPT, T5, and more. These models - coupled with advances in compute power and transfer learning from massive unsupervised training sets - have started to outperform humans on some key NLP benchmarks, including question answering. . In this paradigm, one does not need to identify the answer type, the parts of speech, or the proper nouns. One need only feed the question and the passage into the model and wait for the answer. While this is an exciting development, it does have its drawbacks. When the model doesn’t work, it’s not always straightforward to identify the problem - and scaling these models is still a challenging prospect. These models generally perform better (according to your quantitative metric of choice) relative to the number of parameters they have (the more, the better), but the cost of inference also goes up - and with it, the difficulty of implementation in settings like federated learning scenarios or on mobile devices. . Building a Question-Answerer . At the beginning of this article, we said we were going to build a QA system. Now that we’ve covered some background, we can describe our approach. Over the course of the next two months, two of Cloudera Fast Forward’s Research Engineers, Melanie Beck and Ryan Micallef, will build a QA system following the information retrieval-based method, by creating a document retriever and document reader. We’ll focus our efforts on exploring and experimenting with various Transformer architectures (like BERT) for the document reader, as well as off-the-shelf search engine algorithms for the retriever. Neither of us has built a system like this before, so it’ll be a learning experience for everyone. And that’s precisely why we wanted to invite you along for the journey! We’ll share what we learn each step of the way by posting and discussing example code, in addition to articles covering topics like: . existing QA training sets for Transformers and what you’ll need to develop your own | how to evaluate the quality of a QA system - both the reader and retriever | building a search engine over a large set of documents | and more! | . Because we’ll be writing about our work as we go, we might end up in some dead ends or run into some nasty bugs; such is the nature of research! When these things happen, we’ll share our thoughts on what worked, what didn’t, and why - but it’s important to note upfront that while we do have a solid goal in mind, the end product may turn out to be quite different than what we currently envision. Stay tuned; in our next post we’ll start digging into the nuts and bolts! .",
            "url": "https://qa.fastforwardlabs.com/methods/background/2020/04/28/Intro-to-QA.html",
            "relUrl": "/methods/background/2020/04/28/Intro-to-QA.html",
            "date": " • Apr 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Us",
          "content": "Cloudera Fast Forward Labs is a machine intelligence research and advisory group within Cloudera. We routinely publish comprehensive reports that focus on a wide variety of machine learning capabilities. This blog focuses on question answering, a natural language processing and understanding (NLP/U) task that has recently begun to show signs of maturity, and documents our approach to implementing this model in a general setting, but with corporate use cases in mind. This project is led by two of CFF’s Research Engineers. . Melanie R. Beck, Ph.D. . Melanie is a reformed astrophysicist who now spends her days dreaming about machines and algorithms (to be honest, she still dreams about galaxies, too). She excels at understanding academic inventions but is even better at translating them into a language the rest of us can understand. Lately, she’s been obsessed with all things NLP. With experience as a data scientist in multiple industries ranging from hardware manufacturing to cybersecurity, she is a jack-of-all-trades with a passion for sharing what she’s learned. When she’s not reading about some esoteric machine learning framework, she can be found playing with her dog and two cats, or cross- stitching. . Ryan Micallef . Ryan researches emerging machine learning technologies and helps clients apply them. He is also an attorney barred in New York. He was an intellectual property litigator focused on technical cases for almost a decade before joining Fast Forward Labs (which has since been acquired by Cloudera). Ryan has a bachelor’s degree in Computer Science from Georgia Tech and a Juris Doctor degree from Brooklyn Law School. Ryan spends his free time soldering circuits and wrenching motorcycles, and teaches microcontroller programming at his local hackerspace, NYC Resistor. .",
          "url": "https://qa.fastforwardlabs.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}