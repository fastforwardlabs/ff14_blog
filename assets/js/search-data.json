{
  
    
        "post0": {
            "title": "Evaluating the Retriever & End-to-End System",
            "content": "In our last post, Evaluating QA: Metrics, Predictions, and the Null Response, we took a deep dive look at how to asses the quality of a BERT-like Reader for Question Answering (QA) using the Hugging Face framework. In this post, we&#39;ll focus on the former component of an end-to-end QA system - the Retriever. Specifically, we&#39;ll introduce Elasticsearch as a powerful and efficient Information Retrieval (IR) tool that can be used to scour through large corpora and retrieve relevant documents. Through the post, we&#39;ll explain how to implement and evaluate a Retriever in the context of Question Answering and demonstrate the impact it has on an end-to-end QA system. . Prerequisites . a basic understanding of Information Retrieval &amp; Search | a basic understanding of IR based QA systems (see previous posts) | a basic understanding of Transformers and PyTorch | a basic understanding of the SQuAD2.0 dataset | . Retrieving the right document is important . . We believe what Michael Scott really meant to say is: . &quot;You miss 100% of the questions if the answer doesn&#39;t appear in the input context&quot; . As we have discussed throughout this blog series, many modern QA systems take a two-staged approach to answering questions. In the first stage, a document retriever selects N potentially relevant documents from a given corpus. Subsequently, a machine comprehension model processes each of the N documents to determine an answer to the input question. Because of recent advances in NLP and deep learning (i.e. flashy Transformer models), the machine comprehension component of question answering has typically been the main focus of evaluation. Stage one of these systems has recieved limited attention despite its obvious importance...stage two is bounded by performance at stage one. Let&#39;s get more specific. . We recently explained methods that enable BERT-like models to produce robust answers given a question and context passage by selectively processing predictions and by refraining from answering certain questions at all. While the ability to properly comprehend a passage and produce a correct answer is a critical feature of any QA tool, the success of the overall system is highly dependent on first providing a correct passage to read through. Without being fed a context passage that actually contains the ground-truth answer, the overall system&#39;s performance is limited to how well it can predict no-answer questions. To demonstrate, we&#39;ll revisit an example from our second blog post where three questions were asked of the Wikipedia search engine based QA system: . **Example 1: Incorrect** Question: When was Barack Obama born? Top wiki result: &lt;WikipediaPage &#39;Barack Obama Sr.&#39;&gt; Answer: 18 June 1936 / February 2 , 1961 / **Example 2: Correct** Question: Why is the sky blue? Top wiki result: &lt;WikipediaPage &#39;Diffuse sky radiation&#39;&gt; Answer: Rayleigh scattering / **Example 3: Correct** Question: How many sides does a pentagon have? Top wiki result: &lt;WikipediaPage &#39;The Pentagon&#39;&gt; Answer: five / . In Example 1, the Reader had no chance of producing the correct answer because of its outright absence from the context served up by the Retriever. Namely, the Retriever erroneously provided a page about Barack Obama Sr. instead of his son, the former US President. In this case, the only way the Reader could have possibly produced the correct answer was if the correct answer was actually not to answer at all. On the flip side, in Example 3, the Retriever did not identify the globally &quot;correct&quot; document - it returned an article about &quot;The Pentagon&quot; instead of a page about geometry - but nonetheless, it provided enough context for the Reader to succeed. . These quick examples illustrate why an effective Retriever is crucial for an end-to-end QA system. Now let&#39;s take a deeper look at a classic tool used for information retrieval - Elasticsearch. . Elasticsearch as an IR Tool . . Modern QA systems employ a variety of techniques for the task of information retrieval ranging from traditional sparse vector word matching (ex. Elasticsearch) to novel approaches using dense representations of encoded passages combined with efficient search capabilities. Despite the flurry of contemporary research efforts in this area, the traditional sparse vector approach performs very well overall and has only recently been overtaken by embedding-based systems for end-to-end QA retrieval tasks. For that reason, we&#39;ll explore Elasticsearch as an easy to use framework for document retrieval. So, what exactly is Elasticsearch? . Elasticsearch is a powerful open-source search and analytics engine built on the Apache Lucene library that is capable of handling all types of data including textual, numerical, geospatial, structrured, and unstructured. It is built to scale with a robust set of features, rich ecosystem, and diverse list of client libraries making it easy to integrate and use. In the context of information retrieval for automated question answering, we are keenly interested in the features surrounding full-text search. Elasticsearch provides a convenient way to index documents so they can quickly be queried for nearest neighbor search using a TF-IDF based similarity metric. Specifically, it uses BM25 term weighting to represent question and context passages as high-dimensional, sparse vectors that are efficiently searched in an inverted index. For more information on how an inverted index works under the hood, we recommend this quick and concise blog post. . Using Elasticsearch with SQuAD2.0 . With this basic understanding of how Elasticsearch works, let&#39;s dive in and build our own Document Retrieval system by indexing a set of Wikipedia article paragraphs that support questions and answers from the SQuAD2.0 dataset. Before we get started, we&#39;ll need to download and prepare data from SQuAD2.0. . Download and Prepare SQUAD2.0 . # collapse-hide # Download the SQuAD2.0 train &amp; dev sets !wget -P data/squad/ https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json !wget -P data/squad/ https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json import json . . A common practice in IR for QA is to segment large articles into smaller passages before indexing for two main reasons: . Transformer based Readers are very slow - providing an entire Wikipedia article to BERT for processing takes a considerable amount of time and also defeats the purpose of using an IR tool to narrow the search space for the Reader. | Smaller passages reduce noise - by identifying a more concise context passage for BERT to read through, we reduce the chance of BERT getting lost. | Of course the chunking method proposed here doesn&#39;t come without a cost. Larger document size means each document contains more information. By reducing passage size, we are potentially trading off system recall for speed - though there are techniques to alleviate this as we will disucss later in the post. . With our chunking approach, each article paragraph will be prepended with the article title and collectively serve as a corpus of documents for which our Elasticsearch Retriever will search over. In practice, open-domain QA systems sit atop massive collections of documents (think all of Wikipedia) to provide a breadth of information to answer general-knowledge questions from. For the purposes of demonstrating Elasticsearch functionality, we will limit our corpus to only the Wikipedia articles supporting SQuAD2.0 questions. . The following parse_qa_records function will extract question/answer examples, as well as paragraph content from the SQuAD2.0 data set. . # collapse-hide def parse_qa_records(data): &#39;&#39;&#39; Loop through SQuAD2.0 dataset and parse out question/answer examples and unique article paragraphs Returns: qa_records (list) - Question/answer examples as list of dictionaries wiki_articles (list) - Unique Wikipedia titles and article paragraphs recreated from SQuAD data &#39;&#39;&#39; num_with_ans = 0 num_without_ans = 0 qa_records = [] wiki_articles = {} for article in data: for i, paragraph in enumerate(article[&#39;paragraphs&#39;]): wiki_articles[article[&#39;title&#39;]+f&#39;_{i}&#39;] = article[&#39;title&#39;] + &#39; &#39; + paragraph[&#39;context&#39;] for questions in paragraph[&#39;qas&#39;]: qa_record = {} qa_record[&#39;example_id&#39;] = questions[&#39;id&#39;] qa_record[&#39;document_title&#39;] = article[&#39;title&#39;] qa_record[&#39;question_text&#39;] = questions[&#39;question&#39;] try: qa_record[&#39;short_answer&#39;] = questions[&#39;answers&#39;][0][&#39;text&#39;] num_with_ans += 1 except: qa_record[&#39;short_answer&#39;] = &quot;&quot; num_without_ans += 1 qa_records.append(qa_record) wiki_articles = [{&#39;document_title&#39;:title, &#39;document_text&#39;: text} for title, text in wiki_articles.items()] print(f&#39;Data contains {num_with_ans} question/answer pairs with a short answer, and {num_without_ans} without.&#39;+ f&#39; nThere are {len(wiki_articles)} unique wikipedia article paragraphs.&#39;) return qa_records, wiki_articles . . # load and parse data train_file = &quot;data/squad/train-v2.0.json&quot; dev_file = &quot;data/squad/dev-v2.0.json&quot; train = json.load(open(train_file, &#39;rb&#39;)) dev = json.load(open(dev_file, &#39;rb&#39;)) qa_records, wiki_articles = parse_qa_records(train[&#39;data&#39;]) qa_records_dev, wiki_articles_dev = parse_qa_records(dev[&#39;data&#39;]) . Data contains 86821 question/answer pairs with a short answer, and 43498 without. There are 19035 unique wikipedia article paragraphs. Data contains 5928 question/answer pairs with a short answer, and 5945 without. There are 1204 unique wikipedia article paragraphs. . # Parsed record example qa_records[10] . {&#39;example_id&#39;: &#39;56d43c5f2ccc5a1400d830ab&#39;, &#39;document_title&#39;: &#39;Beyonc√©&#39;, &#39;question_text&#39;: &#39;What was the first album Beyonc√© released as a solo artist?&#39;, &#39;short_answer&#39;: &#39;Dangerously in Love&#39;} . # Parsed wiki paragraph example print(wiki_articles[10]) . {&#39;document_title&#39;: &#39;Beyonc√©_10&#39;, &#39;document_text&#39;: &#39;Beyonc√© Beyonc√© &#39;s first solo recording was a feature on Jay Z &#39;s &#34; &#39;03 Bonnie &amp; Clyde&#34; that was released in October 2002, peaking at number four on the U.S. Billboard Hot 100 chart. Her first solo album Dangerously in Love was released on June 24, 2003, after Michelle Williams and Kelly Rowland had released their solo efforts. The album sold 317,000 copies in its first week, debuted atop the Billboard 200, and has since sold 11 million copies worldwide. The album &#39;s lead single, &#34;Crazy in Love&#34;, featuring Jay Z, became Beyonc√© &#39;s first number-one single as a solo artist in the US. The single &#34;Baby Boy&#34; also reached number one, and singles, &#34;Me, Myself and I&#34; and &#34;Naughty Girl&#34;, both reached the top-five. The album earned Beyonc√© a then record-tying five awards at the 46th Annual Grammy Awards; Best Contemporary R&amp;B Album, Best Female R&amp;B Vocal Performance for &#34;Dangerously in Love 2&#34;, Best R&amp;B Song and Best Rap/Sung Collaboration for &#34;Crazy in Love&#34;, and Best R&amp;B Performance by a Duo or Group with Vocals for &#34;The Closer I Get to You&#34; with Luther Vandross.&#39;} . Download Elasticsearch . With our data ready to go, let&#39;s download, install, and configure Elasticsearch. We recommend running this notebook in Colab and executing the following code snippet for automatic setup. However, if you prefer to run locally, follow the instructions here. After completing the setup, we will have an Elasticsearch service running locally. . # collapse-hide # If using Colab - Start Elasticsearch from source ! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q ! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz ! chown -R daemon:daemon elasticsearch-7.6.2 import os from subprocess import Popen, PIPE, STDOUT es_server = Popen([&#39;elasticsearch-7.6.2/bin/elasticsearch&#39;], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1) # as daemon ) # wait until ES has started ! sleep 30 . . Getting Data into Elasticsearch . We&#39;ll use the official low-level Python client library for interacting with Elasticsearch. . # collapse-hide !pip install elasticsearch !pip install tqdm . . By default, Elasticsearch is launched locally on port 9200. We first need to instantiate an Elasticsearch client object and connect to the service. . from elasticsearch import Elasticsearch config = {&#39;host&#39;:&#39;localhost&#39;, &#39;port&#39;:9200} es = Elasticsearch([config]) # test connection es.ping() . True . Before we go further, let&#39;s introduce a few concepts that are specific to Elasticsearch and the process of indexing data. In Elasticsearch, an index is a collection of documents that have common characteristics (similar to a database schema in an RDBMS). Documents are JSON objects having their own set of key-value pairs consisting of various data types (similar to rows/fields in RDBMS). When we add a document into an index, the value for the document&#39;s text fields go through an analysis process prior to being indexed. This means that when executing a search query against an existing index, we are actually searching against the post-processed representation that is stored in the inverted index, not the raw input document itself. . Image Credit . The anaysis process is a customizable pipeline carried out by a dedicated Analyzer. Elasticsearch analyzers are composed of three sequential steps that form a processing pipeline: character filters, a tokenizer, and token filters. Each of these components modify the input stream of text according to some configurable settings. . Character Filters: First, character filters have the ability to add, remove, or replace specific items in the text field. A common application of this filter is to strip html markup from the raw input. | Tokenizer: After applying character filters, the transformed text is then passed to a tokenizer which breaks up the input string into individual tokens with a provided strategy. By default, the standard tokenizer splits tokens whenever it encounters a whitespace character, and also splits on most symbols (like commas, periods, semicolons, etc.) | Token Filters: Finally, the token stream is passed to a token filter which acts to add, remove, or modify tokens. Typical token filters include lowercase which converts all tokens to lowercase form, and stop which removes commonly occuring tokens called stopwords. | . Elasticsearch comes with several built-in Analyzers that satisfy common use cases and defaults to the Standard Analyzer. The Standard Analyzer doesn&#39;t contain any character filters, uses a standard tokenizer, and applies a lowercase token filter. Let&#39;s take a look at this example sentence as its passed through this pipeline: . &quot;I&#39;m in the mood for drinking semi-dry red wine!&quot; . Image Credit . Crafting analyzers to your use case requires domain knowledge of the problem and dataset at hand, and doing so properly is key to optimizing relevance scoring for your search application. We found this blog series very useful in explaining the importance of analysis in Elasticsearch. . Create an Index . Let&#39;s create a new index and add our Wikipedia articles to it. To do so, we provide a name and optionally some index configurations. Here we are specifying a set of mappings that indicate our anticipated index schema, data types, and how the text fields should be processed. If no body is passed, Elasticsearch will automatically infer fields and data types from incoming documents, as well as apply the Standard Analyzer to any text fields. . index_config = { &quot;settings&quot;: { &quot;analysis&quot;: { &quot;analyzer&quot;: { &quot;standard_analyzer&quot;: { &quot;type&quot;: &quot;standard&quot; } } } }, &quot;mappings&quot;: { &quot;dynamic&quot;: &quot;strict&quot;, &quot;properties&quot;: { &quot;document_title&quot;: {&quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;standard_analyzer&quot;}, &quot;document_text&quot;: {&quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;standard_analyzer&quot;} } } } index_name = &#39;squad-standard-index&#39; es.indices.create(index=index_name, body=index_config, ignore=400) . {&#39;acknowledged&#39;: True, &#39;index&#39;: &#39;squad-standard-index&#39;, &#39;shards_acknowledged&#39;: True} . Populate the Index . We can then loop through our list of Wikipedia titles &amp; articles and add them to our newly created Elasticsearch index. . # collapse-hide from tqdm.notebook import tqdm def populate_index(es_obj, index_name, evidence_corpus): &#39;&#39;&#39; Loads records into an existing Elasticsearch index Args: es_obj (elasticsearch.client.Elasticsearch) - Elasticsearch client object index_name (str) - Name of index evidence_corpus (list) - List of dicts containing data records &#39;&#39;&#39; for i, rec in enumerate(tqdm(evidence_corpus)): try: index_status = es_obj.index(index=index_name, id=i, body=rec) except: print(f&#39;Unable to load document {i}.&#39;) n_records = es_obj.count(index=index_name)[&#39;count&#39;] print(f&#39;Succesfully loaded {n_records} into {index_name}&#39;) return . . all_wiki_articles = wiki_articles + wiki_articles_dev populate_index(es_obj=es, index_name=&#39;squad-standard-index&#39;, evidence_corpus=all_wiki_articles) . Succesfully loaded 20239 into squad-standard-index . Search the Index . Wahoo! We now have some documents loaded into into an index. Elasticsearch provides a rich query language that supports a diverse range of query types. For this example, we&#39;ll use the standard query for performing full-text search called a match query. By default, Elasticsearch sorts and returns a JSON reponse of search results based on a computed a relevance score which indicates how well a given document matches the query. Along with the relevance score of each matched document, the search response also includes the amount of time the query took to run. . Let&#39;s look at a simple match query used to search the document_text field in our newly created index. . Important: As previously mentioned, all documents in the index have gone through an analysis process prior to indexing - this is called index time analysis. To maintain consistency in matching text queries against the post-processed index tokens, the same Analyzer used on a given field at index time is automatically applied to the query text at search time. Search time analysis is applied depending on which query type is used - match queries apply search time analysis by default. . # collapse-hide def search_es(es_obj, index_name, question_text, n_results): &#39;&#39;&#39; Execute an Elasticsearch query on a specified index Args: es_obj (elasticsearch.client.Elasticsearch) - Elasticsearch client object index_name (str) - Name of index to query query (dict) - Query DSL n_results (int) - Number of results to return Returns res - Elasticsearch response object &#39;&#39;&#39; # construct query query = { &#39;query&#39;: { &#39;match&#39;: { &#39;document_text&#39;: question_text } } } res = es_obj.search(index=index_name, body=query, size=n_results) return res . . question_text = &#39;Who was the first president of the Republic of China?&#39; # execute query res = search_es(es_obj=es, index_name=&#39;squad-standard-index&#39;, question_text=question_text, n_results=10) . print(f&#39;Question: {question_text}&#39;) print(f&#39;Query Duration: {res[&quot;took&quot;]} milliseconds&#39;) print(&#39;Title, Relevance Score:&#39;) [(hit[&#39;_source&#39;][&#39;document_title&#39;], hit[&#39;_score&#39;]) for hit in res[&#39;hits&#39;][&#39;hits&#39;]] . Question: Who was the first president of the Republic of China? Query Duration: 74 milliseconds Title, Relevance Score: . [(&#39;Modern_history_54&#39;, 23.131157), (&#39;Nanjing_18&#39;, 17.076923), (&#39;Republic_of_the_Congo_10&#39;, 16.840765), (&#39;Prime_minister_16&#39;, 16.137493), (&#39;Korean_War_29&#39;, 15.801523), (&#39;Korean_War_43&#39;, 15.586578), (&#39;Qing_dynasty_52&#39;, 15.291815), (&#39;Chinese_characters_55&#39;, 14.773873), (&#39;Korean_War_23&#39;, 14.736045), (&#39;2008_Sichuan_earthquake_48&#39;, 14.417962)] . Evaluating Retriever Performance . Ok, so we now have a basic understanding of how to use Elasticsearch as an IR tool to return some results for a given question, but how do we know if it&#39;s working? How do we evaluate what a good IR tool looks like? . To evaluate how well our Retriever is working, we&#39;ll need two things: some labeled examples (i.e. SQuAD2.0 question/answer pairs) and some performance metrics. In the conventional world of information retrieval, there are many metrics) used to quantify the relevance of query results - largely centered around the concepts of precision and recall. For IR in the context of question answering, we adapt some of these ideas into two commonly used evaluation metrics: recall and mean average precision (mAP). Additionally, we evaluate the amount of time required to execute a query since the main point of having a two stage QA system is to efficiently narrow the large search space for our Reader. . Recall . Traditionally, recall in IR indicates the fraction of all relevant documents that are retrieved. In the context of end-to-end QA systems sitting on large corpora of documents, we are less concerned with finding all of the passages containing the answer (because it would take significant time to read through all of them anyway) and more concerned with the binary presence of a passage containing the correct answer being returned. In that light, we define a Retriever&#39;s recall across a set of questions as the percentage of questions for which the answer segment appears in one of the top N pages returned by the search method. . Mean Average Precision . While the recall metric focuses on the minimum viable result set to enable a Reader for success, we do still care about the composition of that result set. We want a metric that rewards a Retriever for a.) returning a lot of answer-containing documents in the result set (i.e. traditional meaning of precision) and b.) returning those answer-containing documents higher up in the result set than non-answer-containing documents (i.e. ranking them correctly). This is precisely (üôÉ) what mean average precision (mAP) does for us. . To explain mAP further, let&#39;s first break down the concept of average precision for information retrieval. If our Retriever is asked to return N documents and the total number of those N documents that actually contains the answer is m, then average precision (AP) is defined as: . . where rel(k) is just a binary indication of whether the kth passage contains the correct answer segment or not. Using a concrete example, consider retrieving N=3 documents, of which one actually contains the correct answer segment. Here are three scenarios for how this could happen: . . Despite the fact that in each scenario we only have one document containing the correct answer, Scenario A is rewarded with the highest score because it was able to correctly rank the ground truth document relative to the others returned. Since average precision is calculated on a per query basis, the mean average precision is simply just the average AP across all queries. . Now using our Wikipedia passage index, let&#39;s define a function called evaluate_retriever to loop through all quesion/answer examples from the SQuAD2.0 train set and see how well our Elasticsearch retriever peforms in terms of recall, mAP, and averege query duration when retrieving N=3 passages. . # collapse-hide import numpy as np import pandas as pd def average_precision(binary_results): &#39;&#39;&#39; Calculates the average precision for a list of binary indicators &#39;&#39;&#39; m = 0 precs = [] for i, val in enumerate(binary_results): if val == 1: m += 1 precs.append(sum(binary_results[:i+1])/(i+1)) ap = (1/m)*np.sum(precs) if m else 0 return ap def evaluate_retriever(es_obj, index_name, qa_records, n_results): &#39;&#39;&#39; This function loops through a set of question/answer examples from SQuAD2.0 and evaluates Elasticsearch as a information retrieval tool in terms of recall, mAP, and query duration. Args: es_obj (elasticsearch.client.Elasticsearch) - Elasticsearch client object index_name (str) - name of index to query qa_records (list) - list of qa_records from preprocessing steps n_results (int) - the number of results ElasticSearch should return for a given query Returns: test_results_df (pd.DataFrame) - a dataframe recording search results info for every example in qa_records &#39;&#39;&#39; results = [] for i, qa in enumerate(tqdm(qa_records)): ex_id = qa[&#39;example_id&#39;] question = qa[&#39;question_text&#39;] answer = qa[&#39;short_answer&#39;] # construct and execute query query = { &#39;query&#39;: { &#39;match&#39;: { &#39;document_text&#39;: question } } } res = search_es(es_obj=es_obj, index_name=index_name, query=query, n_results=n_results) # calculate performance metrics from query response info duration = res[&#39;took&#39;] binary_results = [int(answer.lower() in doc[&#39;_source&#39;][&#39;document_text&#39;].lower()) for doc in res[&#39;hits&#39;][&#39;hits&#39;]] ans_in_res = int(any(binary_results)) ap = average_precision(binary_results) rec = (ex_id, question, answer, duration, ans_in_res, ap) results.append(rec) # format results dataframe cols = [&#39;example_id&#39;, &#39;question&#39;, &#39;answer&#39;, &#39;query_duration&#39;, &#39;answer_present&#39;, &#39;average_precision&#39;] results_df = pd.DataFrame(results, columns=cols) # format results dict metrics = {&#39;Recall&#39;: results_df.answer_present.value_counts(normalize=True)[1], &#39;Mean Average Precision&#39;: results_df.average_precision.mean(), &#39;Average Query Duration&#39;:results_df.query_duration.mean()} return results_df, metrics . . # combine train/dev examples and filter out SQuAD records that # do not have a short answer for the given question all_qa_records = qa_records+qa_records_dev qa_records_answerable = [record for record in all_qa_records if record[&#39;short_answer&#39;] != &#39;&#39;] # run evaluation results_df, metrics = evaluate_retriever(es_obj=es, index_name=&#39;squad-standard-index&#39;, qa_records=qa_records_answerable, n_results=3) . . metrics . {&#39;Recall&#39;: 0.8226180336176131, &#39;Mean Average Precision&#39;: 0.7524133234140888, &#39;Average Query Duration&#39;: 3.0550841518506937} . Improving Search Results with a Custom Analyzer . Identifying a correct passage in the Top 3 results for 82% of the SQuAD questions in ~3 milliseconds per question is not too bad! But that means that we&#39;ve effectively limited our overall QA system to an 82% upper bound on performance. How can we improve upon this? . One simple and obvious way to increase recall would be to just retrieve more passages. The following figure shows the effects of varying corpus size and result size on Elasticsearch retriever recall. As expected we see that the number of passages retrieved (i.e. Top N) has a dramatic impact on recall; a ~10-15 point jump from 1 to 3 passages returned, and ~5 point jump for each of the other tiers. We also see a gradual decrease in recall as corpus size increases, which isn&#39;t surprising. . . While increasing the number of passages retrieved is effective, it also has implications on overall system performance as the already slow Reader now has to reason over more text. Instead, we can lean on best practices in the well explored domain of information retrieval. . Optimizing full-text search is a battle between precision (returning as few irrelevant documents as possible) and recall (returning as many relevant documents as possible). Matching only exact words in the question query results in high precision, however it misses out on many passages that could possibly be considered relevant. Rather we can cast a wider net by searching for terms that are not exactly the same as in the question query, but are related in some way. Let&#39;s explore a few tips and tricks for widening the net. . Earlier in the post we described Elasticsearch Analyzers as a way to deal with the nuances of human language. Analyzers provide a flexible and extensible method to tailor search for a given dataset. Here are a few custom Analyzer components Elasticsearch provides that can help cast a wider net. . Stopwords: Stopwords are the most frequently occuring words in the English language (ex. &quot;and&quot;, &quot;the&quot;, &quot;to&quot;, etc.) and add minimal semantic value to a piece of text. Common practice in information retrieval is to identify and remove them from documents to decrease the size of the index and increase relevance of search results. . | Stemming: The English language is inflected - words can alter their written form to express different meanings. For example, &#39;sing&#39;, &#39;sings&#39;, &#39;sang&#39;, and &#39;singing&#39; are written with slight differences, but really mean the same thing. Stemming algorithms exploit the fact that search intent is usually word-form agnostic and attempts to reduce inflected words to their root form - consequently improving retrievability. We&#39;ll implement the Snowball stemming algorithm as a token filter in our custom Analyzer. . | . # create new index index_config = { &quot;settings&quot;: { &quot;analysis&quot;: { &quot;analyzer&quot;: { &quot;stop_stem_analyzer&quot;: { &quot;type&quot;: &quot;custom&quot;, &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;:[ &quot;lowercase&quot;, &quot;stop&quot;, &quot;snowball&quot; ] } } } }, &quot;mappings&quot;: { &quot;dynamic&quot;: &quot;strict&quot;, &quot;properties&quot;: { &quot;document_title&quot;: {&quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;stop_stem_analyzer&quot;}, &quot;document_text&quot;: {&quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;stop_stem_analyzer&quot;} } } } es.indices.create(index=&#39;squad-stop-stem-index&#39;, body=index_config, ignore=400) # populate the index populate_index(es_obj=es, index_name=&#39;squad-stop-stem-index&#39;, evidence_corpus=all_wiki_articles) # evaluate retriever performance stop_stem_results_df, stop_stem_metrics = evaluate_retriever(es_obj=es, index_name=&#39;squad-stop-stem-index&#39;, qa_records=qa_records_answerable, n_results=3) . stop_stem_metrics . {&#39;Recall&#39;: 0.8501115914996388, &#39;Mean Average Precision&#39;: 0.7800892731997112, &#39;Average Query Duration&#39;: 0.7684287701215108} . Awesome - we&#39;ve increased recall and mAP by ~3 points and reduced our average query duration by nearly 4x through simple pre-processing steps that just scratch the surface of tailored analysis in Elasticsearch. With that, there is no &quot;one size fits all&quot; recipe for optimizing search relevance and every implementation will be different. In addition to custom analysis, there are many other methods for increasing search recall like query expansion - that introduces additional tokens/phrases into a query at search time. We&#39;ll save that topics for another post and instead take a look at how the Retreiver&#39;s performance affects and end-to-end QA system. . Impact of Retriever in End-to-End QA System . We now want to evaluate the full QA system in which the retriever passes relevant documents to the reader. We used the questions from the train set to evaluate the stand-alone retriever in order to provide as large a collection as possible. However, BERT has been trained on those question and would return inflated performance values if we used them for full system evaluation. Instead we&#39;ll resort to our trusty SQuAD2.0 dev set. We&#39;ll also use the more efficient index that we developed in the last section in order to give our reader the best chance of receiving documents that contain the right answer. . Note: In this section we&#8217;ll focus on a discussion. The code to reproduce our results can be found at the end of the post. . Connecting the retriever to the reader . In our last post we evaluated a BERT-like model on the SQuAD2.0 dev set by providing the model with exactly the right paragraph in which to find an answer. This time, the retriever will serve up some number of documents that it deems most relevant. How should the retriever and the reader interact? And how should the reader determine which document out of all them contains the best answer? . This turns out to be one of the trickier subtleties in building a full QA system. There are several ways to approach this problem. We&#39;ll describe two that we tried: . Pass each document to the reader individually then aggregate the resulting scores | Concatenate all documents into one long passage and pass to the reader simultaneously | Pass each document individually . Both methods have pros and cons. In option 1, the reader returns answers and scores for each document. A series of heuristics must be developed to determine which answer is the best, and when the null answer should be returned. These heuristics can be as simple as: &quot;Only return null if the highest scoring answer for all documents is null, otherwise return the highest scoring non-null answer&quot; but complexity can easily grow from there. . Compounding this challenge is the fact that the answer scores associated with each document are actually not directly comparable to the answer scores from any other document. This is because the score returned is a softmax probability over the tokens in a given document - the probabilities sum to 1 for that document only. This means that an answer with a score of 0.78 from one document might not actually be inherently &quot;better&quot; than an answer with a score of .70 from another document! . Finally, this option is slower because each article is passed individually leading to multiple BERT calls. . Pass all documents together as one long context . Option 2 circumvents many of these challenges but leads to other problems. Creating one long context out of the top k articles solves many problems for us: . all candidate answers are scored on the same scale, | handling the null answer is more straightforward (we did that last time), | and we can take advantage of faster compute as HF will chunk long documents for us and pass them through BERT in a batch. | On the other hand, when concatenating multiple passages together, there&#39;s a good chance that BERT will see a mixed context: the end of one paragraph grafted onto the beginning of another, for example. This could make it more difficult for the model to correctly identify an answer in a potentially confusing context. Another drawback is that it&#39;s more difficult to backstrapolate which of the input documents the answer ultimately came from. . We created a reader class that leverages the HF question-answering pipeline to do the brunt of the work for us (loading models and tokenizers, converting text to features, chunking, prediction, etc.). We then created two methods: predict and predict_combine corresponding to option 1 and option 2, respectively. We tested our two methods over 1000 examples from the SQuAD2.0 dev set for a range of number of retrieved documents. . . There are two trends to recognize. First, we see that our method of concatenating documents together (blue bars) and passing them to the reader all at once outperforms passing documents invididually and applying heuristics to the outputs. While more sophistical heuristics can be developed, for short documents (paragraphs in this case) we find that the concatenation method is the most straightforward approach. . The second thing to notice is that, as the number of retrieved documents increases, the overall performance decreases. What&#39;s going on? When we evaluated the retriever, we found that increasing the number of retrieved documents increased the likelihood that the correct answer was contained in at least one of them. So why does reader performance degrade? . Evaluation methods . Standard evaluation on the SQuAD2.0 dev set considers only the best overall answer for each question. This is what we considered in the previous section and found that performance decreased as more documents are fed to the reader. However, we also found it instructive to consider an alternative. Mirroring our retriever recall metric from earlier, we constructed a similar metric for the full system. Specifically, we compute the percent of examples in which the correct answer is found in at least one of the top k documents provided by the retriever. . . The blue bars are the same as the blue bars in the figure above, but this time the orange bars represent our new recall-esque metric - what a difference! This demonstrates that when the model is provided with more documents, the correct answer is present more often! However, trying to predict which one of those answers is the right one is challenging -- a task that is not achieved by a simple heuristic. . Does this mean we shouldn&#39;t use QA systems like this? Of course not! There are several factors to consider: . Use Case: If your QA system seeks to provide enhanced search capabilities then it might not be necessary to predict a single answer with high confidence, instead providing answers from several documents for the user to peruse. On the other hand, if your use case seeks to augment a chatbot, then predicting a high confidence answer might be more important for user experience. . | Better heuristics: While our simple heuristic didn&#39;t perform as well as concatenating all the input documents into one long context, there is considerable research into developing heurstics that work. In particular, one promising approach is to develop a new score based on the both how highly a document is ranked, as well as the answer&#39;s score. . | Document length: Our concatenation method works reasonably well compared to other methods, but the documents are short - they&#39;re small Wikipedia paragraphs. If the document length becomes significantly longer, this method&#39;s performance can degrade significantly. . | Wrapping Up . We did it! We built a full QA system using off-the-shelf parts with ElasticSearch and HuggingFace Transformers. In addition to the takeaways above, we have one final parting thought. . In this post we made some design choices including the choice to index over Wikipedia paragraphs rather than full articles. This allowed us to more easily replicate SQuAD evaluation methods but this isn&#39;t practical. In the real world, a QA system will need to work on with existing indexes which are typically performed over long documents and not split into paragraphs. This provides the retriever with the best chance of returning a relevant document. . However, passing multiple long documents to a Transformer model is a recipe of boredom -- it will take forever and it likely won&#39;t be highly informative. Transformers work best with smaller passages. Thus, extracting a few highly relevant paragraphs from the most relevant document is a better recipe for a practical implementation. . This is exactly the approach we&#39;ll take next time when we (hopefully) address the biggest question of all: . How do I apply a QA system to my data? . Stay tuned! . Post Script: The Code . !pip install transformers . from transformers.data.processors import SquadV2Processor processor = SquadV2Processor() examples = processor.get_dev_examples(data_dir=&#39;data/squad/&#39;) . !curl -L -O https://raw.githubusercontent.com/melaniebeck/question_answering/master/src/readers.py . from readers import Reader() reader = Reader() . # collapse-hide def query(prediction_function, question, index_name, topk): &quot;&quot;&quot; Extracts answers from a full QA system: 1) Constructs query and retrieves the topk relevant documents 2) Passes those documents to the reader&#39;s prediction_function 3) Returns the topk answers for each of the k documents Inputs: prediction_function: either reader.predict or reader.predict_combine question: str, question string index_name: str, name of the index for the retriever topk: int, number of documents to retrieve from retriever; number of answers to extract from reader Outputs: answers: dict with format: { &#39;question&#39;: question string, &#39;answers&#39;: list of answer dicts from reader } &quot;&quot;&quot; query = { &#39;query&#39;: { &#39;match&#39;: { &#39;document_text&#39;: question } } } retriever_results = search_es(es_obj=es, index_name=index_name, query=query, n_results=topk) passages = retriever_results[&#39;hits&#39;][&#39;hits&#39;] docs = [] for passage in passages: doc = { &#39;id&#39;: passage[&#39;_id&#39;], &#39;score&#39;: passage[&#39;_score&#39;], &#39;text&#39;: passage[&#39;_source&#39;][&#39;document_text&#39;], &#39;title&#39;: passage[&#39;_source&#39;][&#39;document_title&#39;], } docs.append(doc) answers = prediction_function(question, docs, topk) return answers . . from transformers.data.metrics.squad_metrics import squad_evaluate from transformers.data.metrics.squad_metrics import compute_exact, normalize_answer def evaluate_qasystem_squadstyle( predition_function, examples, index_name, topk, output_path=&#39;data/&#39;, save_output=True ): &quot;&quot;&quot; Squad-style evaluation requires that only the best answer aggregated from all top k documentsis is provided as the prediction during evaluation. Inputs prediction_function: either reader.predict or reader.predict_combine examples: list, the SQuAD2.0 dev set as loaded by the squad processors index_name: str, name of the index in ElasticSearch topk: int, number of documents to retrieve output_path: str, directory to store prediction output save_output: bool, whether to save prediction output Outputs Saved to disk predictions: Best answer for each SQuAD2.0 question meta_predictions: Top N answers for each SQuAD question Returns results: OrderedDict of results from the HF squad_evaluate method &quot;&quot;&quot; import pickle outfile = output_path+f&quot;predictions_{index_name}_{topk}.pkl&quot; # if we&#39;ve already computed predictions, load them for evaluation if os.path.exists(outfile): predictions = pickle.load(open(outfile, &quot;rb&quot;)) else: predictions = {} meta_predictions = {} for example in tqdm(examples): # retrieve top N relevant documents from retriever reader_results = query(prediction_function, example.question_text, index_name, topk ) # add best answer to predictions answers = reader_results[&#39;answers&#39;] predictions[example.qas_id] = answers[0][&#39;answer_text&#39;] # for debugging/explainability - save the full answer # (not just text answer from top hit) meta_predictions[example.qas_id] = answers if save_output: pickle.dump(predictions, open(outfile, &quot;wb&quot;)) meta_outfile = os.path.splitext(outfile)[0]+&quot;_meta.pkl&quot; pickle.dump(meta_predictions, open(meta_outfile, &quot;wb&quot;)) # compute evaluation with HF results = squad_evaluate(examples, predictions) return results def evaluate_qasystem_recallstyle( predition_function, examples, index_name, topk, output_path=&#39;data/&#39;, save_output=True ): &quot;&quot;&quot; Recall-style evaluation computes the % of all examples that contain the correct answer in at least one of the documents. No prediction output is saved in this version because I got tired. This version only computes Exact Match because it&#39;s really hard to determine whether the correct answer is within the top k answers when computing F1 which is continuous between 0 and 1. Inputs prediction_function: either reader.predict or reader.predict_combine examples: list, the SQuAD2.0 dev set as loaded by the squad processors index_name: str, name of the index in ElasticSearch topk: int, number of documents to retrieve Outputs Returns results: OrderedDict of exact match results &quot;&quot;&quot; qas_id_to_has_answer = {example.qas_id: bool(example.answers) for example in examples} has_answer_qids = [qas_id for qas_id, has_answer in qas_id_to_has_answer.items() if has_answer] no_answer_qids = [qas_id for qas_id, has_answer in qas_id_to_has_answer.items() if not has_answer] has_correct_answer = {} for example in tqdm(examples): # retrieve top N relevant documents from retriever reader_results = query(prediction_function, example.question_text, index_name, topk ) # pull up all gold answers for this example qas_id = example.qas_id gold_answers = [answer[&quot;text&quot;] for answer in example.answers if normalize_answer(answer[&quot;text&quot;])] if not gold_answers: # For unanswerable questions, only correct answer is empty string gold_answers = [&quot;&quot;] # check if any of the gold answers is contained in any of the current predictions exact_scores = [] predictions = reader_results[&#39;answers&#39;] for prediction in predictions: exact_scores.append(max(compute_exact(a, prediction[&#39;answer_text&#39;]) for a in gold_answers)) has_correct_answer[qas_id] = int(any(exact_scores)) evaluation = make_eval_dict(has_correct_answer) if has_answer_qids: has_ans_eval = make_eval_dict(has_correct_answer, qid_list=has_answer_qids) merge_eval(evaluation, has_ans_eval, &quot;HasAns&quot;) if no_answer_qids: no_ans_eval = make_eval_dict(has_correct_answer, qid_list=no_answer_qids) merge_eval(evaluation, no_ans_eval, &quot;NoAns&quot;) return evaluation . Running the cell below requires a gpu so execute with caution or Colab. Evaluation over the top 1 retrieved documents takes about 35 minutes to run. .",
            "url": "https://qa.fastforwardlabs.com/hidden/",
            "relUrl": "/hidden/",
            "date": " ‚Ä¢ Jun 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Evaluating QA: Metrics, Predictions, and the Null Response",
            "content": ". In our last post, Building a QA System with BERT on Wikipedia, we used the HuggingFace framework to train BERT on the SQuAD2.0 dataset and built a simple QA system on top of the Wikipedia search engine. This time, we&#39;ll look at how to assess the quality of a BERT-like model for Question Answering. We&#39;ll cover what metrics are used to quantify quality, how to evaluate a model using the Hugging Face framework, and the importance of the &quot;null response&quot; (questions that don&#39;t have answers) for both improved performance and more realistic QA output. By the end of this post, we&#39;ll have implemented a more robust answering method for our QA system. . Note: Throughout this post we&#8217;ll be using a distilBERT model fine-tuned on SQuAD2.0 by a member of the NLP community; this model can be found here in the HF repository. Additionally, much of the code in this post is inspired by the HF squad_metrics.py script. . Prerequisites . a basic understanding of Transformers and PyTorch | a basic understanding of Transformer outputs (logits) and softmax | a Transformer fine-tuned on SQuAD2.0 | the SQuAD2.0 dev set | . Answering questions is complicated . Quantifying the success of question answering is a tricky task. When you or I ask a question, the correct answer could take multiple forms. For example, in our previous post, BERT answered the question, &quot;Why is the sky blue?&quot; with &quot;Rayleigh scattering,&quot; but another answer would be: . The Earth&#39;s atmosphere scatters short-wavelength light more efficiently than that of longer wavelengths. Because its wavelengths are shorter, blue light is more strongly scattered than the longer-wavelength lights, red or green. Hence the result that when looking at the sky away from the direct incident sunlight, the human eye perceives the sky to be blue. . Both of these answers can be found in the Wikipedia article Diffuse Sky Radiation and both are correct. However, we&#39;ve also had a model answer the same question with &quot;because its wavelengths are shorter,&quot; which is close - but not really a correct answer; the sky itself doesn&#39;t have a wavelength. This answer is missing too much context to be useful. What if we&#39;d asked a question that couldn&#39;t be answered by the Diffuse Sky Radiation page? For example: &quot;Could the sky ever be green?&quot; If you read that Wiki article you&#39;ll see there probably isn&#39;t a sure-fire answer to this question. What should the model do in this case? . How should we judge a model‚Äôs success when there are multiple correct answers, even more incorrect answers, and potentially no answer available to it at all? To properly assess quality, we need a labeled set of questions and answers. Let&#39;s turn back to the SQuAD dataset. . The SQuAD2.0 dev set . The SQuAD dataset comes in two flavors: SQuAD1.1 and SQuAD2.0. The latter contains the same questions and answers as the former, but also includes additional questions that cannot be answered by the accompanying passage. This is intended to create a more realistic question answering task. The ability to identify unanswerable questions is much more challenging for Transformer models, which is why we focused on the SQuAD2.0 dataset rather than SQuAD1.1. . SQuAD2.0 consists of over 150k questions, of which more than 35% are unanswerable in relation to their associated passage. For our last post, we fine-tuned on the train set (130k examples); now we&#39;ll focus on the dev set, which contains nearly 12k examples. Only about half of these examples are answerable questions. In the following section, we&#39;ll look at a couple of these examples to get a feel for them. . (Use the hidden cells below to get set up, if needed.) . # collapse-hide # use this cell to install packages if needed !pip install torch torchvision -f https://download.pytorch.org/whl/torch_stable.html !pip install transformers . . # collapse-hide import json import collections from pprint import pprint import numpy as np import torch from transformers import AutoTokenizer, AutoModelForQuestionAnswering # This is the directory in which we&#39;ll store all evaluation output model_dir = &quot;models/distilbert/twmkn9_distilbert-base-uncased-squad2/&quot; . . # collapse-hide # Download the SQuAD2.0 dev set !wget -P data/squad/ https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json . . Load the dev set using HF data processors . Hugging Face provides the Processors library for facilitating basic processing tasks with some canonical NLP datasets. The processors can be used for loading datasets and converting their examples to features for direct use in the model. We&#39;ll be using the SQuAD processors. . from transformers.data.processors.squad import SquadV2Processor # this processor loads the SQuAD2.0 dev set examples processor = SquadV2Processor() examples = processor.get_dev_examples(&quot;./data/squad/&quot;, filename=&quot;dev-v2.0.json&quot;) print(len(examples)) . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:05&lt;00:00, 6.71it/s] . 11873 . . While examples is a list, most other tasks we&#39;ll work with use a unique identifier - one for each question in the dev set. . # generate some maps to help us identify examples of interest qid_to_example_index = {example.qas_id: i for i, example in enumerate(examples)} qid_to_has_answer = {example.qas_id: bool(example.answers) for example in examples} answer_qids = [qas_id for qas_id, has_answer in qid_to_has_answer.items() if has_answer] no_answer_qids = [qas_id for qas_id, has_answer in qid_to_has_answer.items() if not has_answer] . def display_example(qid): from pprint import pprint idx = qid_to_example_index[qid] q = examples[idx].question_text c = examples[idx].context_text a = [answer[&#39;text&#39;] for answer in examples[idx].answers] print(f&#39;Example {idx} of {len(examples)} n&#39;) print(f&quot;Q: {q} n&quot;) print(&quot;Context:&quot;) pprint(c) print(f&quot; nTrue Answers: n{a}&quot;) . A positive example . Approximately 50% of the examples in the dev set are questions that have answers contained within their corresponding passage. In these cases, up to five possible correct answers are provided (questions and answers were generated and identified by crowd-sourced workers). Answers must be direct excerpts from the passage, but we can see there are several ways to arrive at a &quot;correct&quot; answer. . display_example(answer_qids[1300]) . Example 2548 of 11873 Q: Where on Earth is free oxygen found? Context: (&#34;Free oxygen also occurs in solution in the world&#39;s water bodies. The &#34; &#39;increased solubility of O n&#39; &#39;2 at lower temperatures (see Physical properties) has important implications &#39; &#39;for ocean life, as polar oceans support a much higher density of life due to &#39; &#39;their higher oxygen content. Water polluted with plant nutrients such as &#39; &#39;nitrates or phosphates may stimulate growth of algae by a process called &#39; &#39;eutrophication and the decay of these organisms and other biomaterials may &#39; &#39;reduce amounts of O n&#39; &#39;2 in eutrophic water bodies. Scientists assess this aspect of water quality &#39; &#34;by measuring the water&#39;s biochemical oxygen demand, or the amount of O n&#34; &#39;2 needed to restore it to a normal concentration.&#39;) True Answers: [&#39;water&#39;, &#34;in solution in the world&#39;s water bodies&#34;, &#34;the world&#39;s water bodies&#34;] . A negative example . The other half of the questions in the dev set do not have an answer in the corresponding passage. These questions were generated by crowd-sourced workers to be related and relevant to the passage, but unanswerable by that passage. There are thus no True Answers associated with these questions, as we see in the example below. . Note: In this case, the question is a trick -- the numbers are reoriented in a way that no longer holds true. Will the model pick up on that? . display_example(no_answer_qids[1254]) . Example 2564 of 11873 Q: What happened 3.7-2 billion years ago? Context: (&#34;Free oxygen gas was almost nonexistent in Earth&#39;s atmosphere before &#34; &#39;photosynthetic archaea and bacteria evolved, probably about 3.5 billion &#39; &#39;years ago. Free oxygen first appeared in significant quantities during the &#39; &#39;Paleoproterozoic eon (between 3.0 and 2.3 billion years ago). For the first &#39; &#39;billion years, any free oxygen produced by these organisms combined with &#39; &#39;dissolved iron in the oceans to form banded iron formations. When such &#39; &#39;oxygen sinks became saturated, free oxygen began to outgas from the oceans &#39; &#39;3‚Äì2.7 billion years ago, reaching 10% of its present level around 1.7 &#39; &#39;billion years ago.&#39;) True Answers: [] . Metrics for QA . There are two dominant metrics used by many question answering datasets, including SQuAD: exact match (EM) and F1 score. These scores are computed on individual question+answer pairs. When multiple correct answers are possible for a given question, the maximum score over all possible correct answers is computed. Overall EM and F1 scores are computed for a model by averaging over the individual example scores. . Exact Match . This metric is as simple as it sounds. For each question+answer pair, if the characters of the model&#39;s prediction exactly match the characters of (one of) the True Answer(s), EM = 1, otherwise EM = 0. This is a strict all-or-nothing metric; being off by a single character results in a score of 0. When assessing against a negative example, if the model predicts any text at all, it automatically receives a 0 for that example. . F1 . F1 score is a common metric for classification problems, and widely used in QA. It is appropriate when we care equally about precision and recall. In this case, it&#39;s computed over the individual words in the prediction against those in the True Answer. The number of shared words between the prediction and the truth is the basis of the F1 score: precision is the ratio of the number of shared words to the total number of words in the prediction, and recall is the ratio of the number of shared words to the total number of words in the ground truth. . . Let&#39;s see how these metrics work in practice. We&#39;ll load up a fine-tuned model (this one, to be precise) and its tokenizer, and compare our predictions against the True Answers. . Load a Transformer model fine-tuned on SQuAD 2.0 . tokenizer = AutoTokenizer.from_pretrained(&quot;twmkn9/distilbert-base-uncased-squad2&quot;) model = AutoModelForQuestionAnswering.from_pretrained(&quot;twmkn9/distilbert-base-uncased-squad2&quot;) . The following get_prediction method is essentially identical to what we used last time in our simple QA system. . def get_prediction(qid): # given a question id (qas_id or qid), load the example, get the model outputs and generate an answer question = examples[qid_to_example_index[qid]].question_text context = examples[qid_to_example_index[qid]].context_text inputs = tokenizer.encode_plus(question, context, return_tensors=&#39;pt&#39;) outputs = model(**inputs) answer_start = torch.argmax(outputs[0]) # get the most likely beginning of answer with the argmax of the score answer_end = torch.argmax(outputs[1]) + 1 answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[&#39;input_ids&#39;][0][answer_start:answer_end])) return answer . Below are some functions we&#39;ll need to compute our quality metrics. . # these functions are heavily influenced by the HF squad_metrics.py script def normalize_text(s): &quot;&quot;&quot;Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.&quot;&quot;&quot; import string, re def remove_articles(text): regex = re.compile(r&quot; b(a|an|the) b&quot;, re.UNICODE) return re.sub(regex, &quot; &quot;, text) def white_space_fix(text): return &quot; &quot;.join(text.split()) def remove_punc(text): exclude = set(string.punctuation) return &quot;&quot;.join(ch for ch in text if ch not in exclude) def lower(text): return text.lower() return white_space_fix(remove_articles(remove_punc(lower(s)))) def compute_exact_match(prediction, truth): return int(normalize_text(prediction) == normalize_text(truth)) def compute_f1(prediction, truth): pred_tokens = normalize_text(prediction).split() truth_tokens = normalize_text(truth).split() # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise if len(pred_tokens) == 0 or len(truth_tokens) == 0: return int(pred_tokens == truth_tokens) common_tokens = set(pred_tokens) &amp; set(truth_tokens) # if there are no common tokens then f1 = 0 if len(common_tokens) == 0: return 0 prec = len(common_tokens) / len(pred_tokens) rec = len(common_tokens) / len(truth_tokens) return 2 * (prec * rec) / (prec + rec) def get_gold_answers(example): &quot;&quot;&quot;helper function that retrieves all possible true answers from a squad2.0 example&quot;&quot;&quot; gold_answers = [answer[&quot;text&quot;] for answer in example.answers if answer[&quot;text&quot;]] # if gold_answers doesn&#39;t exist it&#39;s because this is a negative example - # the only correct answer is an empty string if not gold_answers: gold_answers = [&quot;&quot;] return gold_answers . In the following cell, we start by computing EM and F1 for our first example - the one that has several True Answers associated with it. . prediction = get_prediction(answer_qids[1300]) example = examples[qid_to_example_index[answer_qids[1300]]] gold_answers = get_gold_answers(example) em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers) f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers) print(f&quot;Question: {example.question_text}&quot;) print(f&quot;Prediction: {prediction}&quot;) print(f&quot;True Answers: {gold_answers}&quot;) print(f&quot;EM: {em_score} t F1: {f1_score}&quot;) . Question: Where on Earth is free oxygen found? Prediction: water bodies True Answers: [&#39;water&#39;, &#34;in solution in the world&#39;s water bodies&#34;, &#34;the world&#39;s water bodies&#34;] EM: 0 F1: 0.8 . We see that our prediction is actually quite close to some of the True Answers, resulting in a respectable F1 score. However, it does not exactly match any of them, so our EM score is 0. . Let&#39;s try with our negative example now. . prediction = get_prediction(no_answer_qids[1254]) example = examples[qid_to_example_index[no_answer_qids[1254]]] gold_answers = get_gold_answers(example) em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers) f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers) print(f&quot;Question: {example.question_text}&quot;) print(f&quot;Prediction: {prediction}&quot;) print(f&quot;True Answers: {gold_answers}&quot;) print(f&quot;EM: {em_score} t F1: {f1_score}&quot;) . Question: What happened 3.7-2 billion years ago? Prediction: [CLS] what happened 3 . 7 - 2 billion years ago ? [SEP] free oxygen gas was almost nonexistent in earth &#39; s atmosphere before photosynthetic archaea and bacteria evolved , probably about 3 . 5 billion years ago . free oxygen first appeared in significant quantities during the paleoproterozoic eon ( between 3 . 0 and 2 . 3 billion years ago ) . for the first billion years , any free oxygen produced by these organisms combined with dissolved iron in the oceans to form banded iron formations . when such oxygen sinks became saturated , free oxygen began to outgas from the oceans True Answers: [&#39;&#39;] EM: 0 F1: 0 . Wow. Both our metrics are zero, because this model does not correctly assess that this question is unanswerable! Even worse, it seems to have catastrophically failed, including the entire question as part of the answer. In a later section, we&#39;ll explicitly dig into why this happens, but for now, it&#39;s important to note that we got this answer because we simply extracted start and end tokens associated with the maximum score (we took an argmax of the model output in get_prediction) and this lead to some unintended consequences. . Now that we‚Äôve seen the basics of computing QA metrics on a couple of examples, we need to assess the model on the entire dev set. Luckily, there&#39;s a script for that. . Evaluating a model on the SQuAD2.0 dev set with HF . The same run_squad.py script we used to fine-tune a Transformer for question answering can also be used to evaluate the model. (You can grab the script here or run the hidden cell below.) . # collapse-hide # Grab the run_squad.py script !curl -L -O https://raw.githubusercontent.com/huggingface/transformers/master/examples/question-answering/run_squad.py . . Below are the arguments needed to properly evaluate a fine-tuned model for question answering on the SQuAD dev set. Because we&#39;re using SQuAD2.0, it is crucial to include the --version_2_with_negative flag! . !python run_squad.py --model_type distilbert --model_name_or_path twmkn9/distilbert-base-uncased-squad2 --output_dir models/distilbert/twmkn9_distilbert-base-uncased-squad2 --data_dir data/squad --predict_file dev-v2.0.json --do_eval --version_2_with_negative --do_lower_case --per_gpu_eval_batch_size 12 --max_seq_length 384 --doc_stride 128 . Refer to our last post for more details on what these arguments mean and what this script does. For our immediate purposes, running the cell above will produce the following output in the --output_dir directory: . predictions_.json | nbest_predictions_.json | null_odds_.json | . (We&#39;ll go over what these are later on.) Additionally, an overall Results dict will be displayed to the screen. If you run the above cell, the last line of output should display something like the following: . Results = { # a) scores averaged over all examples in the dev set &#39;exact&#39;: 66.25958056093658, &#39;f1&#39;: 69.66994428499025, &#39;total&#39;: 11873, # number of examples in the dev set # b) scores averaged over only positive examples (have answers) &#39;HasAns_exact&#39;: 68.91025641025641, &#39;HasAns_f1&#39;: 75.74076391627662, &#39;HasAns_total&#39;: 5928, # number of positive examples # c) scores averaged over only negative examples (no answers) &#39;NoAns_exact&#39;: 63.61648444070648, &#39;NoAns_f1&#39;: 63.61648444070648, &#39;NoAns_total&#39;: 5945, # number of negative examples # d) given probabilities of no-answer for each example, what would the best scores and thresholds be? &#39;best_exact&#39;: 66.25958056093658, &#39;best_exact_thresh&#39;: 0.0, &#39;best_f1&#39;: 69.66994428499046, &#39;best_f1_thresh&#39;: 0.0 } . The first three blocks of the Results output are pretty straightforward. EM and F1 scores are reported over a) the full dev set, b) the set of positive examples, and c) the set of negative examples. This can provide some insight into whether a model is performing adequately on both answer and no-answer questions. (This particular model is pretty bad at no-answer questions). . However, what&#39;s going on with the last block? This portion of the output is not useful unless we supply the evaluation method with additional information. For that, we&#39;ll need to dig deeper into the evaluation process - because it turns out that we need to compute more than just a prediction for an answer; we must also compute a prediction for NO answer and we must score both predictions! . The following section will dive into the technical details of computing robust predictions on SQuAD2.0 examples, including how to score an answer and the null answer, as well as how to determine which one should be the &quot;correct&quot; prediction for a given example. Feel free to skip to the next section for the punchline. (For those of you considering building your own QA system, we found this information to be invaluable for understanding the inner workings of prediction and assessment.) . [Optional] Computing predictions . . Note: The code in the following section is an under-the-hood dive into the HF compute_predictions_logits method in their squad_metrics.py script. . When the tokenized question+context is passed to the model, the output consists of two sets of logits: one for the start of the answer span, the other for the end of the answer span. These logits represent the likelihood of any given token being the start or end of the answer. Every token passed to the model is assigned a logit, including special tokens (e.g., [CLS], [SEP]), and tokens corresponding to the question itself. . Let&#39;s walk through the process using our last example (Q: What happened 3.7-2 billion years ago?). . inputs = tokenizer.encode_plus(example.question_text, example.context_text, return_tensors=&#39;pt&#39;) start_logits, end_logits = model(**inputs) . # look at how large the logit is in the [CLS] position (index 0)! # strong possibility that this question has no answer... but our prediction returned an answer anyway! start_logits . tensor([[ 6.4914, -9.1416, -8.4068, -7.5684, -9.9081, -9.4256, -10.1625, -9.2579, -10.0554, -9.9653, -9.2002, -8.8657, -9.1162, 0.6481, -2.5947, -4.5072, -8.1189, -6.5871, -5.8973, -10.8619, -11.0953, -10.2294, -9.3660, -7.6017, -10.8009, -10.8197, -6.1258, -8.3507, -4.2463, -10.0987, -10.2659, -8.8490, -6.7346, -8.6513, -9.7573, -5.7496, -5.5851, -8.9483, -7.0652, -6.1369, -5.7810, -9.4366, -8.7670, -9.6743, -9.7446, -7.7905, -7.4541, -1.5963, -3.8540, -7.3450, -8.1854, -9.5566, -8.3416, -8.9553, -8.3144, -6.4132, -4.2285, -9.4427, -9.5111, -9.2931, -8.9154, -9.3930, -8.2111, -8.9774, -9.0274, -7.2652, -7.4511, -9.8597, -9.5869, -9.9735, -7.0526, -9.7560, -8.7788, -9.5117, -9.6391, -8.6487, -9.5994, -7.8213, -5.1754, -4.3561, -4.3913, -7.8499, -7.7522, -8.9651, -3.5229, -0.8312, -2.7668, -7.9180, -10.0320, -8.7797, -4.5965, -5.9465, -9.9442, -3.2135, -5.0734, -8.3462, -7.5366, -3.7073, -7.0968, -4.3325, -1.3691, -4.1477, -5.3794, -7.6138, 1.3183, -3.4190, 3.1457, -3.0152, -0.4102, -2.4606, -3.5971, 6.4519, -0.5654, 0.9829, -1.6682, 3.3549, -4.7847, -2.8024, -3.3160, -0.5868, -0.9617, -8.1925, -4.3299, -7.3923, -5.0875, -5.3880, -5.3676, -3.0878, -4.3427, 4.3975, 1.8860, -5.4661, -9.1565, -3.6369, -3.5462, -4.1448, -2.0250, -2.4492, -8.7015, -7.3292, -7.7616, -7.0786, -4.6668, -4.4089, -9.1182]], grad_fn=&lt;SqueezeBackward1&gt;) . In our simple QA system, we predicted the best answer by selecting the start and end tokens with the largest logits, but that&#39;s not very robust. In fact, the original BERT paper suggested considering any sensible start+end combination as a possible answer to the question. These combinations would then be scored, and the one with the highest score would be considered the best answer. A possible (candidate) answer is scored as the sum of its start and end logits. . Note: This reflects how a basic span extraction classifier works. The raw hidden layer from the model is passed through a Linear layer and then fed to a CrossEntropyLoss for each class. In span extraction, there are two classes: the beginning of the span and the end of the span. The span loss is computed as the sum of the CrossEntropyLoss for the start and end positions. The probability of an answer span is the probability of a given start token S and an end token E: P(S and E) = P(S)P(E), because the start and end tokens are treated as being independent. Thus summing the start and end logits is equivalent to a product of their softmax probabilities. . To mimic this behavior, we&#39;ll start by taking the n largest start_logits and the n largest end_logits as candidates. Any sensible combination of these start + end tokens is considered a candidate answer; however, several consistency checks must first be performed. For example, an answer wherein the end token falls before the start token should be excluded, because that just doesn&#39;t make sense. Candidate answers wherein the start or end tokens are associated with question tokens are also excluded, because the answer to the question should obviously not be in the question itself! It is important to note that the [CLS] token and its corresponding logits are not removed, because this token indicates the null answer. . def to_list(tensor): return tensor.detach().cpu().tolist() # convert our start and end logit tensors to lists start_logits = to_list(start_logits)[0] end_logits = to_list(end_logits)[0] # sort our start and end logits from largest to smallest, keeping track of the index start_idx_and_logit = sorted(enumerate(start_logits), key=lambda x: x[1], reverse=True) end_idx_and_logit = sorted(enumerate(end_logits), key=lambda x: x[1], reverse=True) # select the top n (in this case, 5) print(start_idx_and_logit[:5]) print(end_idx_and_logit[:5]) . [(0, 6.491387367248535), (111, 6.451895713806152), (129, 4.397505760192871), (115, 3.354909658432007), (106, 3.1457457542419434)] [(119, 6.33292293548584), (0, 6.084450721740723), (135, 4.417276382446289), (116, 4.3764214515686035), (112, 4.125303268432617)] . The null answer token (index 0) is in the top five of both the start and end logit lists. . In order to eventually predict a text answer (or empty string), we need to keep track of the indexes which will be used to pull the corresponding token ids later on. We&#39;ll also need to identify which indexes correspond to the question tokens, so we can ensure we don&#39;t allow a nonsensical prediction. . start_indexes = [idx for idx, logit in start_idx_and_logit[:5]] end_indexes = [idx for idx, logit in end_idx_and_logit[:5]] # convert the token ids from a tensor to a list tokens = to_list(inputs[&#39;input_ids&#39;])[0] # question tokens are defined as those between the CLS token (101, at position 0) and first SEP (102) token question_indexes = [i+1 for i, token in enumerate(tokens[1:tokens.index(102)])] question_indexes . [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] . Next, we&#39;ll generate a list of candidate predictions by looping through all combinations of the start and end token indexes, excluding nonsensical combinations. We&#39;ll save these to a list for the next step. . import collections # keep track of all preliminary predictions PrelimPrediction = collections.namedtuple( &quot;PrelimPrediction&quot;, [&quot;start_index&quot;, &quot;end_index&quot;, &quot;start_logit&quot;, &quot;end_logit&quot;] ) prelim_preds = [] for start_index in start_indexes: for end_index in end_indexes: # throw out invalid predictions if start_index in question_indexes: continue if end_index in question_indexes: continue if end_index &lt; start_index: continue prelim_preds.append( PrelimPrediction( start_index = start_index, end_index = end_index, start_logit = start_logits[start_index], end_logit = end_logits[end_index] ) ) . With a list of sensible candidate predictions, it&#39;s time to score them. . For a candidate answer, score = start_logit + end_logit. Below, we sort our candidate predictions by their score. . # sort preliminary predictions by their score prelim_preds = sorted(prelim_preds, key=lambda x: (x.start_logit + x.end_logit), reverse=True) pprint(prelim_preds[:5]) . [PrelimPrediction(start_index=0, end_index=119, start_logit=6.491387367248535, end_logit=6.33292293548584), PrelimPrediction(start_index=111, end_index=119, start_logit=6.451895713806152, end_logit=6.33292293548584), PrelimPrediction(start_index=0, end_index=0, start_logit=6.491387367248535, end_logit=6.084450721740723), PrelimPrediction(start_index=0, end_index=135, start_logit=6.491387367248535, end_logit=4.417276382446289), PrelimPrediction(start_index=111, end_index=135, start_logit=6.451895713806152, end_logit=4.417276382446289)] . Next we need to convert our preliminary predictions into actual text (or the empty string, if null). We&#39;ll keep track of text predictions we&#39;ve seen, because different token combinations can result in the same text prediction and we only want to keep the one with the highest score (we&#39;re looping in descending score order). Finally, we&#39;ll trim this list down to the best 5 predictions. . # keep track of all best predictions BestPrediction = collections.namedtuple( # pylint: disable=invalid-name &quot;BestPrediction&quot;, [&quot;text&quot;, &quot;start_logit&quot;, &quot;end_logit&quot;] ) nbest = [] seen_predictions = [] for pred in prelim_preds: # for now we only care about the top 5 best predictions if len(nbest) &gt;= 5: break # loop through predictions according to their start index if pred.start_index &gt; 0: # non-null answers have start_index &gt; 0 text = tokenizer.convert_tokens_to_string( tokenizer.convert_ids_to_tokens( tokens[pred.start_index:pred.end_index+1] ) ) # clean whitespace text = text.strip() text = &quot; &quot;.join(text.split()) if text in seen_predictions: continue # flag this text as being seen -- if we see it again, don&#39;t add it to the nbest list seen_predictions.append(text) # add this text prediction to a pruned list of the top 5 best predictions nbest.append(BestPrediction(text=text, start_logit=pred.start_logit, end_logit=pred.end_logit)) # and don&#39;t forget -- include the null answer! nbest.append(BestPrediction(text=&quot;&quot;, start_logit=start_logits[0], end_logit=end_logits[0])) . The null answer is scored as the sum of the start_logit and end_logit associated with the [CLS] token. . At this point, we have a neat list of the top 5 best predictions for this question. The number of best predictions for each example is adjustable with the --n_best_size argument of the run_squad.py script. The nbest predictions for every question in the dev set are saved to disk under nbest_predictions_.json in --output_dir. (This is a great resource for digging into how a model is behaving.) Let&#39;s take a look at our nbest predictions. . pprint(nbest) . [BestPrediction(text=&#39;free oxygen began to outgas from the oceans&#39;, start_logit=6.451895713806152, end_logit=6.33292293548584), BestPrediction(text=&#39;free oxygen began to outgas from the oceans 3 ‚Äì 2 . 7 billion years ago , reaching 10 % of its present level&#39;, start_logit=6.451895713806152, end_logit=4.417276382446289), BestPrediction(text=&#39;free oxygen began to outgas&#39;, start_logit=6.451895713806152, end_logit=4.3764214515686035), BestPrediction(text=&#39;free oxygen&#39;, start_logit=6.451895713806152, end_logit=4.125303268432617), BestPrediction(text=&#39;outgas from the oceans&#39;, start_logit=3.354909658432007, end_logit=6.33292293548584), BestPrediction(text=&#39;&#39;, start_logit=6.491387367248535, end_logit=6.084450721740723)] . Our top prediction so far is &quot;free oxygen began to outgas from the oceans,&quot; which is already a far cry better than what we originally predicted. This is because we have successfully excluded nonsensical predictions that would incorporate question tokens as part of the answer. However, we know it&#39;s still incorrect. Let&#39;s keep going. . The last step is to compute the null score -- more specifically, the difference between the null score and the best non-null score as shown below. . # compute the null score as the sum of the [CLS] token logits score_null = start_logits[0] + end_logits[0] # compute the difference between the null score and the best non-null score score_diff = score_null - nbest[0].start_logit - nbest[0].end_logit score_diff . -0.20898056030273438 . This score_diff is computed for every example in the dev set and these scores are saved to disk in the null_odds_.json. Let&#39;s pull up the score stored for the example we&#39;re using and see how we did! . filename = model_dir + &#39;null_odds_.json&#39; null_odds = json.load(open(filename, &#39;rb&#39;)) null_odds[example.qas_id] . -0.2090005874633789 . We basically nailed it! (The full HF version contains a few more checks and some additional subtleties that could account for the slight differences in our score_diff.) . Using the null threshold . In the previous section we covered: . how to generate more robust predictions (e.g., by excluding predictions that include question tokens in the answer), | how to score a prediction as the sum of its start and end logits, | and how to compute the score difference between the null prediction and the best text prediction. | . The run_squad.py script performs all of these tasks for us and saves the score differences for every example in the null_odds_.json. With that, we can now start to make sense of the fourth block of the results output! . According to the original BERT paper, . We predict a non-null answer when sÀÜi,j &gt; s_null + œÑ , where the threshold œÑ is selected on the dev set to maximize F1. . In other words, the authors are saying that one should predict a null answer for a given example if that example&#39;s score difference is above a certain threshold. What should that threshold be? How should we compute it? They give us a recipe:select the threshold that maximizes F1. Rather than rerunning run_squad.py, we can import the aptly-named method that computes SQuAD evaluation: squad_evaluate. (You can take a look at the code for yourself here.) To use squad_evaluate we&#39;ll need: . the original examples (because that&#39;s where the True Answers are stored), | predictions_.json, | null_odds_.json, | and a null threshold. | . # load the predictions we generated earlier filename = model_dir + &#39;predictions_.json&#39; preds = json.load(open(filename, &#39;rb&#39;)) # load the null score differences we generated earlier filename = model_dir + &#39;null_odds_.json&#39; null_odds = json.load(open(filename, &#39;rb&#39;)) . Let&#39;s re-evaluate our model on SQuAD2.0 using the squad_evaluate method. This method uses the score differences for each example in the dev set to determine thresholds that maximize either the EM score or the F1 score. It then recomputes the best possible EM score and F1 score associated with that null threshold. . from transformers.data.metrics.squad_metrics import squad_evaluate # the default threshold is set to 1.0 -- we&#39;ll leave it there for now results_default_thresh = squad_evaluate(examples, preds, no_answer_probs=null_odds, no_answer_probability_threshold=1.0) pprint(results_default_thresh) . OrderedDict([(&#39;exact&#39;, 66.25958056093658), (&#39;f1&#39;, 69.66994428499025), (&#39;total&#39;, 11873), (&#39;HasAns_exact&#39;, 68.91025641025641), (&#39;HasAns_f1&#39;, 75.74076391627662), (&#39;HasAns_total&#39;, 5928), (&#39;NoAns_exact&#39;, 63.61648444070648), (&#39;NoAns_f1&#39;, 63.61648444070648), (&#39;NoAns_total&#39;, 5945), (&#39;best_exact&#39;, 68.36519834919565), (&#39;best_exact_thresh&#39;, -4.189256191253662), (&#39;best_f1&#39;, 71.1144383018176), (&#39;best_f1_thresh&#39;, -3.767639636993408)]) . The first three blocks have identical values as in our initial evaluation because they are based on the default threshold (which is currently 1.0). However, the values in the fourth block have been updated by taking into account the null_odds information. When a given example&#39;s score_diff is greater than the threshold, the prediction is flipped to a null answer which affects the overall EM and F1 scores. . Let&#39;s use the best_f1_thresh and run the evaluation once more to see a breakdown of our model&#39;s performance on HasAns and NoAns examples: . best_f1_thresh = -3.7676548957824707 results_f1_thresh = squad_evaluate(examples, preds, no_answer_probs=null_odds, no_answer_probability_threshold=best_f1_thresh) pprint(results_f1_thresh) . OrderedDict([(&#39;exact&#39;, 68.31466352227744), (&#39;f1&#39;, 71.11106931335648), (&#39;total&#39;, 11873), (&#39;HasAns_exact&#39;, 61.53846153846154), (&#39;HasAns_f1&#39;, 67.13929250294865), (&#39;HasAns_total&#39;, 5928), (&#39;NoAns_exact&#39;, 75.07148864592094), (&#39;NoAns_f1&#39;, 75.07148864592094), (&#39;NoAns_total&#39;, 5945), (&#39;best_exact&#39;, 68.36519834919565), (&#39;best_exact_thresh&#39;, -4.189256191253662), (&#39;best_f1&#39;, 71.1144383018176), (&#39;best_f1_thresh&#39;, -3.767639636993408)]) . When we used the default threshold of 1.0, we saw that our NoAns_f1 score was a mere 63.6, but when we use the best_f1_thresh, we now get a NoAns_f1 score of 75 - nearly a 12 point jump! The downside is that we lose some ground in how well our model correctly predicts HasAns examples. Overall, however, we see a net increase of a couple points in both EM and F1 scores. This demonstrates that computing null scores and properly using a null threshold significantly increases QA performance on the SQuAD2.0 dev set with almost no additional work. . Putting it all together . Below we present a new method that will select more robust predictions, compute scores for the best text predictions (as well as for the null prediction), and use these scores along with a null threshold to determine whether the question should be answered. As a bonus, this method also computes and returns the probability of the answer, which is often easier to interpret than a logit score. Prediction probabilities depend on nbest, since they are computed with a softmax over the number of most likely predictions. . def get_robust_prediction(example, tokenizer, nbest=10, null_threshold=1.0): inputs = get_qa_inputs(example, tokenizer) start_logits, end_logits = model(**inputs) # get sensible preliminary predictions, sorted by score prelim_preds = preliminary_predictions(start_logits, end_logits, inputs[&#39;input_ids&#39;], nbest) # narrow that down to the top nbest predictions nbest_preds = best_predictions(prelim_preds, nbest, tokenizer) # compute the probability of each prediction - nice but not necessary probabilities = prediction_probabilities(nbest_preds) # compute score difference score_difference = compute_score_difference(nbest_preds) # if score difference &gt; threshold, return the null answer if score_difference &gt; null_threshold: return &quot;&quot;, probabilities[-1] else: return nbest_preds[0].text, probabilities[0] . # collapse-hide # -- Helper functions for get_robust_prediction -- # def get_qa_inputs(example, tokenizer): # load the example, convert to inputs, get model outputs question = example.question_text context = example.context_text return tokenizer.encode_plus(question, context, return_tensors=&#39;pt&#39;) def get_clean_text(tokens, tokenizer): text = tokenizer.convert_tokens_to_string( tokenizer.convert_ids_to_tokens(tokens) ) # Clean whitespace text = text.strip() text = &quot; &quot;.join(text.split()) return text def prediction_probabilities(predictions): def softmax(x): &quot;&quot;&quot;Compute softmax values for each sets of scores in x.&quot;&quot;&quot; e_x = np.exp(x - np.max(x)) return e_x / e_x.sum() all_scores = [pred.start_logit+pred.end_logit for pred in predictions] return softmax(np.array(all_scores)) def preliminary_predictions(start_logits, end_logits, input_ids, nbest): # convert tensors to lists start_logits = to_list(start_logits)[0] end_logits = to_list(end_logits)[0] tokens = to_list(input_ids)[0] # sort our start and end logits from largest to smallest, keeping track of the index start_idx_and_logit = sorted(enumerate(start_logits), key=lambda x: x[1], reverse=True) end_idx_and_logit = sorted(enumerate(end_logits), key=lambda x: x[1], reverse=True) start_indexes = [idx for idx, logit in start_idx_and_logit[:nbest]] end_indexes = [idx for idx, logit in end_idx_and_logit[:nbest]] # question tokens are between the CLS token (101, at position 0) and first SEP (102) token question_indexes = [i+1 for i, token in enumerate(tokens[1:tokens.index(102)])] # keep track of all preliminary predictions PrelimPrediction = collections.namedtuple( # pylint: disable=invalid-name &quot;PrelimPrediction&quot;, [&quot;start_index&quot;, &quot;end_index&quot;, &quot;start_logit&quot;, &quot;end_logit&quot;] ) prelim_preds = [] for start_index in start_indexes: for end_index in end_indexes: # throw out invalid predictions if start_index in question_indexes: continue if end_index in question_indexes: continue if end_index &lt; start_index: continue prelim_preds.append( PrelimPrediction( start_index = start_index, end_index = end_index, start_logit = start_logits[start_index], end_logit = end_logits[end_index] ) ) # sort prelim_preds in descending score order prelim_preds = sorted(prelim_preds, key=lambda x: (x.start_logit + x.end_logit), reverse=True) return prelim_preds def best_predictions(prelim_preds, nbest, tokenizer): # keep track of all best predictions # This will be the pool from which answer probabilities are computed BestPrediction = collections.namedtuple( &quot;BestPrediction&quot;, [&quot;text&quot;, &quot;start_logit&quot;, &quot;end_logit&quot;] ) nbest_predictions = [] seen_predictions = [] for pred in prelim_preds: if len(nbest_predictions) &gt;= nbest: break if pred.start_index &gt; 0: # non-null answers have start_index &gt; 0 toks = tokens[pred.start_index : pred.end_index+1] text = get_clean_text(toks, tokenizer) # if this text has been seen already - skip it if text in seen_predictions: continue # flag text as being seen seen_predictions.append(text) # add this text to a pruned list of the top nbest predictions nbest_predictions.append( BestPrediction( text=text, start_logit=pred.start_logit, end_logit=pred.end_logit ) ) # Add the null prediction nbest_predictions.append( BestPrediction( text=&quot;&quot;, start_logit=start_logits[0], end_logit=end_logits[0] ) ) return nbest_predictions def compute_score_difference(predictions): &quot;&quot;&quot; Assumes that the null answer is always the last prediction &quot;&quot;&quot; score_null = predictions[-1].start_logit + predictions[-1].end_logit score_non_null = predictions[0].start_logit + predictions[0].end_logit return score_null - score_non_null . . Will we now get the right answer (an empty string) for that tricky no-answer example we were working with? . print(example.question_text) get_robust_prediction(example, tokenizer, nbest=10, null_threshold=best_f1_thresh) . What happened 3.7-2 billion years ago? . (&#39;&#39;, 0.34412444013709165) . Woohoo!! We got the right answer this time!! . Even if we didn&#39;t have the best threshold in place, our additional checks still allow us to output more sensible looking answers, rejecting predictions that include the question tokens. . print(example.question_text) get_robust_prediction(example, tokenizer, nbest=10, null_threshold=1.0) . What happened 3.7-2 billion years ago? . (&#39;free oxygen began to outgas from the oceans&#39;, 0.42410620054269993) . And if it hadn&#39;t been a trick question, this would be the correct answer! (Seems like distilBERT could use some improvement in number understanding.) . Final Thoughts . Using a robust prediction method like the above will do more than allow a model to perform better on a curated dev set, though this is an important first step. It will also provide the model with a slightly better ability to refrain from answering questions that simply don&#39;t have an answer in the associated passage. This is a crucial feature for QA models, because it&#39;s not enough to get an answer if that answer doesn&#39;t make sense. We want our models to tell us something useful -- and sometimes that means telling us nothing at all. .",
            "url": "https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html",
            "relUrl": "/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html",
            "date": " ‚Ä¢ Jun 9, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Building a QA System with BERT on Wikipedia",
            "content": ". So you&#39;ve decided to build a QA system . You want to start with something simple and general, so you plan to make it open domain, using Wikipedia as a corpus for answering questions. You want to use the best NLP that your compute resources allow (you&#39;re lucky enough to have access to a GPU), so you&#39;re going to focus on the big, flashy Transformer models that are all the rage these days. . Sounds like you&#39;re building an IR-based QA system. In our previous post (Intro to Automated Question Answering), we covered the general design of these systems, which typically require two main components: the document retriever (a search engine) that selects the n most relevant documents from a large collection, and a document reader that processes these candidate documents in search of an explicit answer span. . . Now we&#39;re going to build it! . This post is chock full of code that walks through our approach. We&#39;ll also highlight and clarify some powerful resources (including off-the-shelf models and libraries) that you can use to quickly get going on a QA system of your own. We&#39;ll cover all the necessary steps including: . installing libraries and setting up an environment, | training a Transformer style model on the SQuAD dataset, | understanding Hugging Face&#39;s run_squad.py training script and output, | and passing a full Wikipedia article as context for a question. | . By the end of this post we&#39;ll have a working IR-based QA system, with BERT as the document reader and Wikipedia&#39;s search engine as the document retriever - a fun toy model that hints at potential real-world use cases. . This article was originally developed in a Jupyter Notebook and, thanks to fastpages, converted to a blog post. For an interactive environment, click the &quot;Open in Colab&quot; button above (though we note that, due to Colab&#39;s system constraints, some of the cells in this notebook might not be fully executable. We&#39;ll highlight when this is the case, but don&#39;t worry -- you&#39;ll still be able to play around with all the fun stuff.) . Let&#39;s get started! . Setting up your virtual environment . A virtual environment is always best practice and we&#39;re using venv on our workhorse machine. For this project, we&#39;ll be using PyTorch, which handles the heavy lifting of deep differentiable learning. If you have a GPU you&#39;ll want a PyTorch build that includes CUDA support, though most cells in this notebook will work fine without one. Check out PyTorch&#39;s quick install guide to determine the best build for your GPU and OS. We&#39;ll also be using the Transformers library, which provides easy-to-use implementations of all the popular Transformer architectures, like BERT. Finally, we&#39;ll need the wikipedia library for easy access and parsing of Wikipedia pages. . You can recreate our env (with CUDA 9.2 support -- but use the appropriate version for your machine) with the following commands in your command line: . $ python3 -m venv myenv $ source myenv/bin/activate $ pip install torch==1.5.0+cu92 torchvision==0.6.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html $ pip install transformers==2.5.1 $ pip install wikipedia==1.4.0 . Note: Our GPU machine sports an older version of CUDA (9.2 -- we&#39;re getting around to updating that), so we need to use an older version of PyTorch for the necessary CUDA support. The training script we&#39;ll be using requires some specific packages. More recent versions of PyTorch include these packages; however, older versions do not. If you have to work with an older version of PyTorch, you might need to install TensorboardX (see the hidden code cell below). . # collapse-hide # line 69 of `run_squad.py` script shows why you might need to install # tensorboardX if you have an older version of torch try: from torch.utils.tensorboard import SummaryWriter except ImportError: from tensorboardX import SummaryWriter . . Conversely, if you&#39;re working in Colab, you can run the cell below. . !pip install torch torchvision -f https://download.pytorch.org/whl/torch_stable.html !pip install transformers !pip install wikipedia . Hugging Face Transformers . The Hugging Face Transformers package provides state-of-the-art general-purpose architectures for natural language understanding and natural language generation. They host dozens of pre-trained models operating in over 100 languages that you can use right out of the box. All of these models come with deep interoperability between PyTorch and Tensorflow 2.0, which means you can move a model from TF2.0 to PyTorch and back again with just a line or two of code! . If you&#39;re new to Hugging Face, we strongly recommend working through the HF Quickstart guide as well as their excellent Transformer Notebooks (we did!), as we won&#39;t cover that material in this notebook. We&#39;ll be using AutoClasses, which serve as a wrapper around pretty much any of the base Transformer classes. . Fine-tuning a Transformer model for Question Answering . To train a Transformer for QA with Hugging Face, we&#39;ll need . to pick a specific model architecture, | a QA dataset, and | the training script. | With these three things in hand we&#39;ll then walk through the fine-tuning process. . 1. Pick a Model . Not every Transformer architecture lends itself naturally to the task of question answering. For example, GPT does not do QA; similarly BERT does not do machine translation. HF identifies the following model types for the QA task: . BERT | distilBERT | ALBERT | RoBERTa | XLNet | XLM | FlauBERT | . We&#39;ll stick with the now-classic BERT model in this notebook, but feel free to try out some others (we will - and we&#39;ll let you know when we do). Next up: a training set. . 2. QA dataset: SQuAD . One of the most canonical datasets for QA is the Stanford Question Answering Dataset, or SQuAD, which comes in two flavors: SQuAD 1.1 and SQuAD 2.0. These reading comprehension datasets consist of questions posed on a set of Wikipedia articles, where the answer to every question is a segment (or span) of the corresponding passage. In SQuAD 1.1, all questions have an answer in the corresponding passage. SQuAD 2.0 steps up the difficulty by including questions that cannot be answered by the provided passage. . The following code will download the specified version of SQuAD. . # set path with magic %env DATA_DIR=./data/squad # download the data def download_squad(version=1): if version == 1: !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json else: !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json download_squad(version=2) . env: DATA_DIR=./data/squad --2020-05-11 21:36:52-- https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.109.153, 185.199.108.153, 185.199.111.153, ... Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.109.153|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 42123633 (40M) [application/json] Saving to: ‚Äò./data/squad/train-v2.0.json‚Äô train-v2.0.json 100%[===================&gt;] 40.17M 14.6MB/s in 2.8s 2020-05-11 21:36:55 (14.6 MB/s) - ‚Äò./data/squad/train-v2.0.json‚Äô saved [42123633/42123633] --2020-05-11 21:36:56-- https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.111.153, 185.199.108.153, ... Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 4370528 (4.2M) [application/json] Saving to: ‚Äò./data/squad/dev-v2.0.json‚Äô dev-v2.0.json 100%[===================&gt;] 4.17M 6.68MB/s in 0.6s 2020-05-11 21:36:56 (6.68 MB/s) - ‚Äò./data/squad/dev-v2.0.json‚Äô saved [4370528/4370528] . 3. Fine-tuning script . We&#39;ve chosen a model and we&#39;ve got some data. Time to train! . All the standard models that HF supports have been pre-trained, which means they&#39;ve all been fed massive unsupervised training sets in order to learn basic language modeling. In order to perform well at specific tasks (like question answering), they must be trained further -- fine-tuned -- on specific datasets and tasks. . HF helpfully provides a script that fine-tunes a Transformer model on one of the SQuAD datasets, called run_squad.py. You can grab the script here or run the cell below. . # download the run_squad.py training script !curl -L -O https://raw.githubusercontent.com/huggingface/transformers/master/examples/question-answering/run_squad.py . This script takes care of all the hard work that goes into fine-tuning a model and, as such, it&#39;s pretty complicated. It hosts no fewer than 45 arguments, providing an impressive amount of flexibility and utility for those who do a lot of training. We&#39;ll leave the details of this script for another day, and focus instead on the basic command to fine-tune BERT on SQuAD 1.1 or 2.0. . Below are the most important arguments for the run_squad.py fine-tuning script. . # fine-tuning your own model for QA using HF&#39;s `run_squad.py` # turn flags on and off according to the model you&#39;re training cmd = [ &#39;python&#39;, # &#39;-m torch.distributed.launch --nproc_per_node 2&#39;, # use this to perform distributed training over multiple GPUs &#39;run_squad.py&#39;, &#39;--model_type&#39;, &#39;bert&#39;, # model type (one of the list under &quot;Pick a Model&quot; above) &#39;--model_name_or_path&#39;, &#39;bert-base-uncased&#39;, # specific model name of the given model type (shown, a list is here: https://huggingface.co/transformers/pretrained_models.html) # on first execution this initiates a download of pre-trained model weights; # can also be a local path to a directory with model weights &#39;--output_dir&#39;, &#39;./models/bert/bbu_squad2&#39;, # directory for model checkpoints and predictions # &#39;--overwrite_output_dir&#39;, # use when adding output to a directory that is non-empty -- # for instance, when training crashes midway through and you need to restart it &#39;--do_train&#39;, # execute the training method &#39;--train_file&#39;, &#39;$DATA_DIR/train-v2.0.json&#39;, # provide the training data &#39;--version_2_with_negative&#39;, # ** MUST use this flag if training on SQuAD 2.0! DO NOT use if training on SQuAD 1.1 &#39;--do_lower_case&#39;, # ** set this flag if using an uncased model; don&#39;t use for Cased Models &#39;--do_eval&#39;, # execute the evaluation method on the dev set -- note: # if coupled with --do_train, evaluation runs after fine-tuning &#39;--predict_file&#39;, &#39;$DATA_DIR/dev-v2.0.json&#39;, # provide evaluation data (dev set) &#39;--eval_all_checkpoints&#39;, # evaluate the model on the dev set at each checkpoint &#39;--per_gpu_eval_batch_size&#39;, &#39;12&#39;, # evaluation batch size for each gpu &#39;--per_gpu_train_batch_size&#39;, &#39;12&#39;, # training batch size for each gpu &#39;--save_steps&#39;, &#39;5000&#39;, # how often checkpoints (complete model snapshot) are saved &#39;--threads&#39;, &#39;8&#39;, # num of CPU threads to use for converting SQuAD examples to model features # Model and Feature Hyperparameters &#39;--num_train_epochs&#39;, &#39;3&#39;, # number of training epochs - usually 2-3 for SQuAD &#39;--learning_rate&#39;, &#39;3e-5&#39;, # learning rate for the default optimizer (Adam in this case) &#39;--max_seq_length&#39;, &#39;384&#39;, # maximum length allowed for the full input sequence &#39;--doc_stride&#39;, &#39;128&#39; # used for long documents that must be chunked into multiple features -- # this &quot;sliding window&quot; controls the amount of stride between chunks ] . Here&#39;s what to expect when executing run_squad.py for the first time: . Pre-trained model weights for the specified model type (i.e., bert-base-uncased) are downloaded. | SQuAD training examples are converted into features (takes 15-30 minutes depending on dataset size and number of threads). | Training features are saved to a cache file (so that you don&#39;t have to do this again for this model type). | If --do_train, training commences for as many epochs as you specify, saving the model weights every --save_steps steps until training finishes. These checkpoints are saved in [--output_dir]/checkpoint-[step number] subdirectories. | The final model weights and peripheral files are saved to --output_dir. | If --do_eval, SQuAD dev examples are converted into features. | Dev features are also saved to a cache file. | Evaluation commences and outputs a dizzying assortment of performance scores. | Time to train! . But first, a note on compute requirements. We don&#39;t recommend fine-tuning a Transformer model unless you&#39;re rocking at least one GPU and a considerable amount of RAM. For context, our GPU is several years old (GeForce GTX TITAN X), and while it&#39;s not nearly as fast as the Tesla V100 (the current Cadillac of GPUs), it gets the job done. Fine-tuning bert-base-uncased takes about 1.75 hours per epoch. Additionally, our workhorse machine has 32GB CPU and 12GB GPU memory, which is sufficient for data processing and training most models on either of the SQuAD datasets. . The following cells demonstrate two ways to fine-tune: on the command line and in a Colab notebook. . Training on the command line . We saved the following as a shell script (run_squad.sh) and ran on the command line ($ source run_squad.sh) of our workhorse GPU machine. Shell scripts help prevent numerous mistakes and mis-keys when typing args to a command line, especially for complex scripts like this. They also allow you to keep track of which arguments were used last (though, as we&#39;ll see below, the run_squad.py script has a solution for that). We actually kept two shell scripts -- one explicitly for training and another for evaluation. . #!/bin/sh export DATA_DIR=./data/squad export MODEL_DIR=./models python run_squad.py --model_type bert --model_name_or_path bert-base-uncased --output_dir models/bert/ --data_dir data/squad --overwrite_output_dir --overwrite_cache --do_train --train_file train-v2.0.json --version_2_with_negative --do_lower_case --do_eval --predict_file dev-v2.0.json --per_gpu_train_batch_size 2 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 384 --doc_stride 128 --threads 10 --save_steps 5000 . Training in Colab . Alternatively, you can execute training in the cell as shown below. We note that standard Colab environments only provide 12GB of RAM. Converting the SQuAD dataset to features is memory intensive and may cause the basic Colab environment to fail silently. If you have a Colab instance with additional memory capacity (16GB+), this cell should execute fully. . !python run_squad.py --model_type bert --model_name_or_path bert-base-uncased --output_dir models/bert/ --data_dir data/squad --overwrite_output_dir --overwrite_cache --do_train --train_file train-v2.0.json --version_2_with_negative --do_lower_case --do_eval --predict_file dev-v2.0.json --per_gpu_train_batch_size 2 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 384 --doc_stride 128 --threads 10 --save_steps 5000 . Training Output . Successful completion of the run_squad.py yields a slew of output, which can be found in the --output_dir directory specified above. There you&#39;ll find... . Files for the model&#39;s tokenizer: . tokenizer_config.json | vocab.txt | special_tokens_map.json | . Files for the model itself: . pytorch_model.bin: these are the actual model weights (this file can be several GB for some models) | config.json: details of the model architecture | . Binary representation of the command line arguments used to train this model (so you&#39;ll never forget which arguments you used!) . training_args.bin | . And if you included --do_eval, you&#39;ll also see these files: . predictions_.json: the official best answer for each example | nbest_predictions_.json: the top n best answers for each example | . Providing the path to this directory to AutoModel or AutoModelForQuestionAnswering will load your fine-tuned model for use. . from transformers import AutoTokenizer, AutoModelForQuestionAnswering # Load the fine-tuned model tokenizer = AutoTokenizer.from_pretrained(&quot;./models/bert/bbu_squad2&quot;) model = AutoModelForQuestionAnswering.from_pretrained(&quot;./models/bert/bbu_squad2&quot;) . Using a pre-fine-tuned model from the Hugging Face repository . If you don&#39;t have access to GPUs or don&#39;t have the time to fiddle and train models, you&#39;re in luck! Hugging Face is more than a collection of slick Transformer classes -- it also hosts a repository for pre-trained and fine-tuned models contributed from the wide community of NLP practitioners. Searching for &quot;squad&quot; brings up at least 55 models. . . Each of these links provides explicit code for using the model, and, in some cases, information on how it was trained and what results were achieved. Let&#39;s load one of these pre-fine-tuned models. . import torch from transformers import AutoTokenizer, AutoModelForQuestionAnswering # executing these commands for the first time initiates a download of the # model weights to ~/.cache/torch/transformers/ tokenizer = AutoTokenizer.from_pretrained(&quot;deepset/bert-base-cased-squad2&quot;) model = AutoModelForQuestionAnswering.from_pretrained(&quot;deepset/bert-base-cased-squad2&quot;) . Let&#39;s try our model! . Whether you fine-tuned your own or used a pre-fine-tuned model, it&#39;s time to play with it! There are three steps to QA: . tokenize the input | obtain model scores | get the answer span | These steps are discussed in detail in the HF Transformer Notebooks. . question = &quot;Who ruled Macedonia&quot; context = &quot;&quot;&quot;Macedonia was an ancient kingdom on the periphery of Archaic and Classical Greece, and later the dominant state of Hellenistic Greece. The kingdom was founded and initially ruled by the Argead dynasty, followed by the Antipatrid and Antigonid dynasties. Home to the ancient Macedonians, it originated on the northeastern part of the Greek peninsula. Before the 4th century BC, it was a small kingdom outside of the area dominated by the city-states of Athens, Sparta and Thebes, and briefly subordinate to Achaemenid Persia.&quot;&quot;&quot; # 1. TOKENIZE THE INPUT # note: if you don&#39;t include return_tensors=&#39;pt&#39; you&#39;ll get a list of lists which is easier for # exploration but you cannot feed that into a model. inputs = tokenizer.encode_plus(question, context, return_tensors=&quot;pt&quot;) # 2. OBTAIN MODEL SCORES # the AutoModelForQuestionAnswering class includes a span predictor on top of the model. # the model returns answer start and end scores for each word in the text answer_start_scores, answer_end_scores = model(**inputs) answer_start = torch.argmax(answer_start_scores) # get the most likely beginning of answer with the argmax of the score answer_end = torch.argmax(answer_end_scores) + 1 # get the most likely end of answer with the argmax of the score # 3. GET THE ANSWER SPAN # once we have the most likely start and end tokens, we grab all the tokens between them # and convert tokens back to words! tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;][0][answer_start:answer_end])) . &#39;the Argead dynasty&#39; . QA on Wikipedia pages . We tried our model on a question paired with a short passage, but what if we want to retrieve an answer from a longer document? A typical Wikipedia page is much longer than the example above, and we need to do a bit of massaging before we can use our model on longer contexts. . Let&#39;s start by pulling up a Wikipedia page. . import wikipedia as wiki import pprint as pp question = &#39;What is the wingspan of an albatross?&#39; results = wiki.search(question) print(&quot;Wikipedia search results for our question: n&quot;) pp.pprint(results) page = wiki.page(results[0]) text = page.content print(f&quot; nThe {results[0]} Wikipedia article contains {len(text)} characters.&quot;) . Wikipedia search results for our question: [&#39;Albatross&#39;, &#39;List of largest birds&#39;, &#39;Black-browed albatross&#39;, &#39;Argentavis&#39;, &#39;Pterosaur&#39;, &#39;Mollymawk&#39;, &#39;List of birds by flight speed&#39;, &#39;Largest body part&#39;, &#39;Pelican&#39;, &#39;Aspect ratio (aeronautics)&#39;] The Albatross Wikipedia article contains 38200 characters. . inputs = tokenizer.encode_plus(question, text, return_tensors=&#39;pt&#39;) print(f&quot;This translates into {len(inputs[&#39;input_ids&#39;][0])} tokens.&quot;) . Token indices sequence length is longer than the specified maximum sequence length for this model (10 &gt; 512). Running this sequence through the model will result in indexing errors . This translates into 8824 tokens. . The tokenizer takes the input as text and returns tokens. In general, tokenizers convert words or pieces of words into a model-ingestible format. The specific tokens and format are dependent on the type of model. For example, BERT tokenizes words differently from RoBERTa, so be sure to always use the associated tokenizer appropriate for your model. . In this case, the tokenizer converts our input text into 8824 tokens, but this far exceeds the maximum number of tokens that can be fed to the model at one time. Most BERT-esque models can only accept 512 tokens at once, thus the (somewhat confusing) warning above (how is 10 &gt; 512?). This means we&#39;ll have to split our input into chunks and each chunk must not exceed 512 tokens in total. . When working with Question Answering, it&#39;s crucial that each chunk follows this format: . [CLS] question tokens [SEP] context tokens [SEP] . This means that, for each segment of a Wikipedia article, we must prepend the original question, followed by the next &quot;chunk&quot; of article tokens. . # time to chunk! from collections import OrderedDict # identify question tokens (token_type_ids = 0) qmask = inputs[&#39;token_type_ids&#39;].lt(1) qt = torch.masked_select(inputs[&#39;input_ids&#39;], qmask) print(f&quot;The question consists of {qt.size()[0]} tokens.&quot;) chunk_size = model.config.max_position_embeddings - qt.size()[0] - 1 # the &quot;-1&quot; accounts for # having to add a [SEP] token to the end of each chunk print(f&quot;Each chunk will contain {chunk_size - 2} tokens of the Wikipedia article.&quot;) # create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input chunked_input = OrderedDict() for k,v in inputs.items(): q = torch.masked_select(v, qmask) c = torch.masked_select(v, ~qmask) chunks = torch.split(c, chunk_size) for i, chunk in enumerate(chunks): if i not in chunked_input: chunked_input[i] = {} thing = torch.cat((q, chunk)) if i != len(chunks)-1: if k == &#39;input_ids&#39;: thing = torch.cat((thing, torch.tensor([102]))) else: thing = torch.cat((thing, torch.tensor([1]))) chunked_input[i][k] = torch.unsqueeze(thing, dim=0) . The question consists of 12 tokens. Each chunk will contain 497 tokens of the Wikipedia article. . for i in range(len(chunked_input.keys())): print(f&quot;Number of tokens in chunk {i}: {len(chunked_input[i][&#39;input_ids&#39;].tolist()[0])}&quot;) . Number of tokens in chunk 0: 512 Number of tokens in chunk 1: 512 Number of tokens in chunk 2: 512 Number of tokens in chunk 3: 512 Number of tokens in chunk 4: 512 Number of tokens in chunk 5: 512 Number of tokens in chunk 6: 512 Number of tokens in chunk 7: 512 Number of tokens in chunk 8: 512 Number of tokens in chunk 9: 512 Number of tokens in chunk 10: 512 Number of tokens in chunk 11: 512 Number of tokens in chunk 12: 512 Number of tokens in chunk 13: 512 Number of tokens in chunk 14: 512 Number of tokens in chunk 15: 512 Number of tokens in chunk 16: 512 Number of tokens in chunk 17: 341 . Each of these chunks (except for the last one) has the following structure: . [CLS], 12 question tokens, [SEP], 497 tokens of the Wikipedia article, [SEP] token = 512 tokens . Each of these chunks can now be fed to the model without causing indexing errors. We&#39;ll get an &quot;answer&quot; for each chunk; however, not all answers are useful, since not every segment of a Wikipedia article is informative for our question. The model will return the [CLS] token when it determines that the context does not contain an answer to the question. . def convert_ids_to_string(tokenizer, input_ids): return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids)) answer = &#39;&#39; # now we iterate over our chunks, looking for the best answer from each chunk for _, chunk in chunked_input.items(): answer_start_scores, answer_end_scores = model(**chunk) answer_start = torch.argmax(answer_start_scores) answer_end = torch.argmax(answer_end_scores) + 1 ans = convert_ids_to_string(tokenizer, chunk[&#39;input_ids&#39;][0][answer_start:answer_end]) # if the ans == [CLS] then the model did not find a real answer in this chunk if ans != &#39;[CLS]&#39;: answer += ans + &quot; / &quot; print(answer) . 3 . 7 m / . Putting it all together . Let&#39;s recap. We&#39;ve essentially built a simple IR-based QA system! We&#39;re using wikipedia&#39;s search engine to return a list of candidate documents that we then feed into our document reader (in this case, BERT fine-tuned on SQuAD 2.0). Let&#39;s make our code easier to read and more self-contained by packaging the document reader into a class. . from transformers import AutoTokenizer, AutoModelForQuestionAnswering class DocumentReader: def __init__(self, pretrained_model_name_or_path=&#39;bert-large-uncased&#39;): self.READER_PATH = pretrained_model_name_or_path self.tokenizer = AutoTokenizer.from_pretrained(self.READER_PATH) self.model = AutoModelForQuestionAnswering.from_pretrained(self.READER_PATH) self.max_len = self.model.config.max_position_embeddings self.chunked = False def tokenize(self, question, text): self.inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=&quot;pt&quot;) self.input_ids = self.inputs[&quot;input_ids&quot;].tolist()[0] if len(self.input_ids) &gt; self.max_len: self.inputs = self.chunkify() self.chunked = True def chunkify(self): &quot;&quot;&quot; Break up a long article into chunks that fit within the max token requirement for that Transformer model. Calls to BERT / RoBERTa / ALBERT require the following format: [CLS] question tokens [SEP] context tokens [SEP]. &quot;&quot;&quot; # create question mask based on token_type_ids # value is 0 for question tokens, 1 for context tokens qmask = self.inputs[&#39;token_type_ids&#39;].lt(1) qt = torch.masked_select(self.inputs[&#39;input_ids&#39;], qmask) chunk_size = self.max_len - qt.size()[0] - 1 # the &quot;-1&quot; accounts for # having to add an ending [SEP] token to the end # create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input chunked_input = OrderedDict() for k,v in self.inputs.items(): q = torch.masked_select(v, qmask) c = torch.masked_select(v, ~qmask) chunks = torch.split(c, chunk_size) for i, chunk in enumerate(chunks): if i not in chunked_input: chunked_input[i] = {} thing = torch.cat((q, chunk)) if i != len(chunks)-1: if k == &#39;input_ids&#39;: thing = torch.cat((thing, torch.tensor([102]))) else: thing = torch.cat((thing, torch.tensor([1]))) chunked_input[i][k] = torch.unsqueeze(thing, dim=0) return chunked_input def get_answer(self): if self.chunked: answer = &#39;&#39; for k, chunk in self.inputs.items(): answer_start_scores, answer_end_scores = self.model(**chunk) answer_start = torch.argmax(answer_start_scores) answer_end = torch.argmax(answer_end_scores) + 1 ans = self.convert_ids_to_string(chunk[&#39;input_ids&#39;][0][answer_start:answer_end]) if ans != &#39;[CLS]&#39;: answer += ans + &quot; / &quot; return answer else: answer_start_scores, answer_end_scores = self.model(**self.inputs) answer_start = torch.argmax(answer_start_scores) # get the most likely beginning of answer with the argmax of the score answer_end = torch.argmax(answer_end_scores) + 1 # get the most likely end of answer with the argmax of the score return self.convert_ids_to_string(self.inputs[&#39;input_ids&#39;][0][ answer_start:answer_end]) def convert_ids_to_string(self, input_ids): return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids)) . Below is our clean, fully working QA system! Feel free to add your own questions. . # collapse-hide # to make the following output more readable I&#39;ll turn off the token sequence length warning import logging logging.getLogger(&quot;transformers.tokenization_utils&quot;).setLevel(logging.ERROR) . . questions = [ &#39;When was Barack Obama born?&#39;, &#39;Why is the sky blue?&#39;, &#39;How many sides does a pentagon have?&#39; ] reader = DocumentReader(&quot;deepset/bert-base-cased-squad2&quot;) # if you trained your own model using the training cell earlier, you can access it with this: #reader = DocumentReader(&quot;./models/bert/bbu_squad2&quot;) for question in questions: print(f&quot;Question: {question}&quot;) results = wiki.search(question) page = wiki.page(results[0]) print(f&quot;Top wiki result: {page}&quot;) text = page.content reader.tokenize(question, text) print(f&quot;Answer: {reader.get_answer()}&quot;) print() . Question: When was Barack Obama born? Top wiki result: &lt;WikipediaPage &#39;Barack Obama Sr.&#39;&gt; Answer: 18 June 1936 / February 2 , 1961 / Question: Why is the sky blue? Top wiki result: &lt;WikipediaPage &#39;Diffuse sky radiation&#39;&gt; Answer: Rayleigh scattering / Question: How many sides does a pentagon have? Top wiki result: &lt;WikipediaPage &#39;The Pentagon&#39;&gt; Answer: five / . It got 2 out of 3 questions right! . Notice that, at least for the current questions we&#39;ve chosen, the QA system fails because of Wikipedia&#39;s default search engine, not because of BERT! It pulls up the wrong page for two of our questions: a page about Barack Obama Sr. instead of the former US President, and an article about the US&#39;s Department of Defense building &quot;The Pentagon&quot; instead of a page about geometry. In the latter case, we ended up with the correct answer by coincidence! This illustrates that any successful IR-based QA system requires a search engine (document retriever) as good as the document reader. . Wrapping Up . There we have it! A working QA system on Wikipedia articles. This is great, but it&#39;s admittedly not very sophisticated. Furthermore, we&#39;ve left a lot of questions unanswered: . Why fine-tune on the SQuAD dataset and not something else? What other options are there? | How good is BERT at answering questions? And how do we define &quot;good&quot;? | Why BERT and not another Transformer model? | Currently, our QA system can return an answer for each chunk of a Wiki article, but not all of those answers are correct -- How can we improve our get_answer method? | Additionally, we&#39;re chunking a wiki article in such a way that we could be ending a chunk in the middle of a sentence -- Can we improve our chunkify method? | Over the course of this project, we&#39;ll tackle these questions and more. By the end of this series we hope to demonstrate a snazzier QA model that incorporates everything we learn along the way. Stay tuned! .",
            "url": "https://qa.fastforwardlabs.com/pytorch/hugging%20face/wikipedia/bert/transformers/2020/05/19/Getting_Started_with_QA.html",
            "relUrl": "/pytorch/hugging%20face/wikipedia/bert/transformers/2020/05/19/Getting_Started_with_QA.html",
            "date": " ‚Ä¢ May 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Intro to Automated Question Answering",
            "content": "Welcome to the first edition of the Cloudera Fast Forward blog on Natural Language Processing for Question Answering! Throughout this series, we‚Äôll build a Question Answering (QA) system with off-the-shelf algorithms and libraries and blog about our process and what we find along the way. We hope to wind up with a beginning-to-end documentary that provides: . insight into QA as a tool, | useful context to make decisions for those who might build their own QA system, | tips and tricks we pick up as we go, and | sample code and commentary. | . We‚Äôre trying a new thing here. In the past, we‚Äôve documented our work in discrete reports at the end of our research process. We hope this new format suits the above goals and makes the topic more accessible, while ultimately being useful. . To kick off the series, this introductory post will discuss what QA is and isn‚Äôt, where this technology is being employed, and what techniques are used to accomplish this natural language task. . Question Answering in a Nutshell . Question Answering is a human-machine interaction to extract information from data using natural language queries. Machines do not inherently understand human languages any more than the average human understands machine language. A well-developed QA system bridges the gap between the two, allowing humans to extract knowledge from data in a way that is natural to us, i.e., asking questions. . QA systems accept questions in the form of natural language (typically text based, although you are probably also familiar with systems that accept speech input, such as Amazon‚Äôs Alexa or Apple‚Äôs Siri), and output a concise answer. Google‚Äôs search engine product adds a form of question answering in addition to its traditional search results, as illustrated here: . . Google took our question and returned a set of 1.3 million documents (not shown) relevant to the search terms, i.e., documents about Abraham Lincoln. Google also used what it knows about the contents of some of those documents to provide a ‚Äúsnippet‚Äù that answered our question in one word, presented above a link to the most pertinent website and keyword-highlighted text. . This goes beyond the standard capabilities of a search engine, which typically only return a list of relevant documents or websites. Google recently explained how they are using state-of-the-art NLP to enhance some of their search results. We‚Äôll revisit this example in a later section and discuss how this technology works in practice and how we can (and will!) build our own QA system. . Why Question Answering? . Sophisticated Google searches with precise answers are fun, but how useful are QA systems in general? It turns out that this technology is maturing rapidly. Gartner recently identified natural language processing and conversational analytics as one of the top trends poised to make a substantial impact in the next three to five years. These technologies will provide increased data access, ease of use, and wider adoption of analytics platforms - especially to mainstream users. QA systems specifically will be a core part of the NLP suite, and are already seeing adoption in several areas. . Business Intelligence (BI) platforms are beginning to use Machine Learning (ML) to assist their users in exploring and analyzing their data through ML-augmented data preparation and insight generation. One of the key ways that ML is augmenting BI platforms is through the incorporation of natural language query functionality, which allows users to more easily query systems, and retrieve and visualize insights in a natural and user-friendly way, reducing the need for deep expertise in query languages, such as SQL. . Another area where QA systems will shine is in corporate and general use chatbots. Chatbots have been around for several years, but they mostly rely on hand-tailored responses. QA systems can augment this existing technology, providing a deeper understanding to improve user experience. For example, a QA system with knowledge of a company‚Äôs FAQs can streamline customer experience, while QA systems built atop internal company documentation could provide employees easier access to logs, reports, financial statements, or design docs. . The success of these systems will vary based on the use case, implementation, and richness of data. The field of QA is just starting to become commercially viable and it‚Äôs picking up speed. We think it‚Äôs a field worth exploring in order to understand what uses it might (and might not) have. So how does this technology work? . Designing a Question Answerer . As explained above, question answering systems process natural language queries and output concise answers. This general capability can be implemented in dozens of ways. How a QA system is designed depends, in large part, on three key elements: the knowledge provided to the system, the types of questions it can answer, and the structure of the data supporting the system. . Domain . QA systems operate within a domain, constrained by the data that is provided to them. The domain represents the embodiment of all the knowledge the system can know. There are two domain paradigms: open and closed. Closed domain systems are narrow in scope and focus on a specific topic or regime. Open domain systems are broad, answering general knowledge questions. . The BASEBALL system is an early example of a closed domain QA system. Built in the 1960s, it was limited to answering questions surrounding one year‚Äôs worth of baseball facts and statistics. Not only was this domain constrained to the topic of baseball, it was also constrained in the timeframe of data at its proverbial fingertips. A contemporary example of closed domain QA systems are those found in some BI applications. Generally, their domain is scoped to whatever data the user supplies, so they can only answer questions on the specific datasets to which they have access. . By contrast, open domain QA systems rely on knowledge supplied from vast resources - such as Wikipedia or the World Wide Web - to answer general knowledge questions. These systems can even answer general trivia. One example of such a system is IBM‚Äôs Watson, which won on Jeopardy! in 2011 (perhaps Watson was more of an Answer Questioner? We like jokes). Google‚Äôs QA capability as demonstrated above would also be considered open domain. . Question Type . Once you‚Äôve decided the scope of knowledge your QA system will cover, you must also determine what types of questions it can answer. The vast majority of all QA systems answer factual questions: those that start with who, what, where, when, and how many. These types of questions tend to be straightforward enough for a machine to comprehend, and can be built directly atop structural databases or ontologies, as well as being extracted directly from unstructured text. . However, research is emerging that would allow QA systems to answer hypothetical questions, cause-effect questions, confirmation (yes/no) questions, and inferential questions (questions whose answers can be inferred from one or more pieces of evidence). Much of this research is still in its infancy, however, as the requisite natural language understanding is (for now) beyond the capabilities of most of today‚Äôs algorithms. . Implementation . There‚Äôs more than one way to cuddle a cat, as the saying goes. Question answering seeks to extract information from data and, generally speaking, data come in two broad formats: structured and unstructured. QA algorithms have been developed to harness the information from either paradigm: knowledge-based systems for structured data and information retrieval-based systems for unstructured (text) data. Some QA systems exploit a hybrid design that harvests information from both data types; IBM‚Äôs Watson is a famous example. In this section, we‚Äôll highlight some of the most widely used techniques in each data regime - concentrating more on those for unstructured data, since this will be the focus of our applied research. Because we‚Äôll be discussing explicit methods and techniques, the following sections are more technical. And we‚Äôll note that, while we provide an overview here, an even more comprehensive discussion can be found in the Question Answering chapter of Jurafsky and Martin‚Äôs Speech and Language Processing (a highly accessible textbook). . Knowledge-Based Systems . A large quantity of data is encapsulated in structured formats, e.g., relational databases. The goal of knowledge-based QA systems is to map questions to these structured entities through semantic parsing algorithms. Semantic parsing techniques convert text strings to symbolic logic or query languages, e.g., SQL. . . Semantic parsing algorithms are highly tailored to their specific domain and database, and utilize templates as well as supervised learning approaches. Templates are handwritten rules, useful for frequently observed logical relationships. For example, an employee database might have a start-date template consisting of handwritten rules that search for when and hired since ‚Äúwhen was Employee Name hired‚Äù would likely be a common query. . Supervised methods generalize this approach and are used when there exists a dataset of question-logical form pairs, such as in the figure above. These algorithms process the question, creating a parse tree that then maps the relevant parts of speech (nouns, verbs, and modifiers) to the appropriate logical form. Many algorithms begin with simple relationship mapping: matching segments from the question parse tree to a logical relation, as in the two examples below. . . The algorithm then bootstraps from simple relationship logic to incorporate more specific information from the parse tree, mapping it to more sophisticated logical queries like this birth-year example below. . . These systems can be made more robust by providing lexicons that capture the semantics and variations of natural language. For instance, in our employee database example, a question might contain the word ‚Äúemployed‚Äù rather than ‚Äúhired,‚Äù but the intention is the same. . Information Retrieval-Based Systems: Retrievers and Readers . . Information retrieval-based question answering (IR QA) systems find and extract a text segment from a large collection of documents. The collection can be as vast as the entire web (open domain) or as specific as a company‚Äôs Confluence documents (closed domain). Contemporary IR QA systems first identify the most relevant documents in the collection, and then extract the answer from the contents of those documents. To illustrate this approach, let‚Äôs revisit our Google example from the introduction, only this time we‚Äôll include some of the search results! . . We already talked about how the snippet box acts like a QA system. The search results below the snippet illustrate some of the reasons why an IR QA system can be more useful than a search engine alone. The relevant links vary from what is essentially advertising (study.com) to making fun of Lincoln‚Äôs ears (Reddit at its finest) to a discussion of color blindness (answers.com without the answer we want) to an article about all presidents‚Äô eye colors (getting warmer, Chicago Tribune) to the very last link (answers.yahoo.com, which is on-topic - and narrowly scoped to Lincoln - but gives an ambiguous answer). Without the snippet box at the top, a user would have to skim each of these links to locate their answer - with varying degrees of success. . IR QA systems are not just search engines, which take general natural language terms and provide a list of relevant documents. IR QA systems perform an additional layer of processing on the most relevant documents to deliver a pointed answer, based on the contents of those documents (like the snippet box). While we won‚Äôt hazard a guess at exactly how Google extracted ‚Äúgray‚Äù from these search results, we can examine how an IR QA system could exhibit similar functionality in a real world (e.g., non-Google) implementation. . Below we illustrate the workflow of a generic IR-based QA system. These systems generally have two main components: the document retriever and the document reader. . . The document retriever functions as the search engine, ranking and retrieving relevant documents to which it has access. It supplies a set of candidate documents that could answer the question (often with mixed results, per the Google search shown above). The document reader consists of reading comprehension algorithms built with core NLP techniques. This component processes the candidate documents and extracts from one of them an explicit span of text that best satisfies the query. Let‚Äôs dive deeper into each of these components. . Document Retriever . The document retriever has two core jobs: process the question for use in an IR engine, and use this IR query to retrieve the most appropriate documents and passages. Query processing can be as simple as no processing at all, and instead passing the entire question to the search engine. However, if the question is long or complicated, it often pays to process the query through various techniques - such as stop word removal, removing wh-words, converting to n-grams, or extracting named entities as keywords. . Some systems also extract contextual information from the query, e.g., the focus of the question and the expected answer type, which can then be used in the Document Reader during the answer extraction phase. The focus of a question is the string within the query that the user is looking to fill. The answer type is categorical, e.g., person, location, time, etc. In our earlier example, ‚Äúwhen was Employee Name hired?‚Äù, the focus would be ‚Äúwhen‚Äù and the answer type might be a numeric date-time. . The IR query is then passed to an IR algorithm. These algorithms search over all documents often using standard tf-idf cosine matching to rank documents by relevance. The simplest implementations would pass the top n most relevant documents to the document reader for answer extraction but this, too, can be made more sophisticated by breaking documents into their respective passages or paragraphs and filtering them (based on named entity matching or answer type, for example) to narrow down the number of passages sent to the document reader. . Document Reader . Once we have a selection of relevant documents or passages, it‚Äôs time to extract the answer. The sole purpose of the document reader is to apply reading comprehension algorithms to text segments for answer extraction. Modern reading comprehension algorithms come in two broad flavors: feature-based and neural-based. . Feature-based answer extraction can include rule-based templates, regex pattern matching, or a suite of NLP models (such as parts-of-speech tagging and named entity recognition) designed to identify features that will allow a supervised learning algorithm to determine whether a span of text contains the answer. One useful feature is the answer type identified by the document retriever during query processing. Other features could include the number of matched keywords in the question, the distance between the candidate answer and the query keywords, and the location of punctuation around the candidate answer. This type of QA works best when the answers are short and when the domain is narrow. . Neural-based reading comprehension approaches capitalize on the idea that the question and the answer are semantically similar. Rather than relying on keywords, these methods use extensive datasets that allow the model to learn semantic embeddings for the question and the passage. Similarity functions on these embeddings provide answer extraction. . Neural network models that perform well in this arena are Seq2Seq models and Transformers. (For a detailed dive into these architectures, interested readers should check out these excellent posts for Seq2Seq and Transformers.) The Transformer architecture in particular is currently revolutionizing the entire field of NLP. Models builts on this architecture include BERT (and its myriad off-shoots: RoBERTa, ALBERT, distilBERT, etc.), XLNet, GPT, T5, and more. These models - coupled with advances in compute power and transfer learning from massive unsupervised training sets - have started to outperform humans on some key NLP benchmarks, including question answering. . In this paradigm, one does not need to identify the answer type, the parts of speech, or the proper nouns. One need only feed the question and the passage into the model and wait for the answer. While this is an exciting development, it does have its drawbacks. When the model doesn‚Äôt work, it‚Äôs not always straightforward to identify the problem - and scaling these models is still a challenging prospect. These models generally perform better (according to your quantitative metric of choice) relative to the number of parameters they have (the more, the better), but the cost of inference also goes up - and with it, the difficulty of implementation in settings like federated learning scenarios or on mobile devices. . Building a Question-Answerer . At the beginning of this article, we said we were going to build a QA system. Now that we‚Äôve covered some background, we can describe our approach. Over the course of the next two months, two of Cloudera Fast Forward‚Äôs Research Engineers, Melanie Beck and Ryan Micallef, will build a QA system following the information retrieval-based method, by creating a document retriever and document reader. We‚Äôll focus our efforts on exploring and experimenting with various Transformer architectures (like BERT) for the document reader, as well as off-the-shelf search engine algorithms for the retriever. Neither of us has built a system like this before, so it‚Äôll be a learning experience for everyone. And that‚Äôs precisely why we wanted to invite you along for the journey! We‚Äôll share what we learn each step of the way by posting and discussing example code, in addition to articles covering topics like: . existing QA training sets for Transformers and what you‚Äôll need to develop your own | how to evaluate the quality of a QA system - both the reader and retriever | building a search engine over a large set of documents | and more! | . Because we‚Äôll be writing about our work as we go, we might end up in some dead ends or run into some nasty bugs; such is the nature of research! When these things happen, we‚Äôll share our thoughts on what worked, what didn‚Äôt, and why - but it‚Äôs important to note upfront that while we do have a solid goal in mind, the end product may turn out to be quite different than what we currently envision. Stay tuned; in our next post we‚Äôll start digging into the nuts and bolts! .",
            "url": "https://qa.fastforwardlabs.com/methods/background/2020/04/28/Intro-to-QA.html",
            "relUrl": "/methods/background/2020/04/28/Intro-to-QA.html",
            "date": " ‚Ä¢ Apr 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Us",
          "content": "Cloudera Fast Forward Labs is a machine intelligence research and advisory group within Cloudera. We routinely publish comprehensive reports that focus on a wide variety of machine learning capabilities. This blog focuses on question answering, a natural language processing and understanding (NLP/U) task that has recently begun to show signs of maturity, and documents our approach to implementing this model in a general setting, but with corporate use cases in mind. This project is led by two of CFF‚Äôs Research Engineers. . Melanie R. Beck, Ph.D. . Melanie is a reformed astrophysicist who now spends her days dreaming about machines and algorithms (to be honest, she still dreams about galaxies, too). She excels at understanding academic inventions but is even better at translating them into a language the rest of us can understand. Lately, she‚Äôs been obsessed with all things NLP. With experience as a data scientist in multiple industries ranging from hardware manufacturing to cybersecurity, she is a jack-of-all-trades with a passion for sharing what she‚Äôs learned. When she‚Äôs not reading about some esoteric machine learning framework, she can be found playing with her dog and two cats, or cross- stitching. . Ryan Micallef . Ryan researches emerging machine learning technologies and helps clients apply them. He is also an attorney barred in New York. He was an intellectual property litigator focused on technical cases for almost a decade before joining Fast Forward Labs (which has since been acquired by Cloudera). Ryan has a bachelor‚Äôs degree in Computer Science from Georgia Tech and a Juris Doctor degree from Brooklyn Law School. Ryan spends his free time soldering circuits and wrenching motorcycles, and teaches microcontroller programming at his local hackerspace, NYC Resistor. .",
          "url": "https://qa.fastforwardlabs.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}