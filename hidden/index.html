<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Building a QA System with BERT on Wikipedia | NLP for Question Answering</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Building a QA System with BERT on Wikipedia" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A high-level code walk-through of an IR-based QA system with PyTorch and Hugging Face." />
<meta property="og:description" content="A high-level code walk-through of an IR-based QA system with PyTorch and Hugging Face." />
<link rel="canonical" href="https://qa.fastforwardlabs.com/hidden/" />
<meta property="og:url" content="https://qa.fastforwardlabs.com/hidden/" />
<meta property="og:site_name" content="NLP for Question Answering" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-13T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"A high-level code walk-through of an IR-based QA system with PyTorch and Hugging Face.","@type":"BlogPosting","headline":"Building a QA System with BERT on Wikipedia","dateModified":"2020-05-13T00:00:00-05:00","datePublished":"2020-05-13T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://qa.fastforwardlabs.com/hidden/"},"url":"https://qa.fastforwardlabs.com/hidden/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://qa.fastforwardlabs.com/feed.xml" title="NLP for Question Answering" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-157475426-3','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Building a QA System with BERT on Wikipedia | NLP for Question Answering</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Building a QA System with BERT on Wikipedia" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A high-level code walk-through of an IR-based QA system with PyTorch and Hugging Face." />
<meta property="og:description" content="A high-level code walk-through of an IR-based QA system with PyTorch and Hugging Face." />
<link rel="canonical" href="https://qa.fastforwardlabs.com/hidden/" />
<meta property="og:url" content="https://qa.fastforwardlabs.com/hidden/" />
<meta property="og:site_name" content="NLP for Question Answering" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-13T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"A high-level code walk-through of an IR-based QA system with PyTorch and Hugging Face.","@type":"BlogPosting","headline":"Building a QA System with BERT on Wikipedia","dateModified":"2020-05-13T00:00:00-05:00","datePublished":"2020-05-13T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://qa.fastforwardlabs.com/hidden/"},"url":"https://qa.fastforwardlabs.com/hidden/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://qa.fastforwardlabs.com/feed.xml" title="NLP for Question Answering" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-157475426-3','auto');ga('require','displayfeatures');ga('send','pageview');</script>



<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">NLP for Question Answering</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Us</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Building a QA System with BERT on Wikipedia</h1><p class="page-description">A high-level code walk-through of an IR-based QA system with PyTorch and Hugging Face.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-05-13T00:00:00-05:00" itemprop="datePublished">
        May 13, 2020
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      23 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#PyTorch">PyTorch</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#Hugging Face">Hugging Face</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#Wikipedia">Wikipedia</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#BERT">BERT</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#Transformers">Transformers</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/fastforwardlabs/ff14_blog/tree/master/_notebooks/Getting_Started_with_QA.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/fastforwardlabs/ff14_blog/master?filepath=_notebooks%2FGetting_Started_with_QA.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/fastforwardlabs/ff14_blog/blob/master/_notebooks/Getting_Started_with_QA.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#So-you've-decided-to-build-a-QA-system.">So you&#39;ve decided to build a QA system. </a></li>
<li class="toc-entry toc-h1"><a href="#Setting-up-your-virtual-environment">Setting up your virtual environment </a></li>
<li class="toc-entry toc-h1"><a href="#Hugging-Face-Transformers">Hugging Face Transformers </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Fine-tuning-a-Transformer-model-for-Question-Answering">Fine-tuning a Transformer model for Question Answering </a>
<ul>
<li class="toc-entry toc-h3"><a href="#1.-Pick-a-Model">1. Pick a Model </a></li>
<li class="toc-entry toc-h3"><a href="#2.-QA-dataset:-SQuAD">2. QA dataset: SQuAD </a></li>
<li class="toc-entry toc-h3"><a href="#3.-Fine-tuning-script">3. Fine-tuning script </a></li>
<li class="toc-entry toc-h3"><a href="#Training-requirements">Training requirements </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Training-on-the-command-line">Training on the command line </a></li>
<li class="toc-entry toc-h4"><a href="#Training-in-Colab">Training in Colab </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Training-Output">Training Output </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Using-a-pre-fine-tuned-model-from-the-Hugging-Face-repository">Using a pre-fine-tuned model from the Hugging Face repository </a></li>
<li class="toc-entry toc-h2"><a href="#Let's-try-our-model!">Let&#39;s try our model! </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#QA-on-Wikipedia-pages">QA on Wikipedia pages </a></li>
<li class="toc-entry toc-h1"><a href="#Putting-it-all-together">Putting it all together </a></li>
<li class="toc-entry toc-h1"><a href="#Wrapping-Up">Wrapping Up </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/Getting_Started_with_QA.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/copied_from_nb/my_icons/markus-spiske-C0koz3G1I4I-unsplash.jpg" alt="" title="Image by Markus Spiske at Unsplash.com"></p>
<h1 id="So-you've-decided-to-build-a-QA-system.">
<a class="anchor" href="#So-you've-decided-to-build-a-QA-system." aria-hidden="true"><span class="octicon octicon-link"></span></a>So you've decided to build a QA system.<a class="anchor-link" href="#So-you've-decided-to-build-a-QA-system."> </a>
</h1>
<p>You want to start with something simple and general so you plan to make it open domain using Wikipedia as a corpus for answering questions. You want to use the best NLP that your compute resources allow (you're lucky enough to have access to a GPU) so you're going to focus on the big, flashy Transformer models that are all the rage these days.</p>
<p>Sounds like you're building an IR-based QA system. In our previous post (<a href="https://qa.fastforwardlabs.com/methods/background/2020/04/28/Intro-to-QA.html">Intro to Automated Question Answering</a>), we covered the general design of these systems, which typically require two main components: the document retriever (a search engine) that selects the <em>n</em> most relevant documents from a large collection; and a document reader that processes these candidate documents in search of an explicit answer span.</p>
<p><img src="/images/copied_from_nb/my_icons/QAworkflow.png" alt="" title="IR-based automated question anwering workflow"></p>
<p>This time we're going to build it! This post is chock full of code that walks through our approach. We'll also highlight and clarify some powerful resources, including off-the-shelf models and libraries, that you can use to quickly get going on a QA system of your own. We'll cover all the necessary steps including:</p>
<ul>
<li>installing libraries and setting up an environment</li>
<li>training a Transformer style model on the SQuAD dataset</li>
<li>understanding Hugging Face's <code>run_squad.py</code> training script and output</li>
<li>passing a full Wikipedia article as context for a question</li>
</ul>
<p>By the end of this post we'll have a working IR-based QA system with BERT as the document reader and Wikipedia's search engine as the document retriever - a fun toy model that that hints at potential real-world use cases.</p>
<p>This article was originally developed in a Jupyter Notebook and, thanks to <a href="https://fastpages.fast.ai/">fastpages</a>, converted to a blog post. For an interactive environment, click the "Open in Colab" button above, though we note that, due to Colab's system constraints, some of the cells in this notebook might not be fully executable. We'll note when this is the case, but don't worry -- you'll still be able to play around with all the fun stuff.</p>
<p>Let's get started!</p>
<h1 id="Setting-up-your-virtual-environment">
<a class="anchor" href="#Setting-up-your-virtual-environment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Setting up your virtual environment<a class="anchor-link" href="#Setting-up-your-virtual-environment"> </a>
</h1>
<p>A virtual environment is always best practice and we're using <code>venv</code> (though I'm also partial to <code>conda</code>) on our workhorse machine. For this project we'll be using PyTorch, which handles the heavy lifting of deep differentiable learning. If you have a GPU you'll want a PyTorch build that includes CUDA support, however, most cells in this notebook will work without a GPU. Check out <a href="https://pytorch.org/">PyTorch's quick install guide</a> to determine the best build for your GPU and OS. We'll also be using the <a href="https://huggingface.co/transformers/index.html">Transformers</a> libarary, which provides easy-to-use implementations of all the popular Transformer architectures, like BERT. Finally, we'll need the <a href="https://pypi.org/project/wikipedia/">wikipedia</a> library for easy access and parsing of Wikipedia pages.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can recreate our env (with CUDA 9.2 support -- but use the appropriate version for your machine) with the following commands in your command line:</p>
<div class="highlight"><pre><span></span>$ python3 -m venv myenv
$ <span class="nb">source</span> myenv/bin/activate
$ pip install <span class="nv">torch</span><span class="o">==</span><span class="m">1</span>.5.0+cu92 <span class="nv">torchvision</span><span class="o">==</span><span class="m">0</span>.6.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html
$ pip install <span class="nv">transformers</span><span class="o">==</span><span class="m">2</span>.5.1
$ pip install <span class="nv">wikipedia</span><span class="o">==</span><span class="m">1</span>.4.0
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note: Our GPU machine sports an older version of CUDA (9.2 -- we're getting around to updating that), so we need to use an older version of PyTorch for the necessary CUDA support.  The training script we'll be using requires some specific packages. More recent versions of PyTorch include these packages; however, older versions do not. If you have to work with an older version of PyTorch you might need to install <code>TensorboardX</code> (see the hidden code cell below).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># collapse-hide </span>

<span class="c1"># line 69 of `run_squad.py` script shows why you might need to install </span>
<span class="c1"># tensorboardX if you have an older version of torch</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">tensorboardX</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Conversely, if you're working in Colab, you can run the cell below.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install torch  torchvision -f https://download.pytorch.org/whl/torch_stable.html
<span class="o">!</span>pip install transformers
<span class="o">!</span>pip install wikipedia
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Hugging-Face-Transformers">
<a class="anchor" href="#Hugging-Face-Transformers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hugging Face Transformers<a class="anchor-link" href="#Hugging-Face-Transformers"> </a>
</h1>
<p>I'm new to PyTorch and even newer to Hugging Face (HF) but I'm quickly becoming a convert! The <a href="https://huggingface.co/transformers/#">Hugging Face Transformers</a> package provides state-of-the-art general-purpose architecures for natural language understanding and natural language generation. They host dozens of pre-trained models operating in over 100 languages that you can use right out of the box. All of these models come with deep interoperability between PyTorch and Tensorflow 2.0, which means you can move a model from TF2.0 to PyTorch and back again with a line or two of code!</p>
<p>If you're new to Hugging Face, we strongly recommend working through the HF <a href="https://huggingface.co/transformers/quickstart.html">Quickstart guide</a> as well as their excellent <a href="https://huggingface.co/transformers/notebooks.html">Transformer Notebooks</a> (we did!), as we won't cover that material in this notebook. We'll be using <code>AutoClasses</code>, which serve as a wrapper around pretty much any of the base Transformer classes.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Fine-tuning-a-Transformer-model-for-Question-Answering">
<a class="anchor" href="#Fine-tuning-a-Transformer-model-for-Question-Answering" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fine-tuning a Transformer model for Question Answering<a class="anchor-link" href="#Fine-tuning-a-Transformer-model-for-Question-Answering"> </a>
</h2>
<p>To train a Transformer for QA with Hugging Face we'll need</p>
<ol>
<li>to pick a specific model architecture</li>
<li>a QA dataset</li>
<li>the training script</li>
</ol>
<h3 id="1.-Pick-a-Model">
<a class="anchor" href="#1.-Pick-a-Model" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Pick a Model<a class="anchor-link" href="#1.-Pick-a-Model"> </a>
</h3>
<p>Not every Transformer architecture lends itself naturally to the task of question answering. For example, GPT does not do QA; similarly BERT does not do machine translation!  HF identify the following model types for the QA task:</p>
<ul>
<li>BERT</li>
<li>distilBERT </li>
<li>ALBERT</li>
<li>RoBERTa</li>
<li>XLNet</li>
<li>XLM</li>
<li>FlauBERT</li>
</ul>
<p>We'll stick with the now-classic BERT model in this notebook, but feel free to try out some others (we will - and we'll let you know when we do). Next up -- a training set.</p>
<h3 id="2.-QA-dataset:-SQuAD">
<a class="anchor" href="#2.-QA-dataset:-SQuAD" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. QA dataset: SQuAD<a class="anchor-link" href="#2.-QA-dataset:-SQuAD"> </a>
</h3>
<p>One of the most canonical datasets for QA is the Stanford Question Answering Dataset, or SQuAD, which comes in two flavors: SQuAD 1.1 and SQuAD 2.0. These reading comprehension datasets consist of questions posed on a set of Wikipedia articles, where the answer to every question is a segment (or span) of the corresponding passage. In SQuAD 1.1, all questions have an answer in the corresponding passage. SQuAD 2.0 steps up the difficulty by including questions that cannot be answered by the provided passage.</p>
<p>The following code will download the specified version of SQuAD.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Set path with magic</span>
<span class="o">%</span><span class="k">env</span> DATA_DIR=./data/squad 

<span class="c1"># Download the data</span>
<span class="k">def</span> <span class="nf">download_squad</span><span class="p">(</span><span class="n">version</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">version</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="o">!</span>wget -P <span class="nv">$DATA_DIR</span> https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json
        <span class="o">!</span>wget -P <span class="nv">$DATA_DIR</span> https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json
    <span class="k">else</span><span class="p">:</span>
        <span class="o">!</span>wget -P <span class="nv">$DATA_DIR</span> https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json
        <span class="o">!</span>wget -P <span class="nv">$DATA_DIR</span> https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json
            
<span class="n">download_squad</span><span class="p">(</span><span class="n">version</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>env: DATA_DIR=./data/squad
--2020-05-11 21:36:52--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json
Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.109.153, 185.199.108.153, 185.199.111.153, ...
Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.109.153|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 42123633 (40M) [application/json]
Saving to: â€˜./data/squad/train-v2.0.jsonâ€™

train-v2.0.json     100%[===================&gt;]  40.17M  14.6MB/s    in 2.8s    

2020-05-11 21:36:55 (14.6 MB/s) - â€˜./data/squad/train-v2.0.jsonâ€™ saved [42123633/42123633]

--2020-05-11 21:36:56--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json
Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.111.153, 185.199.108.153, ...
Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4370528 (4.2M) [application/json]
Saving to: â€˜./data/squad/dev-v2.0.jsonâ€™

dev-v2.0.json       100%[===================&gt;]   4.17M  6.68MB/s    in 0.6s    

2020-05-11 21:36:56 (6.68 MB/s) - â€˜./data/squad/dev-v2.0.jsonâ€™ saved [4370528/4370528]

</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.-Fine-tuning-script">
<a class="anchor" href="#3.-Fine-tuning-script" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. Fine-tuning script<a class="anchor-link" href="#3.-Fine-tuning-script"> </a>
</h3>
<p>We've chosen a model and we've got some data. Time to train!</p>
<p>All the standard models that HF support have been pre-trained, which means they've all been fed massive unsupervised training sets in order to learn basic language modeling. In order to perform well at specific tasks, like question answering, they must be trained further -- fine-tuned -- on specific datasets and tasks.</p>
<p>HF helpfully provide a script that fine-tunes a Transformer model on one of the SQuAD datasets, called <code>run_squad.py</code>. You can grab the script <a href="https://github.com/huggingface/transformers/blob/master/examples/question-answering/run_squad.py">here</a> or run the cell below.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Download the run_squad.py training script</span>
<span class="o">!</span>curl -L -O https://raw.githubusercontent.com/huggingface/transformers/master/examples/question-answering/run_squad.py
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This script takes care of all the hard work that goes into fine-tuning a model and as such, it's pretty complicated. It hosts no fewer than 45 arguments, providing an impressive amount of flexibility and utility for those who do a lot of training. While the details of this script will have to be left for another day, for now we'll walk through the basic command to fine-tune BERT on SQuAD 1.1 or 2.0. Below are the most important arguments for the <code>run_squad.py</code> fine-tuning script.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Fine-tuning your own model for QA using HF's `run_squad.py`</span>
<span class="c1"># Turn flags on and off according to the model you're training</span>

<span class="n">cmd</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">'python'</span><span class="p">,</span> 
<span class="c1">#    '-m torch.distributed.launch --nproc_per_node 2', # use this to perform distributed training over multiple GPUs</span>
    <span class="s1">'run_squad.py'</span><span class="p">,</span> 
    
    <span class="s1">'--model_type'</span><span class="p">,</span> <span class="s1">'bert'</span><span class="p">,</span>                            <span class="c1"># model type (one of the list under "Pick a Model" above)</span>
    
    <span class="s1">'--model_name_or_path'</span><span class="p">,</span> <span class="s1">'bert-base-uncased'</span><span class="p">,</span>       <span class="c1"># specific model name of the given model type (shown, a list is here: https://huggingface.co/transformers/pretrained_models.html) </span>
                                                       <span class="c1"># on first execution this initiates a download of pre-trained model weights;</span>
                                                       <span class="c1"># can also be a local path to a directory with model weights</span>
    
    <span class="s1">'--output_dir'</span><span class="p">,</span> <span class="s1">'./models/bert/bbu_squad2'</span><span class="p">,</span>        <span class="c1"># directory for model checkpoints and predictions</span>
    
<span class="c1">#    '--overwrite_output_dir',                         # use when adding output to a directory that is non-empty --</span>
                                                       <span class="c1"># for instance, when training crashes midway through and you need to restart it</span>
    
    <span class="s1">'--do_train'</span><span class="p">,</span>                                      <span class="c1"># execute the training method </span>
    
    <span class="s1">'--train_file'</span><span class="p">,</span> <span class="s1">'$DATA_DIR/train-v2.0.json'</span><span class="p">,</span>       <span class="c1"># provide the training data</span>
    
    <span class="s1">'--version_2_with_negative'</span><span class="p">,</span>                       <span class="c1"># ** MUST use this flag if training on SQuAD 2.0! DO NOT use if training on SQuAD 1.1</span>
    
    <span class="s1">'--do_lower_case'</span><span class="p">,</span>                                 <span class="c1"># ** set this flag if using an uncased model; don't use for Cased Models</span>
    
    <span class="s1">'--do_eval'</span><span class="p">,</span>                                       <span class="c1"># execute the evaluation method on the dev set -- note: </span>
                                                       <span class="c1"># if coupled with --do_train, evaluation runs after fine-tuning </span>
    
    <span class="s1">'--predict_file'</span><span class="p">,</span> <span class="s1">'$DATA_DIR/dev-v2.0.json'</span><span class="p">,</span>       <span class="c1"># provide evaluation data (dev set)</span>
    
    <span class="s1">'--eval_all_checkpoints'</span><span class="p">,</span>                          <span class="c1"># evaluate the model on the dev set at each checkpoint</span>
    
    <span class="s1">'--per_gpu_eval_batch_size'</span><span class="p">,</span> <span class="s1">'12'</span><span class="p">,</span>                 <span class="c1"># evaluation batch size for each gpu</span>
    
    <span class="s1">'--per_gpu_train_batch_size'</span><span class="p">,</span> <span class="s1">'12'</span><span class="p">,</span>                <span class="c1"># training batch size for each gpu</span>
    
    <span class="s1">'--save_steps'</span><span class="p">,</span> <span class="s1">'5000'</span><span class="p">,</span>                           <span class="c1"># How often checkpoints (complete model snapshot) are saved </span>
    
    <span class="s1">'--threads'</span><span class="p">,</span> <span class="s1">'8'</span>                                   <span class="c1"># num of CPU threads to use for converting SQuAD examples to model features</span>
    
    <span class="c1"># --- Model and Feature Hyperparameters --- </span>
    <span class="s1">'--num_train_epochs'</span><span class="p">,</span> <span class="s1">'3'</span><span class="p">,</span>                         <span class="c1"># number of training epochs - usually 2-3 for SQuAD </span>
    
    <span class="s1">'--learning_rate'</span><span class="p">,</span> <span class="s1">'3e-5'</span><span class="p">,</span>                         <span class="c1"># learning rate for the default optimizer (Adam in this case)</span>
    
    <span class="s1">'--max_seq_length'</span><span class="p">,</span> <span class="s1">'384'</span><span class="p">,</span>                         <span class="c1"># maximum length allowed for the full input sequence </span>
    
    <span class="s1">'--doc_stride'</span><span class="p">,</span> <span class="s1">'128'</span><span class="p">,</span>                             <span class="c1"># Used for long documents that must be chunked into multiple features -- </span>
                                                       <span class="c1"># this "sliding window" controls the amount of stride between chunks</span>
<span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here's what to expect when executing <code>run_squad.py</code> for the first time:</p>
<ol>
<li>Pre-trained model weights for the specified model type (i.e., <code>bert-base-uncased</code>) are downloaded</li>
<li>SQuAD training examples are converted into features (takes 15-30 minutes depending on dataset size and number of threads)</li>
<li>Training features are saved to a cache file (so that you don't have to do this again <em>for this model type</em>)</li>
<li>If <code>--do_train</code>, training commences for as many epochs as you specify, saving the model weights every <code>--save_steps</code> steps until training finishes</li>
<li>The final model weights and peripheral files are saved to <code>--output_dir</code>
</li>
<li>If <code>--do_eval</code>, SQuAD dev examples are converted into features</li>
<li>Dev features are also saved to a cache file</li>
<li>Evaluation commences and outputs a dizzying assortment of performance scores</li>
</ol>
<h3 id="Training-requirements">
<a class="anchor" href="#Training-requirements" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training requirements<a class="anchor-link" href="#Training-requirements"> </a>
</h3>
<p>The following cells demonstrate two ways to fine-tune: on the command line and in a Colab notebook. We don't recommend fine-tuning a Transformer model unless you're rocking at least one GPU and a considerable amount of RAM. For context, our GPU is several years old (GeForce GTX TITAN X) and while it's not nearly as fast as the Tesla V100 (the current cadillac of GPUs) it gets the job done. Fine-tuning <code>bert-base-uncased</code> takes about 1.75 hours <em>per epoch</em>. Additionally, our workhorse machine has 16GB of RAM which is sufficient to train most models on either of the SQuAD datasets.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Training-on-the-command-line">
<a class="anchor" href="#Training-on-the-command-line" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training on the command line<a class="anchor-link" href="#Training-on-the-command-line"> </a>
</h4>
<p>I save the following as a shell script (<code>run_squad.sh</code>) and run on the command line (<code>$ source run_squad.sh</code>) of our workhorse GPU machine.  Shell scripts help prevent numerous mistakes and mis-keys when typing args to a command line, especially for complex scripts like this. They also allow you to keep track of which arguments were used last (though, as we'll see below, the <code>run_squad.py</code> script has a solution for that). I actually keep two shell scripts -- one explicitly for training and another for evaluation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="ch">#!/bin/sh</span>
<span class="nb">export</span> <span class="nv">DATA_DIR</span><span class="o">=</span>./data/squad 
<span class="nb">export</span> <span class="nv">MODEL_DIR</span><span class="o">=</span>./models
python run_squad.py  <span class="se">\</span>
    --model_type bert   <span class="se">\</span>
    --model_name_or_path bert-base-uncased  <span class="se">\</span>
    --output_dir <span class="nv">$MODEL_DIR</span>/bert/bbu_squad2/ <span class="se">\</span>
    --overwrite_output_dir <span class="se">\</span>
    --do_train  <span class="se">\</span>
    --train_file <span class="nv">$DATA_DIR</span>/train-v2.0.json   <span class="se">\</span>
    --version_2_with_negative <span class="se">\</span>
    --do_lower_case  <span class="se">\</span>
    --do_eval   <span class="se">\</span>
    --predict_file <span class="nv">$DATA_DIR</span>/dev-v2.0.json   <span class="se">\</span>
    --per_gpu_train_batch_size <span class="m">12</span>   <span class="se">\</span>
    --learning_rate 3e-5   <span class="se">\</span>
    --num_train_epochs <span class="m">2</span>.0   <span class="se">\</span>
    --max_seq_length <span class="m">384</span>   <span class="se">\</span>
    --doc_stride <span class="m">128</span>   <span class="se">\</span>
    --save_steps <span class="m">5000</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Training-in-Colab">
<a class="anchor" href="#Training-in-Colab" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training in Colab<a class="anchor-link" href="#Training-in-Colab"> </a>
</h4>
<p>Alternatively, you can execute training in the cell as shown below. Standard Colab environments only provide 12GB of RAM. Converting the SQuAD dataset to features is memory intensive and crashes the basic Colab environment. If you have a Colab instance with additional memory capacity (16GB+), this cell should work! Otherwise, skip it. In the next section we'll download a model that has already been fine-tuned.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>python run_squad.py  <span class="err">\</span>
    <span class="o">--</span><span class="n">model_type</span> <span class="n">bert</span>   \
    <span class="o">--</span><span class="n">model_name_or_path</span> <span class="n">bert</span><span class="o">-</span><span class="n">base</span><span class="o">-</span><span class="n">uncased</span>  \
    <span class="o">--</span><span class="n">output_dir</span> <span class="n">models</span><span class="o">/</span><span class="n">bert</span><span class="o">/</span> \
    <span class="o">--</span><span class="n">data_dir</span> <span class="n">data</span><span class="o">/</span><span class="n">squad</span>   \
    <span class="o">--</span><span class="n">overwrite_output_dir</span> \
    <span class="o">--</span><span class="n">overwrite_cache</span> \
    <span class="o">--</span><span class="n">do_train</span>  \
    <span class="o">--</span><span class="n">train_file</span> <span class="n">train</span><span class="o">-</span><span class="n">v2</span><span class="o">.</span><span class="mf">0.</span><span class="n">json</span>   \
    <span class="o">--</span><span class="n">version_2_with_negative</span> \
    <span class="o">--</span><span class="n">do_lower_case</span>  \
    <span class="o">--</span><span class="n">do_eval</span>   \
    <span class="o">--</span><span class="n">predict_file</span> <span class="n">dev</span><span class="o">-</span><span class="n">v2</span><span class="o">.</span><span class="mf">0.</span><span class="n">json</span>   \
    <span class="o">--</span><span class="n">per_gpu_train_batch_size</span> <span class="mi">2</span>   \
    <span class="o">--</span><span class="n">learning_rate</span> <span class="mf">3e-5</span>   \
    <span class="o">--</span><span class="n">num_train_epochs</span> <span class="mf">2.0</span>   \
    <span class="o">--</span><span class="n">max_seq_length</span> <span class="mi">384</span>   \
    <span class="o">--</span><span class="n">doc_stride</span> <span class="mi">128</span>   \
    <span class="o">--</span><span class="n">threads</span> <span class="mi">10</span>   \
    <span class="o">--</span><span class="n">save_steps</span> <span class="mi">5000</span> 
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training-Output">
<a class="anchor" href="#Training-Output" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training Output<a class="anchor-link" href="#Training-Output"> </a>
</h3>
<p>Successful completion of the <code>run_squad.py</code> yields a slew of output, which can be found in the <code>--output_dir</code> directory specified above. There you'll find...</p>
<p>Files for the model's tokenizer:</p>
<ul>
<li><code>tokenizer_config.json</code></li>
<li><code>vocab.txt</code></li>
<li><code>special_tokens_map.json</code></li>
</ul>
<p>Files for the model itself:</p>
<ul>
<li>
<code>pytorch_model.bin</code>: these are the actual model weights (this file can be quite large)</li>
<li>
<code>config.json</code>: details of the model architecture</li>
</ul>
<p>Binary representation of the command line arguments used to train this model (so you'll never forget which arguments you used!)</p>
<ul>
<li><code>training_args.bin</code></li>
</ul>
<p>And if you included <code>--do_eval</code>, you'll also see these files:</p>
<ul>
<li>
<code>predictions_.json</code>: the official best answer for each example</li>
<li>
<code>nbest_predictions_.json</code>: the top n best answers for each example</li>
</ul>
<p>Providing the path to this directory to <code>AutoModel</code> or <code>AutoModelForQuestionAnswering</code> will load your fine-tuned model for use.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForQuestionAnswering</span>

<span class="c1"># Load the fine-tuned model</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"./models/bert/bbu_squad2"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"./models/bert/bbu_squad2"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Using-a-pre-fine-tuned-model-from-the-Hugging-Face-repository">
<a class="anchor" href="#Using-a-pre-fine-tuned-model-from-the-Hugging-Face-repository" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using a pre-fine-tuned model from the Hugging Face repository<a class="anchor-link" href="#Using-a-pre-fine-tuned-model-from-the-Hugging-Face-repository"> </a>
</h2>
<p>If you don't have access to GPUs or don't have the time to fiddle and train models, you're in luck! Hugging Face is more than a collection of slick Transformer classes -- it also hosts <a href="https://huggingface.co/models">a repository</a> for pre-trained and fine-tuned models contributed from the wide community of NLP practitioners. Searching for "squad" brings up at least 55 models.</p>
<p><img src="/images/copied_from_nb/my_icons/HF_repo.png" alt=""></p>
<p>Clicking one of these links gives explicit code for using the model, and, in some cases, information on how it was trained and what results were achieved. Let's load one of these pre-fine-tuned models.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForQuestionAnswering</span>

<span class="c1"># Executing these commands for the first time initiates a download of the </span>
<span class="c1"># model weights to ~/.cache/torch/transformers/</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"deepset/bert-base-cased-squad2"</span><span class="p">)</span> 
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"deepset/bert-base-cased-squad2"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Let's-try-our-model!">
<a class="anchor" href="#Let's-try-our-model!" aria-hidden="true"><span class="octicon octicon-link"></span></a>Let's try our model!<a class="anchor-link" href="#Let's-try-our-model!"> </a>
</h2>
<p>Whether you fine-tuned your own or used a pre-fine-tuned model, it's time to play with it! There are three steps to QA:</p>
<ol>
<li>tokenize the input</li>
<li>obtain model scores</li>
<li>get the answer span</li>
</ol>
<p>These steps are discussed in detail in the HF <a href="https://huggingface.co/transformers/notebooks.html">Transformer Notebooks</a>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">question</span> <span class="o">=</span> <span class="s2">"Who ruled Macedonia"</span>

<span class="n">context</span> <span class="o">=</span> <span class="s2">"""Macedonia was an ancient kingdom on the periphery of Archaic and Classical Greece, </span>
<span class="s2">and later the dominant state of Hellenistic Greece. The kingdom was founded and initially ruled </span>
<span class="s2">by the Argead dynasty, followed by the Antipatrid and Antigonid dynasties. Home to the ancient </span>
<span class="s2">Macedonians, it originated on the northeastern part of the Greek peninsula. Before the 4th </span>
<span class="s2">century BC, it was a small kingdom outside of the area dominated by the city-states of Athens, </span>
<span class="s2">Sparta and Thebes, and briefly subordinate to Achaemenid Persia."""</span>


<span class="c1"># 1. TOKENIZE THE INPUT</span>
<span class="c1"># Note: if you don't include return_tensors='pt' you'll get a list of lists which is easier for </span>
<span class="c1"># exploration but you cannot feed that into a model. </span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span> 

<span class="c1"># 2. OBTAIN MODEL SCORES</span>
<span class="c1"># The AutoModelForQuestionAnswering class includes a span predictor on top of the model. </span>
<span class="c1"># The model returns answer start and end scores for each word in the text</span>
<span class="n">answer_start_scores</span><span class="p">,</span> <span class="n">answer_end_scores</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">answer_start</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">answer_start_scores</span><span class="p">)</span>  <span class="c1"># Get the most likely beginning of answer with the argmax of the score</span>
<span class="n">answer_end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">answer_end_scores</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># Get the most likely end of answer with the argmax of the score</span>

<span class="c1"># 3. GET THE ANSWER SPAN</span>
<span class="c1"># Once we have the most likely start and end tokens, we grab all the tokens between them</span>
<span class="c1"># and convert tokens back to words!</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_string</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">answer_start</span><span class="p">:</span><span class="n">answer_end</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>'the Argead dynasty'</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="QA-on-Wikipedia-pages">
<a class="anchor" href="#QA-on-Wikipedia-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>QA on Wikipedia pages<a class="anchor-link" href="#QA-on-Wikipedia-pages"> </a>
</h1>
<p>We tried our model on a question paired with a short passage. But what if we want to search for answers in longer documents? A typical Wikipedia page is much longer than the example above and we need to do a bit of massaging before we can use our model on longer contexts.</p>
<p>Let's start by pulling up a Wikipedia page.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">wikipedia</span> <span class="k">as</span> <span class="nn">wiki</span>
<span class="kn">import</span> <span class="nn">pprint</span> <span class="k">as</span> <span class="nn">pp</span>

<span class="n">question</span> <span class="o">=</span> <span class="s1">'What is the wingspan of an albatross?'</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">wiki</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Wikipedia search results for our question:</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="n">pp</span><span class="o">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="n">page</span> <span class="o">=</span> <span class="n">wiki</span><span class="o">.</span><span class="n">page</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">page</span><span class="o">.</span><span class="n">content</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">The </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> Wikipedia article contains </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="si">}</span><span class="s2"> characters."</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Wikipedia search results for our question:

['Albatross',
 'List of largest birds',
 'Black-browed albatross',
 'Argentavis',
 'Pterosaur',
 'Mollymawk',
 'List of birds by flight speed',
 'Largest body part',
 'Pelican',
 'Aspect ratio (aeronautics)']

The Albatross Wikipedia article contains 38200 characters.
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'pt'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"This translates into </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2"> tokens."</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Token indices sequence length is longer than the specified maximum sequence length for this model (10 &gt; 512). Running this sequence through the model will result in indexing errors
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>This translates into 8824 tokens.
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The tokenizer takes the input as text and returns tokens. In general, tokenizers convert words or pieces of words into a model-ingestable format. The specific tokens and format are dependent on the type of model. For example, BERT tokenizes words differently from RoBERTa, so be sure to always use the associated tokenizer appropriate for your model.</p>
<p>In this case, the tokenizer converts our input text into 8824 tokens, but this far exceeds the maximum number of tokens that can be fed to the model at one time. Most BERT-esque models can only accept 512 tokens at once, thus the (somewhat confusing) warning above (how is 10 &gt; 512?). This means we'll have to split our input into chunks and each chunk must not exceed 512 tokens in total.</p>
<p>When working with Question Answering, it's crucial that each chunk follows this format:</p>
<p>[CLS] question tokens [SEP] context tokens [SEP]</p>
<p>This means that, for each segment of a Wikipedia article, we must prepend the original question, followed by the next "chunk" of article tokens.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Time to chunk!</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>

<span class="c1"># identify question tokens (token_type_ids = 0)</span>
<span class="n">qmask</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">'token_type_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">qt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">],</span> <span class="n">qmask</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The question consists of </span><span class="si">{</span><span class="n">qt</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> tokens."</span><span class="p">)</span>

<span class="n">chunk_size</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">-</span> <span class="n">qt</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># the "-1" accounts for</span>
<span class="c1"># having to add a [SEP] token to the end of each chunk</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Each chunk will contain </span><span class="si">{</span><span class="n">chunk_size</span> <span class="o">-</span> <span class="mi">2</span><span class="si">}</span><span class="s2"> tokens of the Wikipedia article."</span><span class="p">)</span>

<span class="c1"># create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input</span>
<span class="n">chunked_input</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">qmask</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="o">~</span><span class="n">qmask</span><span class="p">)</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">chunked_input</span><span class="p">:</span>
            <span class="n">chunked_input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="n">thing</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">q</span><span class="p">,</span> <span class="n">chunk</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s1">'input_ids'</span><span class="p">:</span>
                <span class="n">thing</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">thing</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">102</span><span class="p">])))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">thing</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">thing</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])))</span>

        <span class="n">chunked_input</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">thing</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The question consists of 12 tokens.
Each chunk will contain 497 tokens of the Wikipedia article.
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">chunked_input</span><span class="o">.</span><span class="n">keys</span><span class="p">())):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of tokens in chunk </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">chunked_input</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">'input_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Number of tokens in chunk 0: 512
Number of tokens in chunk 1: 512
Number of tokens in chunk 2: 512
Number of tokens in chunk 3: 512
Number of tokens in chunk 4: 512
Number of tokens in chunk 5: 512
Number of tokens in chunk 6: 512
Number of tokens in chunk 7: 512
Number of tokens in chunk 8: 512
Number of tokens in chunk 9: 512
Number of tokens in chunk 10: 512
Number of tokens in chunk 11: 512
Number of tokens in chunk 12: 512
Number of tokens in chunk 13: 512
Number of tokens in chunk 14: 512
Number of tokens in chunk 15: 512
Number of tokens in chunk 16: 512
Number of tokens in chunk 17: 341
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Each of these chunks (except for the last one) has the following structure:</p>
<p>[CLS], 12 question tokens, [SEP], 497 tokens of the Wikipedia article, [SEP] token = 512 tokens</p>
<p>Each of these chunks can now be fed to the model without causing indexing errors. We'll get an "answer" for each chunk, however not all answers are useful since not every segment of a Wikipedia article is informative for our question. The model will return the [CLS] token when it determines that the context does not contain an answer to the question.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">convert_ids_to_string</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_string</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">))</span>

<span class="n">answer</span> <span class="o">=</span> <span class="s1">''</span>

<span class="c1"># Now we iterate over our chunks, looking for the best answer from each chunk</span>
<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chunked_input</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">answer_start_scores</span><span class="p">,</span> <span class="n">answer_end_scores</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">chunk</span><span class="p">)</span>

    <span class="n">answer_start</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">answer_start_scores</span><span class="p">)</span>
    <span class="n">answer_end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">answer_end_scores</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="n">ans</span> <span class="o">=</span> <span class="n">convert_ids_to_string</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">chunk</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">answer_start</span><span class="p">:</span><span class="n">answer_end</span><span class="p">])</span>
    
    <span class="c1"># If the ans == [CLS] then the model did not find a real answer in this chunk</span>
    <span class="k">if</span> <span class="n">ans</span> <span class="o">!=</span> <span class="s1">'[CLS]'</span><span class="p">:</span>
        <span class="n">answer</span> <span class="o">+=</span> <span class="n">ans</span> <span class="o">+</span> <span class="s2">" / "</span>
        
<span class="nb">print</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>3 . 7 m / 
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Putting-it-all-together">
<a class="anchor" href="#Putting-it-all-together" aria-hidden="true"><span class="octicon octicon-link"></span></a>Putting it all together<a class="anchor-link" href="#Putting-it-all-together"> </a>
</h1>
<p>Let's recap. We've essentially built a simple IR-based QA system! We're using <code>wikipedia</code>'s search engine to return a list of candidate documents that we then feed into our Document Reader (in this case, BERT fine-tuned on SQuAD 2.0).  Let's make our code easier to read and more self-contained by packaging the Document Reader into a class.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForQuestionAnswering</span>


<span class="k">class</span> <span class="nc">DocumentReader</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="s1">'bert-large-uncased'</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">READER_PATH</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">READER_PATH</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">READER_PATH</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chunked</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">question</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_len</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunkify</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">chunked</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">chunkify</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">""" </span>
<span class="sd">        Break up a long article into chunks that fit within the max token</span>
<span class="sd">        requirement for that Transformer model. </span>

<span class="sd">        Calls to BERT / RoBERTa / ALBERT require the following format:</span>
<span class="sd">        [CLS] question tokens [SEP] context tokens [SEP]</span>
<span class="sd">        """</span>

        <span class="c1"># create question mask based on token_type_ids</span>
        <span class="c1"># value is 0 for question tokens, 1 for context tokens</span>
        <span class="n">qmask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="s1">'token_type_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">qt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">],</span> <span class="n">qmask</span><span class="p">)</span>
        <span class="n">chunk_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_len</span> <span class="o">-</span> <span class="n">qt</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># the "-1" accounts for</span>
        <span class="c1"># having to add an ending [SEP] token to the end</span>

        <span class="c1"># create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input</span>
        <span class="n">chunked_input</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">qmask</span><span class="p">)</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="o">~</span><span class="n">qmask</span><span class="p">)</span>
            <span class="n">chunks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">chunked_input</span><span class="p">:</span>
                    <span class="n">chunked_input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

                <span class="n">thing</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">q</span><span class="p">,</span> <span class="n">chunk</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s1">'input_ids'</span><span class="p">:</span>
                        <span class="n">thing</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">thing</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">102</span><span class="p">])))</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">thing</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">thing</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])))</span>

                <span class="n">chunked_input</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">thing</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">chunked_input</span>

    <span class="k">def</span> <span class="nf">get_answer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunked</span><span class="p">:</span>
            <span class="n">answer</span> <span class="o">=</span> <span class="s1">''</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">answer_start_scores</span><span class="p">,</span> <span class="n">answer_end_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">chunk</span><span class="p">)</span>

                <span class="n">answer_start</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">answer_start_scores</span><span class="p">)</span>
                <span class="n">answer_end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">answer_end_scores</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

                <span class="n">ans</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_string</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">answer_start</span><span class="p">:</span><span class="n">answer_end</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">ans</span> <span class="o">!=</span> <span class="s1">'[CLS]'</span><span class="p">:</span>
                    <span class="n">answer</span> <span class="o">+=</span> <span class="n">ans</span> <span class="o">+</span> <span class="s2">" / "</span>
            <span class="k">return</span> <span class="n">answer</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">answer_start_scores</span><span class="p">,</span> <span class="n">answer_end_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>

            <span class="n">answer_start</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">answer_start_scores</span><span class="p">)</span>  <span class="c1"># Get the most likely beginning of answer with the argmax of the score</span>
            <span class="n">answer_end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">answer_end_scores</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># Get the most likely end of answer with the argmax of the score</span>
        
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span>
                                              <span class="n">answer_start</span><span class="p">:</span><span class="n">answer_end</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">convert_ids_to_string</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Below is our clean, fully working QA system! Feel free to add your own questions.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># collapse-hide </span>

<span class="c1"># To make the following output more readable I'll turn off the token sequence length warning</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">"transformers.tokenization_utils"</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">questions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">'When was Barack Obama born?'</span><span class="p">,</span>
    <span class="s1">'Why is the sky blue?'</span><span class="p">,</span>
    <span class="s1">'How many sides does a pentagon have?'</span>
<span class="p">]</span>

<span class="n">reader</span> <span class="o">=</span> <span class="n">DocumentReader</span><span class="p">(</span><span class="s2">"deepset/bert-base-cased-squad2"</span><span class="p">)</span> 

<span class="c1"># if you trained your own model using the training cell earlier you can access it with this:</span>
<span class="c1">#reader = DocumentReader("./models/bert/bbu_squad2")</span>

<span class="k">for</span> <span class="n">question</span> <span class="ow">in</span> <span class="n">questions</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Question: </span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">wiki</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>

    <span class="n">page</span> <span class="o">=</span> <span class="n">wiki</span><span class="o">.</span><span class="n">page</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Top wiki result: </span><span class="si">{</span><span class="n">page</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">text</span> <span class="o">=</span> <span class="n">page</span><span class="o">.</span><span class="n">content</span>

    <span class="n">reader</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Answer: </span><span class="si">{</span><span class="n">reader</span><span class="o">.</span><span class="n">get_answer</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Question: When was Barack Obama born?
Top wiki result: &lt;WikipediaPage 'Barack Obama Sr.'&gt;
Answer: 18 June 1936 / February 2 , 1961 / 

Question: Why is the sky blue?
Top wiki result: &lt;WikipediaPage 'Diffuse sky radiation'&gt;
Answer: Rayleigh scattering / 

Question: How many sides does a pentagon have?
Top wiki result: &lt;WikipediaPage 'The Pentagon'&gt;
Answer: five / 

</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We got 2 out of 3 questions right!</p>
<p>Notice that, at least for the current questions I've chosen, the QA systems fails not because of BERT but because of  Wikipedia's default search engine! It pulls up the wrong page for two of our questions: a page about Barack Obama Sr. instead of the former US President, and an article about the US's Dept of Defense building "The Pentagon" instead of a page about geometry. In the latter case we ended up with the correct answer by coincidence! This illustrates that any successful IR-based QA system requires a search engine (document retriever) as good as the document reader.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Wrapping-Up">
<a class="anchor" href="#Wrapping-Up" aria-hidden="true"><span class="octicon octicon-link"></span></a>Wrapping Up<a class="anchor-link" href="#Wrapping-Up"> </a>
</h1>
<p>There we have it! A working QA system on Wikipedia articles. This is great but it's admittedly not very sophisticated. Furthermore, we've left a lot of questions unanswered:</p>
<ol>
<li>Why the SQuAD dataset and not something else? What other options are there? </li>
<li>How good is BERT at answering questions? And how do we define "good"?</li>
<li>Why BERT and not another Transformer model? </li>
<li>Currently, our QA system can return an answer for each chunk of a Wiki article but not all of those answers are correct -- How can we improve our <code>get_answer</code> method?</li>
<li>Additionally, we're chunking a wiki article in such a way that we could be ending a chunk in the midde of a sentence -- Can we improve our <code>chunkify</code> method? </li>
</ol>
<p>Stay tuned for future posts as we'll tackle these questions and more!</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="fastforwardlabs/ff14_blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/hidden/" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>CFF builds a state-of-the-art QA application with the latest NLP techniques</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastforwardlabs" title="fastforwardlabs"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/FastForwardLabs" title="FastForwardLabs"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
