<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://qa.fastforwardlabs.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://qa.fastforwardlabs.com/" rel="alternate" type="text/html" /><updated>2020-04-23T18:36:08-05:00</updated><id>https://qa.fastforwardlabs.com/feed.xml</id><title type="html">NLP for Question Answering</title><subtitle>A CFFL journey towards building a state-of-the-art QA application with the latest NLP techniques</subtitle><entry><title type="html">NLP for Automated Question Answering</title><link href="https://qa.fastforwardlabs.com/markdown/2020/04/28/Intro-to-QA.html" rel="alternate" type="text/html" title="NLP for Automated Question Answering" /><published>2020-04-28T00:00:00-05:00</published><updated>2020-04-28T00:00:00-05:00</updated><id>https://qa.fastforwardlabs.com/markdown/2020/04/28/Intro-to-QA</id><content type="html" xml:base="https://qa.fastforwardlabs.com/markdown/2020/04/28/Intro-to-QA.html">&lt;p&gt;Welcome to the first edition of the Cloudera Fast Forward blog on Natural 
Language Processing for Question Answering! Throughout this series, we’ll 
build a Question Answering (QA) system with off-the-shelf parts and blog about 
our process and what we find along the way. We hope to wind up with a 
beginning-to-end documentary that provides:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;insight into QA as a tool,&lt;/li&gt;
  &lt;li&gt;useful context to make decisions for those who might build their own QA
 system,&lt;/li&gt;
  &lt;li&gt;tips and tricks we pick up as we go,&lt;/li&gt;
  &lt;li&gt;sample code, if we’re lucky (we’ll show our work in notebooks), and&lt;/li&gt;
  &lt;li&gt;entertainment, as you watch us fumble our way through the process. (Feedback
 and commentary welcome.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’re trying a new thing here. In the past, Cloudera Fast Forward has
 documented its work in &lt;a href=&quot;https://www.cloudera.com/products/fast-forward-labs-research/fast-forward-labs-research-reports.html&quot;&gt;discrete reports&lt;/a&gt;. 
 We hope this new format suits the above goals and makes the topic more
  accessible, while ultimately being useful.&lt;/p&gt;

&lt;p&gt;To kick off the series, this introductory post will discuss what QA is and isn’t, where this technology is being employed, and what techniques are used to accomplish this natural language task.&lt;/p&gt;

&lt;h2 id=&quot;question-answering-in-a-nutshell&quot;&gt;Question Answering in a Nutshell&lt;/h2&gt;
&lt;p&gt;Question Answering is a human-machine interaction to extract information from 
data using natural language queries. Machines do not inherently understand human
 languages any more than the average human understands machine language. A 
 well-developed QA system bridges the gap between the two, allowing humans to 
 extract knowledge from data in a way that is natural to us, i.e., asking questions.&lt;/p&gt;

&lt;p&gt;QA systems accept questions in the form of natural language (typically text 
based, although you are probably also familiar with systems that accept speech 
input, such as Amazon’s Alexa or Apple’s Siri), and output a concise answer. 
Google’s search engine product adds a form of question answering in addition to 
its traditional search results, as illustrated here:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/post1/abe_search_crop.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Google took our question and returned a set of 1.3 million documents (not shown)
 relevant to the search terms, i.e., documents about Abraham Lincoln. 
 Google also used what it knows about the contents of some of those documents 
 to provide a “&lt;a href=&quot;https://support.google.com/websearch/answer/9351707?p=featured_snippets&quot;&gt;snippet&lt;/a&gt;” 
 that answered our question in one word, presented above 
 a link to the most pertinent website and keyword-highlighted text.&lt;/p&gt;

&lt;p&gt;This goes beyond the standard capabilities of a search engine, which typically only return a list of relevant documents or websites. Google is pretty tight-lipped about its algorithms, so we can’t tell you whether they are harnessing elements of natural language understanding or which algorithms they’re using. We’ll revisit this example in a later section and discuss how we can (and will!) build our own system to demonstrate how question answering works in practice.&lt;/p&gt;

&lt;h2 id=&quot;why-question-answering&quot;&gt;Why Question Answering?&lt;/h2&gt;
&lt;p&gt;Sophisticated Google searches with precise answers are fun, but how useful are 
QA systems in general? It turns out that this technology is maturing rapidly. 
Gartner recently identified &lt;a href=&quot;https://www.gartner.com/smarterwithgartner/gartner-top-10-data-analytics-trends/&quot;&gt;natural language processing and conversational 
analytics&lt;/a&gt; 
as one of the top trends poised to make a substantial impact in the 
next three to five years.  These technologies will provide increased data access, 
ease of use, and wider adoption of analytics platforms - especially to 
mainstream users. QA systems specifically will be a core part of the NLP suite, 
and are already seeing adoption in several areas.&lt;/p&gt;

&lt;div class=&quot;Toast&quot;&gt;
   &lt;span class=&quot;Toast-icon&quot;&gt;&lt;svg class=&quot;octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;
   &lt;span class=&quot;Toast-content&quot;&gt;Missing some paragraphs from the Google Doc&lt;/span&gt;
&lt;/div&gt;

&lt;h2 id=&quot;designing-a-question-answerer&quot;&gt;Designing a Question Answerer&lt;/h2&gt;
&lt;p&gt;As explained above, question answering systems process natural language queries 
and output concise answers. This general capability can be implemented in dozens 
of ways. How a QA system is designed depends, in large part, on three key 
elements: the knowledge provided to the system, the types of questions it can 
answer, and the structure of the data supporting the system.&lt;/p&gt;

&lt;h3 id=&quot;domain&quot;&gt;Domain&lt;/h3&gt;
&lt;p&gt;QA systems operate within a domain, constrained by the data that is provided to 
them. The domain represents the embodiment of all the knowledge the system can 
know. There are two domain paradigms: open and closed. Closed domain systems 
are narrow in scope and focus on a specific topic or regime. Open domain systems 
are broad, answering general knowledge questions.&lt;/p&gt;

&lt;h3 id=&quot;question-type&quot;&gt;Question Type&lt;/h3&gt;
&lt;p&gt;Once you’ve decided the scope of knowledge your QA system will cover, you must 
also determine what types of questions it can answer. The vast majority of all 
QA systems answer factual questions: those that start with who, what, where, 
when, and how many. These types of questions tend to be straightforward enough 
for a machine to comprehend, and can be built directly atop structural databases 
or ontologies, as well as being extracted directly from unstructured text.&lt;/p&gt;

&lt;p&gt;However, research is emerging that would allow QA systems to answer hypothetical 
questions, cause-effect questions, confirmation (yes/no) questions, and 
inferential questions (questions whose answers can be inferred from one or more 
pieces of evidence). Much of this research is still in its infancy, however, as 
the requisite natural language understanding is (for now) beyond the capabilities 
of most of today’s algorithms.&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;
&lt;p&gt;There’s more than one way to cuddle a cat, as the saying goes. Question 
answering seeks to extract information from data and, generally speaking, data 
come in two broad formats: structured and unstructured. QA algorithms have been 
developed to harness the information from either paradigm: knowledge- based 
systems for structured data and information retrieval-based systems for 
unstructured (text) data. Some QA systems exploit a hybrid design that harvests 
information from both data types; IBM’s Watson is a famous example. In this 
section, we’ll highlight some of the most widely used techniques in each data 
regime - concentrating more on those for unstructured data, since this will be 
the focus of our applied research. Because we’ll be discussing explicit methods 
and techniques, the following sections are more technical. And we’ll note that, 
while we provide an overview here, an even more comprehensive discussion can be 
found in the Question Answering chapter of Jurafsky and Martin’s Speech and 
Language Processing textbook.&lt;/p&gt;

&lt;h4 id=&quot;information-retrieval-based-systems-retrievers-and-readers&quot;&gt;Information Retrieval-Based Systems: Retrievers and Readers&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/images/post1/reading_retriever.jpg&quot; alt=&quot;&quot; title=&quot;Get it? Retriever? Reader?&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Information retrieval-based question answering (IR QA) systems find and extract 
a text segment from a large collection of documents. The collection can be as 
vast as the entire web (open domain) or as specific as a company’s Confluence 
documents (closed domain). Contemporary IR QA systems first identify the most 
relevant documents in the collection, and then extract the answer from the 
contents of those documents. To illustrate this approach, let’s revisit our 
Google example from the introduction, only this time we’ll include some of the 
search results!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/post1/abe_search.png&quot; alt=&quot;&quot; title=&quot;Did Abe have big
 ears?&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We already talked about how the snippet box acts like a QA system. 
The search results below the snippet illustrate some of the reasons why an IR QA 
system can be more useful than a search engine alone. The relevant links vary 
from essentially advertising (study.com), to making fun of Lincoln’s ears 
(Reddit at its finest), to a discussion of color blindness (answers.com without 
the answer we want), to an article about all presidents’ eye color (getting 
warmer, Chicago Tribune), to the last link, answers.yahoo.com, which is on-topic, 
and narrowly scoped to Lincoln but gives an ambiguous answer. Without the 
snippet box at the top, a user would have to skim each of these links looking 
for their answer.&lt;/p&gt;

&lt;p&gt;IR QA systems are not just search engines, which take general natural language 
terms and provide a list of relevant documents. IR QA systems perform an 
additional layer of processing on the most relevant documents to deliver a 
pointed answer based on the contents of those documents (like the snippet box). 
While we won’t hazard a guess at exactly how Google extracted “gray” from these 
search results, we can examine how an IR QA system could exhibit similar 
functionality in a real world (e.g., non-Google) implementation. Below we 
illustrate the workflow of a generic IR-based QA system. These systems generally 
have two main components: the document retriever and the document reader.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/post1/QAworkflow.png&quot; alt=&quot;&quot; title=&quot;Generic IR QA
 system&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The document retriever functions as the search engine, ranking and retrieving 
relevant documents to which it has access. It supplies a set of candidate 
documents that could answer the question (often with mixed results, per the 
Google search shown above). The second component is the document reader: 
reading comprehension algorithms built with core NLP techniques. This component 
processes the candidate documents and extracts from one of them an explicit span 
of text that best satisfies the query. Let’s dive into each of these components.&lt;/p&gt;</content><author><name></name></author><summary type="html">Welcome to the first edition of the Cloudera Fast Forward blog on Natural Language Processing for Question Answering! Throughout this series, we’ll build a Question Answering (QA) system with off-the-shelf parts and blog about our process and what we find along the way. We hope to wind up with a beginning-to-end documentary that provides:</summary></entry></feed>